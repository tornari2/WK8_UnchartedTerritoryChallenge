# Polars String Similarity Kernels - Product Requirements Document

## Project Overview
Add new native string similarity and cosine similarity functions to Polars, enabling efficient fuzzy matching and text comparison at scale.

## Core Requirements

### Similarity Functions to Implement
1. Hamming Similarity - Compare strings of equal length by counting differing codepoints
2. Levenshtein Similarity - Edit distance based similarity using Wagner-Fischer algorithm
3. Damerau-Levenshtein Similarity (OSA) - Edit distance with transposition support
4. Jaro-Winkler Similarity - Prefix-weighted character matching algorithm
5. Cosine Similarity - Vector similarity for embeddings (Array<f32> or List<f32>)

### Technical Requirements

#### Task 1: Environment Setup and Polars Codebase Onboarding
Set up local Polars build environment, run existing tests, and study key modules:
- polars-core: ChunkedArray and kernels code
- polars-plan: expression DSL and logical plan
- polars-expr: physical expression execution
- polars-ops: existing string operations
- py-polars: Python bindings
- Review Arrow UTF-8 and list/array layout

#### Task 2: Implement Hamming Similarity Kernel
Create crates/polars-ops/src/chunked_array/strings/similarity.rs with:
- Input: &StringChunked, &StringChunked
- Output: Float32Chunked
- Count differing codepoints, normalize by length
- Return null if lengths differ, 1.0 if identical
- Handle null bitmaps and multi-chunk columns

#### Task 3: Implement Levenshtein Similarity Kernel
Add levenshtein_similarity function:
- Space-optimized Wagner-Fischer algorithm O(min(n,m))
- Normalize: 1.0 - (distance / max(len_a, len_b))
- Codepoint-level operations
- Validate against RapidFuzz

#### Task 4: Implement Damerau-Levenshtein Similarity (OSA)
Extend Levenshtein for transpositions:
- OSA variant (single edit per character)
- Normalize result
- Reuse Levenshtein machinery

#### Task 5: Implement Jaro-Winkler Similarity
Add jaro_winkler_similarity:
- Jaro algorithm: matching chars in window, count transpositions
- Winkler modification: prefix_weight=0.1, prefix_length=4
- Formula: jaro + (prefix_len * 0.1 * (1 - jaro))

#### Task 6: Implement Cosine Similarity for Vectors
Create crates/polars-ops/src/chunked_array/array/similarity.rs:
- Input: &ArrayChunked or &ListChunked (f32)
- Algorithm: dot(a, b) / (||a|| * ||b||)
- Handle mismatched lengths, zero-magnitude, nulls
- Numeric stability with epsilon

#### Task 7: Add FunctionExpr Variants
In polars-plan/src/dsl/functions.rs:
- Add StringSimilarity(StringSimilarityType) variant
- Create StringSimilarityType enum: Levenshtein, DamerauLevenshtein, JaroWinkler, Hamming
- Add CosineSimilarity variant
- Ensure serialization works

#### Task 8: DSL Methods in String Namespace
In polars-plan/src/dsl/strings.rs (Utf8NameSpace):
- levenshtein_sim(self, other: Expr) -> Expr
- damerau_levenshtein_sim(self, other: Expr) -> Expr
- jaro_winkler_sim(self, other: Expr) -> Expr
- hamming_sim(self, other: Expr) -> Expr

#### Task 9: DSL Methods in Array Namespace
In polars-plan/src/dsl/arrays.rs (ArrayNameSpace):
- cosine_similarity(self, other: Expr) -> Expr

#### Task 10: Wire Up Physical Expression Builder
In polars-lazy/src/physical_plan/expression.rs:
- Add match arms for StringSimilarity and CosineSimilarity
- Handle column-to-column and column-to-literal cases
- Integration tests for expression execution

#### Task 11: Python Bindings for String Similarity
In py-polars/src/polars/ (.str namespace):
- levenshtein_sim
- damerau_levenshtein_sim
- jaro_winkler_sim
- hamming_sim
- Type hints and docstrings

#### Task 12: Python Bindings for Cosine Similarity
In py-polars/src/polars/ (.arr namespace):
- cosine_similarity method
- Handle list/array literal conversion

#### Task 13: Comprehensive Testing Suite
Create test files:
- crates/polars-ops/tests/strings_similarity.rs
- crates/polars-ops/tests/array_similarity.rs
- py-polars/tests/unit/expressions/test_string_similarity.py

Include:
- Unit tests with RapidFuzz/NumPy validation
- Edge cases: nulls, empty strings, mismatched lengths
- Integration tests: eager/lazy execution
- Fuzz tests for invariants
- Performance benchmarks

#### Task 14: Documentation and Examples
- Rust docstrings with algorithm descriptions
- Python docstrings with examples
- Usage examples: fuzzy filtering, deduplication, ML features
- Performance notes and limitations

## Dependencies
- Task 1 is foundation (no dependencies)
- Tasks 2-6 depend on Task 1 (kernel implementations)
- Task 7 depends on Tasks 3-5 (needs kernels first)
- Tasks 8-9 depend on Task 7 (DSL needs FunctionExpr)
- Task 10 depends on Tasks 7-9 (wires up DSL to kernels)
- Tasks 11-12 depend on Task 10 (Python wraps Rust)
- Task 13 depends on Tasks 2-6, 10 (tests all components)
- Task 14 depends on Tasks 11-13 (docs after implementation)

## Success Criteria
- All similarity functions return Float32 in [0.0, 1.0]
- Results validate against RapidFuzz (strings) and NumPy (cosine)
- Python API: df.select(pl.col("name").str.levenshtein_sim(other))
- All tests pass, documentation complete

## Performance Optimization Requirements

### Phase 2: Performance Optimization

After initial implementation and benchmarking, optimize similarity functions to match or exceed RapidFuzz performance, particularly for Levenshtein distance which currently shows 2.75-8.87x slower performance.

#### High Priority Optimizations (High Impact, Easier Implementation)

**Task 15: ASCII Fast Path Optimization**
- Detect ASCII-only strings and use byte-level operations instead of Unicode codepoint iteration
- Implement `levenshtein_distance_bytes()` for ASCII strings
- Apply to all string similarity functions (Levenshtein, Damerau-Levenshtein, Jaro-Winkler, Hamming)
- Expected impact: 2-5x speedup for common ASCII text

**Task 16: Early Exit Optimizations**
- Add length difference check: if `max_len - min_len > max_len / 2`, return early with similarity 0.0
- Add identical string check before running full algorithm
- Add early termination with threshold for bounded similarity queries
- Expected impact: 1.5-3x speedup for mismatched strings

**Task 17: Parallel Chunk Processing**
- Use Rayon to process chunks in parallel
- Implement parallel iterators for ChunkedArray operations
- Ensure thread safety and proper null handling
- Expected impact: 2-4x speedup on multi-core systems

**Task 18: Memory Pool and Buffer Reuse**
- Implement thread-local buffer pool for DP matrix rows
- Reuse Vec allocations across function calls
- Reduce allocation overhead in hot loops
- Expected impact: 10-20% speedup, reduced memory pressure

#### Medium Priority Optimizations (Moderate Complexity)

**Task 19: Myers' Bit-Parallel Algorithm**
- Implement Myers' bit-parallel algorithm for small strings (< 64 chars)
- Use for bounded distance calculations
- Fall back to Wagner-Fischer for larger strings
- Expected impact: 2-3x speedup for short strings

**Task 20: Early Termination with Threshold**
- Add optional threshold parameter to similarity functions
- Exit early if minimum distance exceeds threshold
- Useful for filtering scenarios where only high-similarity matches are needed
- Expected impact: 1.5-2x speedup for threshold-based queries

**Task 21: Branch Prediction Optimization**
- Add `#[likely]`/`#[unlikely]` attributes for common code paths
- Optimize inner loop branches (character equality checks)
- Use `#[inline(always)]` for hot functions
- Expected impact: 5-15% speedup

#### Advanced Optimizations (High Impact, Higher Complexity)

**Task 22: SIMD Character Comparison**
- Use SIMD instructions to compare multiple characters at once
- Implement using `std::simd` or `portable_simd` feature
- Vectorize character equality checks in inner loops
- Expected impact: 2-4x speedup for character comparisons

**Task 23: Inner Loop Optimization**
- Unroll inner loops for small string lengths
- Use unsafe indexing with bounds checks removed in hot paths
- Optimize memory access patterns for cache efficiency
- Expected impact: 10-30% speedup

**Task 24: Integer Type Optimization**
- Use `u16` or `u8` for distance calculations when strings are bounded
- Reduce memory footprint and improve cache locality
- Dynamic type selection based on string length
- Expected impact: 5-15% speedup, reduced memory usage

#### Cosine Similarity Optimizations

**Task 25: SIMD for Cosine Similarity**
- Vectorize dot product calculation using SIMD
- Vectorize magnitude (sum of squares) computation
- Use SIMD for element-wise multiplication
- Expected impact: 3-5x speedup for vector operations

**Task 26: Cosine Similarity Memory Optimization**
- Optimize memory access patterns for vector operations
- Cache-friendly iteration order
- Reduce temporary allocations
- Expected impact: 10-20% speedup

#### Highest Impact Optimizations (Critical for Levenshtein Performance)

**Task 27: Diagonal Band Optimization for Levenshtein (HIGHEST PRIORITY)**
- Implement diagonal band algorithm to reduce computation from O(m×n) to O(m×k) where k is max distance
- Only compute cells within diagonal band: [i-j] <= max_distance
- Use banded matrix storage to reduce memory footprint
- Estimate max distance from length difference and threshold
- Apply to both bounded and unbounded Levenshtein calculations
- Expected impact: 5-10x speedup for typical cases (makes Levenshtein competitive with RapidFuzz)
- This is the highest ROI optimization - addresses the 8x performance gap

**Task 28: SIMD for Diagonal Band Computation**
- Add explicit SIMD vectorization to diagonal band algorithm
- Use `std::simd` (portable_simd) to process multiple cells in band in parallel
- Vectorize the min operations in the DP recurrence relation
- Handle data dependencies in SIMD-friendly way
- Runtime CPU feature detection (AVX-512, AVX2, SSE, NEON)
- Expected impact: Additional 2-4x speedup on top of diagonal band (10-40x total vs baseline)
- Would make Levenshtein significantly faster than RapidFuzz

**Task 29: Explicit SIMD for Character Comparison**
- Replace auto-vectorized character comparison with explicit `std::simd` implementation
- Use `u8x64` vectors for AVX-512, `u8x32` for AVX2, `u8x16` for SSE/NEON
- Implement `count_differences_simd()` using explicit SIMD intrinsics
- Add CPU feature detection for optimal lane width selection
- Expected impact: 2-4x additional speedup over current auto-vectorization
- Applies to Hamming distance and character comparison in other algorithms

**Task 30: Explicit SIMD for Cosine Similarity Enhancement**
- Enhance existing cosine similarity SIMD with explicit `std::simd` implementation
- Use `f64x8` vectors for AVX-512, `f64x4` for AVX2
- Replace loop unrolling with explicit SIMD vector operations
- Add CPU feature detection and runtime selection
- Expected impact: Additional 2-3x speedup (20-50x total vs NumPy)

#### Phase 2: Comprehensive SIMD Optimization (NEW)

**Task 31: Jaro-Winkler SIMD Optimization (CRITICAL PRIORITY)**
- **Status:** Currently 0.88x slower than RapidFuzz on large datasets - ONLY function slower than reference
- **Priority:** CRITICAL - Highest ROI optimization
- **Expected Impact:** 3-5x speedup (would make it 2.6-4.4x faster than RapidFuzz)

**Subtask 31.1: SIMD Buffer Clearing**
- Implement `clear_buffer_simd()` using u8x32 vectors
- Replace loop-based buffer clearing in `jaro_similarity_bytes()`
- Use `ptr::write_bytes` as fallback when SIMD unavailable
- Expected: 10-20% speedup

**Subtask 31.2: SIMD Character Comparison in Matching Loop**
- Implement SIMD character matching using u8x32 vectors
- Process 32 bytes at a time in matching window
- Use `simd_eq()` and `to_bitmask()` for efficient comparison
- Expected: 2-4x speedup for matching phase

**Subtask 31.3: Early Exit Optimizations**
- Add length difference check (return 0.0 if >50% difference)
- Add character set overlap check for long strings (>10 chars)
- Add `#[likely]`/`#[unlikely]` branch hints
- Expected: 1.5-3x speedup for mismatched strings

**Subtask 31.4: SIMD Transposition Counting**
- Vectorize character comparison in transposition counting
- Use SIMD for parallel character checks where possible
- Expected: 5-15% speedup

**Subtask 31.5: Hash-Based Matching for Long Strings (Optional)**
- Use HashMap for O(1) character lookup when strings >50 chars
- Fall back to SIMD matching for shorter strings
- Expected: 3-5x speedup for very long strings

**Task 32: Damerau-Levenshtein SIMD Optimization**
- **Status:** Currently 1.90x faster than RapidFuzz, but no SIMD
- **Priority:** High
- **Expected Impact:** 2-3x speedup (would make it 3.8-5.7x faster than RapidFuzz)

**Subtask 32.1: SIMD Min Operations for DP Matrix**
- Implement `simd_min3_u32x8()` for parallel min operations
- Vectorize the DP recurrence: `min(prev_row[i] + 1, curr_row[i-1] + 1, prev_row[i-1] + cost)`
- Process 8 cells in parallel using u32x8 vectors
- Expected: 2-3x speedup

**Subtask 32.2: SIMD Character Comparison**
- Use SIMD for cost calculation (character equality)
- Vectorize the inner loop character comparisons
- Expected: 10-20% additional speedup

**Subtask 32.3: SIMD Transposition Check**
- Vectorize the transposition condition check
- Use SIMD for parallel character comparisons in transposition detection
- Expected: 5-10% additional speedup

**Task 33: Extend Levenshtein SIMD to Unbounded Queries**
- **Status:** Currently only bounded queries have SIMD (partial coverage)
- **Priority:** Medium
- **Expected Impact:** 1.5-2x speedup for unbounded queries

**Subtask 33.1: Adaptive Band with SIMD**
- Extend `levenshtein_distance_adaptive_band()` to use SIMD
- Apply SIMD min operations to adaptive band computation
- Use runtime distance estimation for band width
- Expected: 1.5-2x speedup

**Subtask 33.2: SIMD for Standard Wagner-Fischer**
- Add SIMD optimization to unbounded `levenshtein_distance_bytes()`
- Vectorize min operations in DP recurrence
- Process multiple cells in parallel
- Expected: 1.2-1.5x speedup

**Task 34: Enhanced SIMD for Cosine Similarity**
- **Status:** Already has SIMD (39x faster), but can be enhanced
- **Priority:** Low (already excellent performance)
- **Expected Impact:** Additional 1.5-2x speedup (60-80x total vs NumPy)

**Subtask 34.1: AVX-512 Support**
- Add f64x8 vectors for AVX-512 capable CPUs
- Runtime CPU feature detection
- Process 8 doubles at a time (vs current 4)
- Expected: 1.5-2x speedup on AVX-512 systems

**Subtask 34.2: Fused Multiply-Add (FMA)**
- Use FMA instructions for dot product: `a * b + acc`
- More accurate and faster than separate multiply/add
- Expected: 5-10% additional speedup

### Phase 2 Implementation Strategy
- **Week 1-2:** Task 31 (Jaro-Winkler SIMD) - CRITICAL
- **Week 3-4:** Task 32 (Damerau-Levenshtein SIMD) - High
- **Week 5:** Task 33 (Levenshtein SIMD extension) - Medium
- **Week 6:** Task 34 (Cosine SIMD enhancement) - Low

### Phase 3: Final Performance Gap Closure (NEW)

These optimizations target the remaining performance gaps identified in benchmarking:
- Hamming: RapidFuzz 1.14x faster on small datasets (1K strings)
- Jaro-Winkler: RapidFuzz 1.08x faster on large datasets (100K strings)

**Task 35: Hamming Similarity Small Dataset Optimization (HIGH PRIORITY)**
- **Status:** RapidFuzz 1.14x faster on 1K strings (length 10)
- **Root Cause:** Per-element overhead dominates for small strings
- **Priority:** High
- **Expected Impact:** 1.5-2x speedup on small datasets (would make Polars faster than RapidFuzz)

**Subtask 35.1: Batch ASCII Detection at Column Level**
- Check if entire column is ASCII once using SIMD scan
- Store ASCII flag in column metadata or compute once per operation
- Skip per-element `is_ascii_str()` checks when column is pure ASCII
- Expected: 20-30% speedup by eliminating redundant checks

**Subtask 35.2: Ultra-Fast Inline Path for Very Small Strings**
- For strings ≤16 bytes, use completely inline comparison
- No function calls, no bounds checks in hot path
- Use `#[inline(always)]` and manual loop unrolling
- Process 8 bytes at a time using u64 XOR comparison
- Expected: 30-50% speedup for small strings

**Subtask 35.3: Branchless XOR-Based Counting**
- Replace `if s1[i] != s2[i] { 1 } else { 0 }` with branchless version
- Use `((s1[i] ^ s2[i]) != 0) as usize` or SIMD popcount
- Better CPU pipelining, no branch mispredictions
- Expected: 10-20% speedup

**Subtask 35.4: Specialized Column-Level Processing**
- For homogeneous columns (all same length, all ASCII), bypass generic iterator
- Implement `hamming_similarity_batch()` that processes entire column directly
- Reduce per-element function call overhead
- Expected: 15-25% speedup for batch processing

**Task 36: Jaro-Winkler Large Dataset Optimization (CRITICAL PRIORITY)**
- **Status:** RapidFuzz 1.08x faster on 100K strings (length 30)
- **Root Cause:** Match window iteration and function call overhead
- **Priority:** Critical
- **Expected Impact:** 1.3-1.8x speedup on large datasets (would make Polars faster than RapidFuzz)

**Subtask 36.1: Inline SIMD Character Search**
- Inline `simd_find_match_in_range()` into main Jaro loop
- Eliminates 3M+ function calls for 100K string pairs
- Use `#[inline(always)]` or manual inlining
- Expected: 15-25% speedup from eliminated call overhead

**Subtask 36.2: Bit-Parallel Match Tracking**
- Replace `Vec<bool>` match arrays with `u64` bitmasks (up to 64 chars)
- Use bit operations for match tracking: `matches |= 1 << j`
- Check matches with: `(matches >> j) & 1 == 0`
- Faster clearing, checking, and setting
- Expected: 20-30% speedup

**Subtask 36.3: Pre-Indexed Character Position Lookup**
- Build character position index for s2: `HashMap<u8, SmallVec<[u8; 4]>>`
- O(1) lookup for potential match positions instead of linear search
- Especially effective for longer strings with repeated characters
- Expected: 20-40% speedup for strings >20 chars

**Subtask 36.4: Stack-Allocated Buffers for Small Strings**
- Use `[bool; 64]` stack array instead of thread-local Vec for strings ≤64 chars
- Eliminates `JARO_BUFFER.with()` overhead per call
- Thread-local only for larger strings
- Expected: 10-15% speedup

**Subtask 36.5: Parallel Processing with Rayon**
- Process multiple string pairs in parallel using Rayon
- Split column into chunks, process chunks in parallel
- Merge results maintaining order
- Expected: 2-4x speedup on multi-core systems

**Task 37: General Column-Level Optimizations (MEDIUM PRIORITY)**
- **Priority:** Medium
- **Expected Impact:** 10-20% speedup across all similarity functions

**Subtask 37.1: Pre-scan Column Metadata**
- Scan column once to determine: all ASCII? max length? min length?
- Use metadata to select optimal algorithm path
- Skip per-element checks when column is homogeneous

**Subtask 37.2: Chunked Parallel Processing**
- Implement chunked parallel iteration for all similarity functions
- Use Rayon's `par_chunks()` for multi-threaded processing
- Ensure thread-local buffers don't cause contention

**Subtask 37.3: SIMD Column Scanning**
- Use SIMD to scan columns for ASCII detection
- Vectorized length extraction for batch processing
- Pre-compute column statistics for algorithm selection

### Phase 3 Implementation Strategy
- **Week 1:** Task 35 (Hamming small dataset optimization) - High
- **Week 2:** Task 36 (Jaro-Winkler large dataset optimization) - Critical
- **Week 3:** Task 37 (General column-level optimizations) - Medium

### Phase 4: Additional Jaro-Winkler Optimizations (NEW)

These optimizations target further performance improvements for Jaro-Winkler similarity, building on the work completed in Tasks 31 and 36. Expected combined impact: 2-3x additional speedup (6-15x total vs baseline).

#### High Priority Optimizations (Expected 1.5-2x Speedup)

**Task 38: SIMD-Optimized Prefix Calculation**
- **Priority:** High
- **Expected Impact:** 10-20% speedup for prefix calculation
- **Description:** Use SIMD to compare up to 4 bytes at once (MAX_PREFIX_LENGTH = 4) instead of iterating byte-by-byte
- **Implementation:** 
  - Create `calculate_prefix_simd()` using `u8x4` vectors
  - Use `simd_eq()` and `to_bitmask()` for efficient comparison
  - Fall back to scalar for strings < 4 bytes
- **Location:** `jaro_winkler_similarity_impl()` and `jaro_winkler_similarity_impl_ascii()`

**Task 39: Early Termination with Threshold**
- **Priority:** High (CRITICAL for threshold-based queries)
- **Expected Impact:** 2-5x speedup for threshold-based queries (common use case)
- **Description:** Stop matching early if we can prove similarity will be below threshold
- **Implementation:**
  - Add `min_threshold` parameter to `jaro_similarity_bytes()` variants
  - Calculate minimum matches needed for threshold using Jaro formula
  - Early exit check: if remaining potential matches can't reach threshold, return None
  - Integrate with `jaro_winkler_similarity_with_threshold()` function
- **Location:** `jaro_similarity_bytes()`, `jaro_similarity_bytes_simd()`, `jaro_winkler_similarity_with_threshold()`

**Task 40: Character Frequency Pre-Filtering**
- **Priority:** High
- **Expected Impact:** 15-30% speedup for character set overlap check
- **Description:** Replace HashSet-based character set overlap check with 256-element array
- **Implementation:**
  - Create `check_character_set_overlap_fast()` using stack-allocated `[bool; 256]` array
  - Count characters in s1, check overlap with s2 in single pass
  - Zero allocation overhead, faster than HashSet for ASCII strings
- **Location:** Replace `check_character_set_overlap()` in `jaro_similarity_bytes_simd()`

#### Medium Priority Optimizations (Expected 1.2-1.5x Additional Speedup)

**Task 41: Improved Transposition Counting with SIMD**
- **Priority:** Medium
- **Expected Impact:** 10-20% speedup for transposition counting
- **Description:** Use SIMD to find next matched positions in s2_matches instead of sequential scanning
- **Implementation:**
  - Create `count_transpositions_simd_optimized()` function
  - Use SIMD to scan `s2_matches` in 32-byte chunks
  - Use `simd_eq()` and `to_bitmask()` to find next matched position
  - Replace existing transposition counting in `jaro_similarity_bytes_simd_large()`
- **Location:** `jaro_similarity_bytes_simd_large()`

**Task 42: Optimized Hash-Based Implementation**
- **Priority:** Medium
- **Expected Impact:** 10-20% speedup for hash-based path
- **Description:** Use `SmallVec` instead of `Vec<usize>` in HashMap to avoid heap allocations for small character sets
- **Implementation:**
  - Add `smallvec` dependency to `polars-ops/Cargo.toml`
  - Replace `HashMap<u8, Vec<usize>>` with `HashMap<u8, SmallVec<[usize; 4]>>`
  - Most strings have <10 unique characters, so SmallVec avoids allocations
- **Location:** `jaro_similarity_bytes_hash_based()`

**Task 43: Adaptive Algorithm Selection**
- **Priority:** Medium
- **Expected Impact:** 10-30% speedup by using optimal algorithm for each case
- **Description:** Choose algorithm variant based on string characteristics (length, character distribution, etc.)
- **Implementation:**
  - Create `jaro_similarity_bytes_adaptive()` dispatch function
  - Analyze string characteristics: unique character count, average frequency
  - Select optimal algorithm:
    - ≤64 chars: bit-parallel
    - >50 chars + high frequency: hash-based
    - Low diversity: hash-based with small maps
    - Default: SIMD path
  - Replace current dispatch logic in `jaro_similarity_bytes()`
- **Location:** `jaro_similarity_bytes()`, new helper functions for character analysis

### Phase 4 Implementation Strategy
- **Week 1:** Tasks 38-40 (High priority optimizations) - Expected 1.5-2x speedup
- **Week 2:** Tasks 41-43 (Medium priority optimizations) - Expected 1.2-1.5x additional speedup
- **Week 3:** Integration testing and benchmarking

### Optimization Success Criteria
- Levenshtein similarity: Match or exceed RapidFuzz performance (currently 8x slower on large datasets)
  - Target: 0.016-0.032s for 100K strings (currently 0.161s, RapidFuzz 0.0198s)
  - Diagonal band optimization should achieve this
  - SIMD enhancement should exceed RapidFuzz performance
- **Hamming similarity: Exceed RapidFuzz on ALL dataset sizes**
  - Small datasets (1K): Currently 1.14x slower → Target 1.2x+ faster
  - Medium/Large: Already 1.73-2.45x faster → Maintain
  - Task 35 should close the small dataset gap
- Cosine similarity: Maintain and improve current 40-50x advantage over NumPy
- **Jaro-Winkler similarity: Exceed RapidFuzz on ALL dataset sizes**
  - Small/Medium: Already 2-3x faster → Maintain
  - Large datasets (100K): Currently 1.08x slower → Target 1.3x+ faster
  - Task 36 should close the large dataset gap
- **Damerau-Levenshtein similarity: Improve from 1.90x to 3.5x+ faster than RapidFuzz**
  - Task 32 SIMD optimization should achieve this
- All optimizations maintain correctness (all tests still pass)
- Benchmark results documented and tracked

---

## Phase 5: Basic Fuzzy Join Implementation

After completing the similarity metrics (Phases 1-4), implement fuzzy join functionality that leverages these existing kernels to join DataFrames based on string similarity thresholds.

### Overview

Fuzzy joins match rows between two DataFrames where string columns are "similar enough" (above a threshold) rather than exactly equal. This is essential for:
- Entity resolution (matching customer records across systems)
- Deduplication (finding near-duplicate records)
- Data cleaning (linking messy data to reference tables)
- Record linkage across databases with inconsistent naming

### Core API Design

**Task 44: Define Fuzzy Join API and Types**
- **Priority:** High
- **Description:** Define the public API for fuzzy joins including type definitions, join arguments, and expression syntax
- **Implementation:**
  - Add `FuzzyJoinType` enum to `polars-ops/src/frame/join/args.rs`:
    ```rust
    pub enum FuzzyJoinType {
        Levenshtein,
        DamerauLevenshtein,
        JaroWinkler,
        Hamming,
    }
    ```
  - Add `FuzzyJoinArgs` struct:
    ```rust
    pub struct FuzzyJoinArgs {
        pub similarity_type: FuzzyJoinType,
        pub threshold: f32,           // Minimum similarity (0.0 to 1.0)
        pub left_on: String,          // Left column name
        pub right_on: String,         // Right column name
        pub suffix: String,           // Suffix for right columns (default: "_right")
        pub keep: FuzzyJoinKeep,      // best_match, all_matches, first_match
    }
    ```
  - Add `FuzzyJoinKeep` enum for output control
- **Test Strategy:** Compile tests for type definitions, ensure serialization works

**Task 45: Implement Core Fuzzy Join Logic**
- **Priority:** High
- **Dependencies:** Task 44
- **Description:** Implement the core fuzzy join algorithm using nested loop approach
- **Implementation:**
  - Create `polars-ops/src/frame/join/fuzzy.rs`
  - Implement `fuzzy_join_inner()` function:
    ```rust
    pub fn fuzzy_join_inner(
        left: &DataFrame,
        right: &DataFrame,
        args: FuzzyJoinArgs,
    ) -> PolarsResult<DataFrame>
    ```
  - Algorithm (O(n*m) baseline):
    1. Extract string columns from both DataFrames
    2. For each row in left, compute similarity with all rows in right
    3. Filter pairs above threshold
    4. Based on `keep` strategy, select matching rows
    5. Build result DataFrame with matched rows
  - Handle null values (null similarity = null, excluded from matches)
  - Return joined DataFrame with similarity score column
- **Test Strategy:** Unit tests with small DataFrames, validate correctness against manual calculations

**Task 46: Implement Join Type Variants**
- **Priority:** High
- **Dependencies:** Task 45
- **Description:** Implement left, right, outer, and cross fuzzy join variants
- **Implementation:**
  - `fuzzy_join_left()`: All left rows, matched right rows (nulls for non-matches)
  - `fuzzy_join_right()`: All right rows, matched left rows
  - `fuzzy_join_outer()`: All rows from both, matched where possible
  - `fuzzy_join_cross()`: Cartesian product filtered by similarity
  - Unified `fuzzy_join()` dispatcher function
- **Test Strategy:** Test each variant with known expected outputs

**Task 47: Add FunctionExpr for Fuzzy Join**
- **Priority:** High
- **Dependencies:** Task 46
- **Description:** Integrate fuzzy join into the expression system
- **Implementation:**
  - Add `FuzzyJoin` variant to `FunctionExpr` in `polars-plan`
  - Implement schema inference for fuzzy join output
  - Handle serialization/deserialization
  - Wire up in physical expression builder
- **Test Strategy:** Expression round-trip tests, lazy evaluation tests

**Task 48: DataFrame Method Interface**
- **Priority:** High
- **Dependencies:** Task 47
- **Description:** Add `fuzzy_join` method to DataFrame
- **Implementation:**
  - In `polars-lazy/src/frame/mod.rs`, add:
    ```rust
    pub fn fuzzy_join(
        self,
        other: LazyFrame,
        left_on: &str,
        right_on: &str,
        similarity: FuzzyJoinType,
        threshold: f32,
    ) -> LazyFrame
    ```
  - Support method chaining with other operations
  - Validate column types (must be String/Utf8)
  - Propagate errors for invalid configurations
- **Test Strategy:** Integration tests with lazy and eager evaluation

**Task 49: Python Bindings for Fuzzy Join**
- **Priority:** High
- **Dependencies:** Task 48
- **Description:** Expose fuzzy join to Python API
- **Implementation:**
  - In `py-polars/polars/dataframe/frame.py`, add:
    ```python
    def fuzzy_join(
        self,
        other: DataFrame,
        left_on: str,
        right_on: str,
        similarity: Literal["levenshtein", "damerau_levenshtein", "jaro_winkler", "hamming"] = "levenshtein",
        threshold: float = 0.8,
        suffix: str = "_right",
        keep: Literal["best", "all", "first"] = "best",
    ) -> DataFrame:
        """
        Join DataFrames based on string similarity.
        
        Parameters
        ----------
        other : DataFrame
            Right DataFrame to join with
        left_on : str
            Column name in left DataFrame to match on
        right_on : str  
            Column name in right DataFrame to match on
        similarity : str
            Similarity metric: "levenshtein", "damerau_levenshtein", "jaro_winkler", "hamming"
        threshold : float
            Minimum similarity score (0.0 to 1.0) for a match
        suffix : str
            Suffix to add to right DataFrame columns
        keep : str
            "best" (highest similarity), "all" (all above threshold), "first" (first match)
            
        Returns
        -------
        DataFrame
            Joined DataFrame with similarity scores
        """
    ```
  - Add to LazyFrame as well
  - Type hints and comprehensive docstrings
- **Test Strategy:** Python unit tests, validate against Rust implementation

**Task 50: Fuzzy Join Testing Suite**
- **Priority:** High
- **Dependencies:** Tasks 45-49
- **Description:** Comprehensive test suite for fuzzy join functionality
- **Implementation:**
  - Create `crates/polars-ops/tests/fuzzy_join.rs`
  - Create `py-polars/tests/unit/operations/test_fuzzy_join.py`
  - Test cases:
    - Basic inner/left/right/outer joins
    - All similarity metrics
    - Various threshold values
    - Null handling
    - Empty DataFrames
    - Single row DataFrames
    - Large DataFrames (performance)
    - Unicode strings
    - Edge cases (identical strings, completely different strings)
  - Validate against manual calculations
- **Test Strategy:** All tests pass, edge cases covered

**Task 51: Fuzzy Join Documentation**
- **Priority:** Medium
- **Dependencies:** Task 50
- **Description:** Document fuzzy join functionality with examples
- **Implementation:**
  - Rust docstrings with algorithm description
  - Python docstrings with usage examples
  - User guide section with:
    - When to use fuzzy joins
    - Choosing similarity metrics
    - Threshold selection guidelines
    - Performance considerations
  - Example notebooks:
    - Entity resolution example
    - Deduplication workflow
    - Data cleaning pipeline
- **Test Strategy:** Documentation review, examples run successfully

### Phase 5 Success Criteria
- Fuzzy join works with all 4 string similarity metrics
- Supports inner, left, right, outer join types
- Python API: `df.fuzzy_join(other, "name", "company_name", similarity="jaro_winkler", threshold=0.85)`
- All tests pass
- Documentation complete with examples

---

## Phase 6: Optimized Fuzzy Join Implementation

After basic fuzzy join is working, optimize for performance to handle large datasets efficiently.

### Overview

The basic O(n*m) fuzzy join becomes prohibitively slow for large datasets. This phase implements algorithmic optimizations to reduce comparisons and parallelize execution.

### Blocking and Candidate Generation

**Task 52: Implement Blocking Strategy**
- **Priority:** High
- **Dependencies:** Task 50
- **Description:** Reduce comparisons using blocking (candidate generation)
- **Implementation:**
  - Create `FuzzyJoinBlocker` trait for pluggable blocking strategies
  - Implement `FirstNCharsBlocker`:
    - Group strings by first N characters
    - Only compare within same block
    - Configurable N (default: 3)
  - Implement `NGramBlocker`:
    - Generate n-grams for each string
    - Use inverted index to find candidate pairs
    - Only compare pairs sharing at least one n-gram
  - Implement `LengthBlocker`:
    - Group by string length buckets
    - Only compare strings within length difference threshold
  - Add `blocking` parameter to `FuzzyJoinArgs`
- **Test Strategy:** Verify blocking doesn't miss valid matches, measure reduction in comparisons

**Task 53: Implement Sorted Neighborhood Method**
- **Priority:** Medium
- **Dependencies:** Task 52
- **Description:** Sort-based blocking for ordered string comparison
- **Implementation:**
  - Sort both columns by string value
  - Use sliding window to compare nearby strings
  - Window size configurable (default: 10)
  - More efficient for already-sorted or nearly-sorted data
  - Implement `SortedNeighborhoodBlocker`
- **Test Strategy:** Compare results with full scan, measure speedup

**Task 54: Multi-Column Blocking**
- **Priority:** Medium
- **Dependencies:** Task 52
- **Description:** Support blocking on multiple columns
- **Implementation:**
  - Allow multiple blocking columns
  - Combine blocking keys (e.g., first letter + state)
  - Union or intersection of candidate pairs
  - Add `blocking_on: Vec<String>` to args
- **Test Strategy:** Multi-column blocking produces correct results

### Parallel Execution

**Task 55: Parallel Fuzzy Join with Rayon**
- **Priority:** High
- **Dependencies:** Task 52
- **Description:** Parallelize fuzzy join across CPU cores
- **Implementation:**
  - Partition left DataFrame into chunks
  - Process chunks in parallel using Rayon
  - Each thread computes matches for its chunk
  - Merge results while maintaining row order
  - Thread-local similarity computation (no contention)
  - Configurable parallelism (num_threads parameter)
- **Test Strategy:** Results match single-threaded, measure multi-core speedup

**Task 56: Batch Similarity Computation**
- **Priority:** High
- **Dependencies:** Task 55
- **Description:** Compute similarities in batches for better cache utilization
- **Implementation:**
  - Process blocks of (left_rows × right_rows) at a time
  - Optimize memory access patterns
  - Reuse buffers across batch computations
  - Tune batch size for cache efficiency (default: 1024)
- **Test Strategy:** Benchmark batch vs row-by-row, measure cache efficiency

### Index-Based Optimization

**Task 57: Implement Similarity Index**
- **Priority:** Medium
- **Dependencies:** Task 52
- **Description:** Pre-compute index structure for faster lookups
- **Implementation:**
  - Create `SimilarityIndex` struct
  - Build inverted index of n-grams → row IDs
  - For each query string:
    - Generate n-grams
    - Look up candidate row IDs
    - Score only candidates
  - Support incremental index updates
  - Persist index for reuse
- **Test Strategy:** Index produces same results as full scan, faster lookup

**Task 58: BK-Tree for Edit Distance**
- **Priority:** Low
- **Dependencies:** Task 57
- **Description:** Implement BK-tree for efficient edit distance queries
- **Implementation:**
  - Build BK-tree from right DataFrame strings
  - Query tree with threshold for candidates
  - Prunes search space using triangle inequality
  - Only applicable to Levenshtein/Damerau-Levenshtein
  - Trade-off: build time vs query time
- **Test Strategy:** BK-tree produces correct candidates, measure query speedup

**Task 64: LSH (Locality Sensitive Hashing) Blocking Strategy**
- **Priority:** High
- **Dependencies:** Task 52
- **Description:** Implement Locality Sensitive Hashing for approximate nearest neighbor blocking
- **Implementation:**
  - Create `LSHBlocker` struct implementing `FuzzyJoinBlocker` trait
  - Implement MinHash LSH for Jaccard similarity estimation:
    - Generate shingles (character n-grams) from strings
    - Apply multiple hash functions to create signature
    - Band the signature into b bands of r rows
    - Hash each band to bucket - strings in same bucket are candidates
  - Implement SimHash LSH for cosine/angular similarity:
    - Generate character-level feature vectors
    - Apply random hyperplane hashing
    - Strings with same hash bits are candidates
  - Configurable parameters:
    - `num_hashes`: Number of hash functions (default: 100)
    - `num_bands`: Number of bands for banding (default: 20)
    - `shingle_size`: Size of character shingles (default: 3)
  - Add `BlockingStrategy::LSH { num_hashes, num_bands, shingle_size }` variant
  - Support both MinHash and SimHash variants
  - Tune parameters for target similarity threshold using formulas:
    - Probability of becoming candidate: 1 - (1 - s^r)^b where s is similarity
- **Expected Impact:** 
  - Sub-linear candidate generation O(n) instead of O(n*m)
  - 95-99% reduction in comparisons for large datasets
  - Especially effective for 10K+ rows
- **Test Strategy:** 
  - Verify LSH produces candidates with high recall (>95%)
  - Measure precision/recall tradeoff at different parameter settings
  - Benchmark against other blocking strategies on large datasets

**Task 65: Memory-Efficient Batch Processing for Large Datasets**
- **Priority:** High
- **Dependencies:** Task 56
- **Description:** Implement streaming batch processing to handle datasets larger than memory
- **Implementation:**
  - Create `BatchedFuzzyJoin` struct for memory-efficient processing:
    ```rust
    pub struct BatchedFuzzyJoin {
        batch_size: usize,        // Rows per batch (default: 10000)
        memory_limit_mb: usize,   // Max memory usage (default: 1024)
        streaming_mode: bool,     // Process without loading all data
    }
    ```
  - Implement chunked processing pipeline:
    1. Split left DataFrame into batches
    2. For each left batch:
       a. Build temporary index for right DataFrame (or iterate in batches)
       b. Compute matches for current batch
       c. Yield results, release memory
    3. Merge results maintaining order
  - Memory-aware batch sizing:
    - Estimate memory per row based on string lengths
    - Dynamically adjust batch size to stay within limit
  - Streaming output mode:
    - Return iterator instead of collecting all results
    - Enable processing of unbounded data streams
  - Add Python API:
    ```python
    df.fuzzy_join(
        other,
        left_on="name", right_on="company",
        batch_size=10000,
        memory_limit_mb=1024,
        streaming=True,  # Returns iterator
    )
    ```
- **Expected Impact:**
  - Enable fuzzy joins on datasets 10x larger than RAM
  - Predictable memory usage regardless of input size
  - 10-30% overhead for batching vs in-memory
- **Test Strategy:**
  - Process 10M row dataset with 2GB memory limit
  - Verify results match non-batched processing
  - Measure memory usage stays within limits

**Task 66: Progressive Batch Processing with Early Results**
- **Priority:** Medium
- **Dependencies:** Task 65
- **Description:** Return partial results as batches complete for faster time-to-first-result
- **Implementation:**
  - Implement `FuzzyJoinIterator` for streaming results:
    ```rust
    pub struct FuzzyJoinIterator {
        left_batches: BatchIterator,
        right_df: DataFrame,
        args: FuzzyJoinArgs,
        current_batch: usize,
    }
    
    impl Iterator for FuzzyJoinIterator {
        type Item = PolarsResult<DataFrame>;
        fn next(&mut self) -> Option<Self::Item>;
    }
    ```
  - Progressive result delivery:
    - Yield results as each batch completes
    - Allow early termination after N results
    - Support `take(n)` to limit results
  - Priority ordering for `keep="best"`:
    - Use heap to track best matches across batches
    - Optionally sort candidates by blocking score first
  - Add callback API for progress reporting:
    ```python
    def on_progress(batch_num, total_batches, matches_found):
        print(f"Batch {batch_num}/{total_batches}: {matches_found} matches")
    
    df.fuzzy_join(other, ..., on_progress=on_progress)
    ```
- **Expected Impact:**
  - Time to first result: <1s for any dataset size
  - Memory usage: O(batch_size) instead of O(n)
  - Better user experience for interactive use
- **Test Strategy:**
  - Verify streaming results match batch results
  - Measure time-to-first-result improvement
  - Test early termination correctness

**Task 67: Batch-Aware Blocking Integration**
- **Priority:** Medium
- **Dependencies:** Task 64, Task 65
- **Description:** Optimize blocking strategies to work efficiently with batch processing
- **Implementation:**
  - Persistent blocking index:
    - Build blocking index once for right DataFrame
    - Reuse across all left batches
    - Memory-map large indices to disk
  - Incremental blocking for streaming:
    - Update blocking index as new data arrives
    - Support append-only index updates
  - Batch-aware candidate generation:
    - Generate candidates per batch efficiently
    - Avoid redundant index lookups
  - LSH with batching:
    - Build LSH index for right DataFrame once
    - Query index for each left batch
    - Support disk-backed LSH index for large datasets
  - Add index persistence API:
    ```python
    # Build and save index
    index = other["company"].str.build_blocking_index(
        strategy="lsh",
        num_hashes=100,
    )
    index.save("company_lsh_index.bin")
    
    # Load and use in batched join
    index = pl.load_blocking_index("company_lsh_index.bin")
    df.fuzzy_join(other, ..., blocking_index=index, batch_size=10000)
    ```
- **Expected Impact:**
  - Amortize index building cost across batches
  - Enable batch processing with O(1) blocking lookups
  - Support datasets 100x larger than RAM with blocking
- **Test Strategy:**
  - Verify index persistence correctness
  - Benchmark batched blocking vs rebuilding per batch
  - Test memory-mapped index performance

### Threshold Optimization

**Task 59: Early Termination in Batch Joins**
- **Priority:** High
- **Dependencies:** Task 56
- **Description:** Stop computing similarity once threshold is determined
- **Implementation:**
  - Use bounded distance algorithms from Phase 2
  - For `keep="best"`, track current best and prune
  - For `keep="first"`, stop after first match
  - Skip remaining comparisons when threshold can't be met
- **Test Strategy:** Results match full computation, measure comparisons saved

**Task 60: Adaptive Threshold Estimation**
- **Priority:** Low
- **Dependencies:** Task 59
- **Description:** Automatically suggest optimal threshold
- **Implementation:**
  - Sample pairs from both DataFrames
  - Compute similarity distribution
  - Suggest threshold based on distribution (e.g., 90th percentile)
  - Provide confidence metrics
  - Add `estimate_threshold()` utility function
- **Test Strategy:** Threshold suggestions produce reasonable match rates

### Python API Enhancements

**Task 61: Python Fuzzy Join Optimizations**
- **Priority:** Medium
- **Dependencies:** Tasks 52-60
- **Description:** Expose optimization parameters to Python
- **Implementation:**
  - Add blocking parameters to Python API
  - Add parallel execution control
  - Add index building/loading methods
  - Performance hints in docstrings
  - Example:
    ```python
    df.fuzzy_join(
        other,
        left_on="name",
        right_on="company",
        similarity="jaro_winkler",
        threshold=0.85,
        blocking="first_chars",
        blocking_chars=3,
        parallel=True,
        n_threads=4,
    )
    ```
- **Test Strategy:** Python tests for all optimization options

**Task 62: Fuzzy Join Performance Benchmarks**
- **Priority:** Medium
- **Dependencies:** Task 61
- **Description:** Comprehensive benchmarking suite
- **Implementation:**
  - Create benchmark datasets (1K, 10K, 100K, 1M rows)
  - Benchmark all similarity metrics
  - Benchmark blocking strategies
  - Benchmark parallel scaling
  - Compare with alternatives (recordlinkage, dedupe libraries)
  - Document performance characteristics
- **Test Strategy:** Benchmarks run successfully, results documented

**Task 63: Fuzzy Join Advanced Documentation**
- **Priority:** Medium
- **Dependencies:** Task 62
- **Description:** Performance tuning guide and advanced usage
- **Implementation:**
  - Performance tuning guide:
    - When to use blocking
    - Choosing blocking strategy
    - Parallel execution guidelines
    - Memory considerations
  - Advanced examples:
    - Large-scale entity resolution
    - Incremental matching
    - Multi-pass deduplication
  - API reference for all parameters
- **Test Strategy:** Documentation review, examples verified

### Phase 6 Success Criteria
- 10-100x speedup over baseline for large datasets (100K+ rows)
- Blocking reduces comparisons by 90%+ while maintaining accuracy
- LSH blocking achieves 95-99% comparison reduction on 10K+ rows
- Batch processing enables fuzzy joins on datasets 10x larger than RAM
- Parallel execution scales linearly with cores
- Python API exposes all optimization options
- Benchmarks show competitive performance with specialized libraries
- Documentation covers all optimization strategies

### Phase 5 & 6 Dependencies
- Phase 5 (Tasks 44-51) depends on Phase 1-4 completion (similarity metrics)
- Task 44-46 can proceed in parallel
- Task 47-49 depend on core logic (Task 45-46)
- Task 50-51 depend on all Phase 5 tasks
- Phase 6 (Tasks 52-67) depends on Phase 5 completion
- Tasks 52-54 (blocking) can proceed in parallel
- Task 64 (LSH blocking) depends on Task 52 (blocking infrastructure)
- Tasks 55-56 (parallelization) can proceed in parallel with blocking
- Tasks 57-58 (indexing) depend on blocking
- Tasks 59-60 (threshold optimization) can proceed independently
- Tasks 65-66 (batch processing) depend on Task 56 (batch similarity)
- Task 67 (batch-aware blocking) depends on Tasks 64 and 65
- Tasks 61-63 depend on all optimization tasks including 64-67

---

## Phase 7: Advanced Blocking & Automatic Optimization

After Phase 6's basic blocking strategies, this phase implements intelligent blocking that maximizes recall while maintaining performance through adaptive algorithms, automatic strategy selection, and approximate nearest neighbor pre-filtering.

### Overview

Current blocking strategies (FirstNChars, NGram, Length, SortedNeighborhood, LSH) all use **exact matching** for blocking keys. This means:
- Two strings must share an exact prefix/n-gram/length/hash bucket to be compared
- Valid matches can be missed if blocking keys don't match exactly
- Example: "John" and "Jon" won't be compared if using FirstChars(3) blocking

Phase 7 addresses this by:
1. **Adaptive Blocking**: Use fuzzy matching for blocking keys (approximate blocking key matching)
2. **Automatic Strategy Selection**: Intelligently choose optimal blocking strategy based on data characteristics
3. **Approximate Nearest Neighbor Pre-filtering**: Use ANN for very large datasets (1M+ rows)
4. **Aggressive Default Blocking**: Enable blocking by default with optimal parameters

### Adaptive Blocking Strategy

**Task 68: Implement Adaptive Blocking with Fuzzy Matching**

- **Priority:** High
- **Dependencies:** Task 52 (blocking infrastructure)
- **Description:** Implement adaptive blocking that uses fuzzy matching for blocking keys instead of exact matching
- **Implementation:**
  - Create `AdaptiveBlocker` struct that wraps existing blocking strategies
  - Add `max_key_distance` parameter (default: 1 edit distance for blocking keys)
  - For FirstNChars: Allow approximate prefix matches (e.g., "Joh" matches "Jon" with distance 1)
  - For NGram: Allow approximate n-gram matches (e.g., "abc" matches "abd" with distance 1)
  - For Length: Already approximate (length difference), enhance with fuzzy length matching
  - Expand blocking keys: Generate all keys within edit distance threshold
  - Example: FirstNChars(3) with max_key_distance=1:
    - "John" → keys: ["Joh", "Jon", "Jhn", "ohn"] (original + 1-edit variants)
    - "Jon" → keys: ["Jon", "Joh", "Jhn", "oon"] (original + 1-edit variants)
    - Both strings now share "Joh" and "Jon" keys → will be compared
  - Add `BlockingStrategy::Adaptive { base_strategy, max_key_distance }` variant
  - Integrate with existing blocking infrastructure
- **Expected Impact:**
  - 5-15% improvement in recall (fewer missed matches)
  - 10-30% increase in candidate pairs (more comparisons, but still much less than full scan)
  - Maintains 80-95% comparison reduction vs full scan
- **Test Strategy:** Verify recall improvement on datasets with typos/errors in prefixes, measure precision/recall tradeoff
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs`

### Automatic Blocking Strategy Selection

**Task 69: Implement Automatic Blocking Strategy Selection**

- **Priority:** High
- **Dependencies:** Task 68
- **Description:** Automatically select optimal blocking strategy based on data characteristics
- **Implementation:**
  - Create `BlockingStrategySelector` that analyzes:
    - Dataset size (n, m)
    - String length distribution (mean, std, min, max)
    - Character diversity (unique characters, entropy)
    - Data distribution (sorted, random, clustered)
    - Expected match rate (from threshold)
  - Selection logic:
    - **Small datasets (< 1K rows):** No blocking (full scan faster)
    - **Medium datasets (1K-10K rows):** FirstChars(3) or NGram(3) based on prefix stability
    - **Large datasets (10K-100K rows):** NGram(3) or SortedNeighborhood(10)
    - **Very large datasets (100K+ rows):** LSH with auto-tuned parameters
    - **Sorted data:** SortedNeighborhood (detect via prefix distribution)
    - **High character diversity:** NGram (more robust to variations)
    - **Low character diversity:** FirstChars (fewer collisions)
  - Add `BlockingStrategy::Auto` variant that triggers automatic selection
  - Cache selection results for repeated joins on same columns
  - Provide `recommend_blocking_strategy()` utility function
- **Expected Impact:**
  - Users don't need to manually tune blocking parameters
  - Optimal strategy selected automatically for each dataset
  - 20-50% better performance vs manual strategy selection
- **Test Strategy:** Test on various dataset characteristics, verify optimal strategy selected, benchmark performance
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs`, new `fuzzy_blocking_auto.rs`

### Approximate Nearest Neighbor Pre-filtering

**Task 70: Implement Approximate Nearest Neighbor Pre-filtering for Large Datasets**

- **Priority:** Medium
- **Dependencies:** Task 64 (LSH), Task 69
- **Description:** Use approximate nearest neighbor search for very large datasets (1M+ rows) to pre-filter candidates before exact similarity computation
- **Implementation:**
  - Create `ANNPreFilter` struct that wraps LSH or other ANN algorithms
  - Two-stage filtering:
    1. **ANN Stage**: Use LSH/ANN to find approximate nearest neighbors (fast, approximate)
    2. **Exact Stage**: Compute exact similarity only for ANN candidates (slower, exact)
  - For datasets > 1M rows:
    - Build ANN index (LSH, HNSW, or FAISS-style)
    - Query ANN index for each left string → get top-K approximate neighbors
    - Only compute exact similarity for ANN candidates
    - Reduces comparisons from O(n*m) to O(n*log(m) + n*K) where K << m
  - Integrate with existing blocking:
    - Use ANN as pre-filter, then apply blocking on ANN candidates
    - Or use blocking first, then ANN on blocked candidates
  - Configurable K (number of approximate neighbors to retrieve)
  - Add `BlockingStrategy::ANN { k, lsh_config }` variant
  - Support multiple ANN backends:
    - LSH (already implemented)
    - HNSW (Hierarchical Navigable Small World) - optional
    - FAISS integration - optional
- **Expected Impact:**
  - Enable fuzzy joins on billion-scale datasets (1M+ rows)
  - 100-1000x reduction in comparisons for very large datasets
  - Sub-second query time for 1M+ row datasets
- **Test Strategy:** Test on large synthetic datasets (1M+ rows), verify recall > 95%, benchmark query time
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs`, new `fuzzy_ann.rs`

### Aggressive Default Blocking

**Task 71: Use Existing Blocking Strategies More Aggressively by Default**

- **Priority:** Medium
- **Dependencies:** Task 69
- **Description:** Enable blocking by default with optimal parameters, make it easier to use
- **Implementation:**
  - Change default `BlockingStrategy` from `None` to `Auto` in `FuzzyJoinArgs`
  - Auto-enable blocking when:
    - Dataset size > 100 rows (left or right)
    - Expected comparisons > 10,000 (n * m > 10,000)
  - Provide smart defaults for blocking parameters:
    - FirstChars: n = 3 (good balance of precision/recall)
    - NGram: n = 3 (trigrams are standard)
    - Length: max_diff = 2 (reasonable for most use cases)
    - SortedNeighborhood: window = 10 (good for large datasets)
    - LSH: Auto-tune based on dataset size and threshold
  - Add `auto_blocking` parameter (default: true) to allow disabling
  - Update Python API to enable blocking by default
  - Add warnings when blocking is disabled for large datasets
- **Expected Impact:**
  - Users get optimal performance without manual configuration
  - 90%+ of users benefit from automatic blocking
  - Reduced support burden (fewer "why is my join slow?" questions)
- **Test Strategy:** Test default behavior on various dataset sizes, verify blocking enabled appropriately
- **Code Location:** `polars/crates/polars-ops/src/frame/join/args.rs`, `fuzzy.rs`

### Additional Optimizations

**Task 72: Additional Performance Optimizations**

- **Priority:** Low
- **Dependencies:** Tasks 68-71
- **Description:** Additional optimizations identified during Phase 7 implementation
- **Potential Optimizations:**
  - Blocking key caching: Cache generated blocking keys across multiple joins
  - Parallel blocking key generation: Use Rayon for generating blocking keys
  - Blocking index persistence: Save/load blocking indices for repeated joins
  - Multi-threaded candidate generation: Parallelize candidate pair generation
  - Blocking strategy combination: Better support for combining multiple strategies
- **Implementation:** To be determined based on profiling results
- **Test Strategy:** Profile and identify bottlenecks, implement targeted optimizations

### Phase 7 Success Criteria
- Adaptive blocking improves recall by 5-15% while maintaining 80-95% comparison reduction
- Automatic strategy selection chooses optimal strategy for 90%+ of datasets
- ANN pre-filtering enables fuzzy joins on 1M+ row datasets with sub-second query time
- Blocking enabled by default improves performance for 90%+ of users
- All optimizations maintain correctness (all tests still pass)
- Performance benchmarks show improvement over Phase 6

### Phase 7 Dependencies
- Phase 7 (Tasks 68-72) depends on Phase 6 completion (Tasks 52-67)
- Task 68 (Adaptive Blocking) depends on Task 52 (blocking infrastructure)
- Task 69 (Auto Selection) depends on Task 68 (needs adaptive blocking for analysis)
- Task 70 (ANN Pre-filtering) depends on Task 64 (LSH) and Task 69 (auto selection)
- Task 71 (Aggressive Defaults) depends on Task 69 (auto selection)
- Task 72 (Additional Optimizations) depends on Tasks 68-71

---

## Phase 8: Sparse Vector Blocking with TF-IDF and Cosine Similarity

After Phase 7's LSH-based ANN approach, this phase implements an alternative blocking strategy using n-gram sparse vectors with TF-IDF weighting and cosine similarity thresholding. This approach is used by `pl-fuzzy-frame-match` (via `polars-simed`) and provides better recall and more intuitive parameter tuning compared to LSH.

### Overview

Current LSH (MinHash/SimHash) approach has limitations:
- **Probabilistic**: 80-95% recall with potential false negatives
- **Complex parameter tuning**: `num_hashes`, `num_bands` require understanding of banding probability formula
- **Indirect similarity**: Estimates Jaccard similarity via signature comparison, not directly related to edit distance

The Sparse Vector + Cosine Similarity approach addresses these:
1. **Deterministic threshold**: Set minimum cosine similarity directly (0.0-1.0)
2. **Higher recall**: Weighted matching captures more valid candidates
3. **Better for edit distance**: TF-IDF weights penalize common n-grams, reward distinctive ones
4. **Simpler tuning**: Only n-gram size and minimum cosine similarity to configure

### How It Works

1. **N-gram Tokenization**: Break strings into character n-grams (e.g., trigrams)
2. **TF-IDF Weighting**: Weight n-grams by frequency (TF) and rarity (IDF)
3. **Sparse Vector Representation**: Each string becomes a sparse vector of weighted n-grams
4. **Inverted Index**: Build index mapping n-grams to (row_id, weight) pairs
5. **Cosine Similarity via Dot Product**: Use inverted index for efficient sparse dot product
6. **Threshold Filtering**: Return only candidate pairs above minimum cosine similarity

### Core Implementation

**Task 73: Implement TF-IDF N-gram Sparse Vector Blocker**

- **Priority:** Critical
- **Dependencies:** Task 52 (blocking infrastructure), Task 64 (NGramBlocker reference)
- **Description:** Implement a blocking strategy using TF-IDF weighted n-gram sparse vectors with cosine similarity thresholding
- **Implementation:**
  - Create `SparseVectorBlocker` struct implementing `FuzzyJoinBlocker` trait:
    ```rust
    pub struct SparseVectorBlocker {
        ngram_size: usize,           // Size of character n-grams (default: 3)
        min_cosine_similarity: f32,  // Minimum cosine similarity threshold (default: 0.3)
        idf: HashMap<String, f32>,   // Precomputed IDF values
    }
    ```
  - Implement `build_idf()` to compute IDF from both columns:
    - Count documents containing each n-gram
    - IDF = ln(N / df) where N is total documents, df is document frequency
  - Implement `to_sparse_vector()` to convert string to TF-IDF weighted sparse vector:
    - Generate n-grams with term frequency counts
    - Apply TF-IDF weighting: weight = tf * idf
    - L2 normalize the vector for cosine similarity
  - Implement efficient `generate_candidates()` using inverted index:
    - Build inverted index: n-gram → Vec<(left_idx, weight)>
    - For each right string, accumulate dot products via index lookups
    - Filter by min_cosine_similarity threshold
  - Add `BlockingStrategy::SparseVector { ngram_size, min_cosine_similarity }` variant
- **Expected Impact:**
  - 90-98% recall (higher than LSH's 80-95%)
  - Deterministic results (no probabilistic false negatives)
  - Better candidate quality (weighted matching)
  - Simpler parameter tuning
- **Test Strategy:** Compare recall/precision with LSH, benchmark on various dataset sizes
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_sparse_vector.rs`

**Task 74: Optimize Sparse Vector Operations**

- **Priority:** High
- **Dependencies:** Task 73
- **Description:** Optimize sparse vector operations for performance
- **Implementation:**
  - **Subtask 74.1: Efficient IDF Computation**
    - Use parallel iteration with Rayon for IDF computation
    - Compute IDF incrementally during index building
    - Cache IDF values for repeated joins on same columns
  - **Subtask 74.2: SIMD-Accelerated Dot Product Accumulation**
    - Use SIMD for parallel accumulation when multiple n-grams match
    - Vectorize the score aggregation loop
  - **Subtask 74.3: Memory-Efficient Inverted Index**
    - Use `SmallVec<[(usize, f32); 8]>` for common n-grams
    - Compress indices for memory efficiency
    - Consider arena allocation for index entries
  - **Subtask 74.4: Parallel Candidate Generation**
    - Process right strings in parallel using Rayon
    - Thread-local score accumulators to avoid contention
    - Merge candidate lists efficiently
  - **Subtask 74.5: Early Termination Optimizations**
    - Track maximum possible remaining score
    - Skip strings that can't reach threshold
    - Use sorted n-gram iteration for better pruning
- **Expected Impact:**
  - 2-3x speedup over naive implementation
  - Memory usage comparable to LSH
  - Linear scaling with cores
- **Test Strategy:** Benchmark against LSH, profile hotspots, measure memory usage
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_sparse_vector.rs`

**Task 75: Integrate BK-Tree with Sparse Vector Blocking**

- **Priority:** High
- **Dependencies:** Task 58 (BK-Tree), Task 73
- **Description:** Create a hybrid blocking strategy combining BK-Tree for small edit distances with sparse vectors for general matching
- **Implementation:**
  - Create `HybridBlocker` that combines strategies:
    ```rust
    pub struct HybridBlocker {
        bk_tree: Option<BKTree>,           // For Levenshtein with high thresholds
        sparse_blocker: SparseVectorBlocker, // For general matching
        use_bktree_threshold: f32,         // Use BK-Tree when threshold >= this (default: 0.8)
    }
    ```
  - Selection logic:
    - Levenshtein/Damerau-Levenshtein + threshold >= 0.8 → BK-Tree (100% recall, fast for small edit distance)
    - Jaro-Winkler/Hamming or lower thresholds → Sparse Vector (weighted matching)
  - Add `BlockingStrategy::Hybrid { use_bktree_threshold }` variant
  - Auto-select in `BlockingStrategySelector` based on similarity metric and threshold
- **Expected Impact:**
  - 100% recall for high-threshold edit distance queries
  - Optimal strategy for each similarity metric
  - Best of both worlds: BK-Tree precision + sparse vector flexibility
- **Test Strategy:** Compare with standalone strategies, verify optimal selection
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs`

**Task 76: Replace LSH with Sparse Vector in Auto-Selector**

- **Priority:** High
- **Dependencies:** Task 73, Task 74, Task 69
- **Description:** Update the automatic blocking strategy selector to prefer sparse vector over LSH
- **Implementation:**
  - Update `BlockingStrategySelector::select_strategy()`:
    ```rust
    // Previous (Phase 7):
    // 100K-1M comparisons → LSH
    // 1M+ comparisons → ANN (LSH-based)
    
    // New (Phase 8):
    // 10K-100K comparisons → SparseVector(min_cosine=0.3)
    // 100K-1M comparisons → SparseVector(min_cosine=0.5)
    // 1M+ comparisons → ANN with SparseVector backend
    ```
  - Update `ANNPreFilter` to optionally use sparse vectors instead of LSH:
    ```rust
    pub enum ANNBackend {
        LSH(LSHBlocker),           // Original LSH backend (deprecated)
        SparseVector(SparseVectorBlocker),  // New preferred backend
    }
    ```
  - Keep LSH as fallback option for users who explicitly request it
  - Update `BlockingStrategy::ANN` to support backend selection
  - Update default parameters in `FuzzyJoinArgs`
- **Expected Impact:**
  - Better default performance for most use cases
  - Close 28% performance gap with pl-fuzzy-frame-match
  - Simpler parameter tuning for users
- **Test Strategy:** Benchmark old vs new auto-selection, verify improved performance
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs`, `args.rs`

**Task 77: Add Cosine Similarity Threshold Parameter to Python API**

- **Priority:** Medium
- **Dependencies:** Task 76
- **Description:** Expose sparse vector blocking parameters to Python API
- **Implementation:**
  - Update Python `fuzzy_join()` signature:
    ```python
    def fuzzy_join(
        self,
        other: DataFrame,
        left_on: str,
        right_on: str,
        similarity: str = "levenshtein",
        threshold: float = 0.8,
        # New parameters for sparse vector blocking:
        blocking: str = "auto",  # "auto", "sparse_vector", "lsh", "ngram", "first_chars", "none"
        blocking_ngram_size: int = 3,
        blocking_min_cosine: float = 0.3,  # Minimum cosine similarity for blocking
        # Existing parameters:
        keep: str = "best",
        suffix: str = "_right",
        parallel: bool = True,
        **kwargs,
    ) -> DataFrame:
    ```
  - Add documentation explaining blocking strategies:
    - `"auto"`: Automatically select best strategy (recommended)
    - `"sparse_vector"`: TF-IDF weighted n-gram sparse vectors with cosine similarity
    - `"lsh"`: Locality Sensitive Hashing (legacy)
    - `"ngram"`: Simple n-gram blocking (any shared n-gram = candidate)
    - `"first_chars"`: First N characters blocking
    - `"none"`: No blocking (full scan)
  - Add `get_blocking_strategies()` function to list available strategies
  - Update type hints and docstrings
- **Expected Impact:**
  - Users can fine-tune blocking behavior
  - Clear documentation of blocking options
  - Backwards compatible API
- **Test Strategy:** Python tests for all blocking options, verify parameter validation
- **Code Location:** `polars/py-polars/polars/dataframe/frame.py`

**Task 78: Benchmark Sparse Vector vs LSH vs pl-fuzzy-frame-match**

- **Priority:** High
- **Dependencies:** Task 76
- **Description:** Comprehensive benchmarking to validate sparse vector approach
- **Implementation:**
  - Create benchmark script comparing:
    - Sparse Vector blocking (new)
    - LSH blocking (current)
    - NGram blocking (simple baseline)
    - pl-fuzzy-frame-match (external reference)
  - Benchmark dimensions:
    - Dataset sizes: 1K, 10K, 100K, 1M rows
    - Similarity metrics: Levenshtein, Jaro-Winkler, Damerau-Levenshtein
    - Thresholds: 0.5, 0.7, 0.8, 0.9
  - Metrics to measure:
    - Candidate generation time
    - Total fuzzy join time
    - Recall (% of true matches found)
    - Precision (% of candidates that are true matches)
    - Memory usage
    - Comparisons per second
  - Generate comparison report and visualizations
  - Document findings in `SPARSE_VECTOR_BENCHMARK.md`
- **Expected Impact:**
  - Validate sparse vector approach matches or exceeds pl-fuzzy-frame-match
  - Identify optimal parameters for different use cases
  - Provide evidence for default strategy selection
- **Test Strategy:** Run benchmarks on standardized datasets, verify reproducibility
- **Code Location:** `benchmark_sparse_vector.py`, `SPARSE_VECTOR_BENCHMARK.md`

### Advanced Optimizations

**Task 79: Adaptive Cosine Threshold Based on String Length**

- **Priority:** Medium
- **Dependencies:** Task 73
- **Description:** Automatically adjust cosine similarity threshold based on string length characteristics
- **Implementation:**
  - Longer strings have more n-grams, so cosine similarity behaves differently
  - Implement adaptive threshold:
    ```rust
    fn adaptive_threshold(base_threshold: f32, avg_length: f32) -> f32 {
        // Shorter strings need lower threshold (fewer n-grams to match)
        // Longer strings can use higher threshold (more n-grams to distinguish)
        let length_factor = (avg_length / 10.0).min(1.5).max(0.5);
        base_threshold * length_factor
    }
    ```
  - Integrate with `SparseVectorBlocker`:
    - Compute average string length from columns
    - Adjust threshold automatically
    - Allow user override
  - Add `adaptive_threshold: bool` parameter (default: true)
- **Expected Impact:**
  - Better recall for short strings
  - Better precision for long strings
  - More consistent behavior across different datasets
- **Test Strategy:** Test on datasets with varying string lengths, verify improved recall/precision
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_sparse_vector.rs`

**Task 80: Streaming Sparse Vector Index for Large Datasets**

- **Priority:** Low
- **Dependencies:** Task 73, Task 65 (batch processing)
- **Description:** Support streaming/batched sparse vector blocking for datasets larger than memory
- **Implementation:**
  - Implement `StreamingSparseVectorBlocker`:
    - Build IDF from sample of data (or full scan if memory allows)
    - Process left DataFrame in batches
    - For each batch, query inverted index and accumulate scores
    - Yield candidates as batches complete
  - Memory-efficient inverted index:
    - Disk-backed index for very large right DataFrames
    - Memory-mapped file for index storage
    - Lazy loading of index entries
  - Integrate with existing batch processing infrastructure (Task 65)
  - Add `streaming: bool` parameter to `SparseVectorBlocker`
- **Expected Impact:**
  - Enable sparse vector blocking on datasets 10x larger than RAM
  - Predictable memory usage regardless of input size
  - Time-to-first-result improvement for large datasets
- **Test Strategy:** Process large dataset with memory limit, verify results match non-streaming
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_sparse_vector.rs`

### Phase 8 Success Criteria
- Sparse vector blocking achieves 90-98% recall (vs LSH's 80-95%)
- Performance matches or exceeds pl-fuzzy-frame-match on all benchmark sizes
- Close the 28% performance gap observed at 25M comparisons for Levenshtein
- Auto-selector prefers sparse vector for medium-large datasets (10K-1M comparisons)
- BK-Tree + Sparse Vector hybrid achieves 100% recall for high-threshold edit distance
- Python API exposes all new blocking parameters
- Comprehensive benchmarks document performance characteristics
- All existing tests still pass

### Phase 8 Dependencies
- Phase 8 (Tasks 73-80) depends on Phase 6-7 completion (Tasks 52-72)
- Task 73 (Core Sparse Vector) depends on Task 52 (blocking infrastructure)
- Task 74 (Optimizations) depends on Task 73
- Task 75 (BK-Tree Hybrid) depends on Task 58 (BK-Tree) and Task 73
- Task 76 (Replace LSH) depends on Tasks 73-74 and Task 69 (auto-selector)
- Task 77 (Python API) depends on Task 76
- Task 78 (Benchmarks) depends on Task 76
- Tasks 79-80 (Advanced) depend on Task 73

---

## Phase 9: Advanced SIMD and Memory Optimizations to Beat pl-fuzzy-frame-match

After Phase 8's sparse vector blocking, this phase implements advanced SIMD optimizations and memory access patterns to consistently beat pl-fuzzy-frame-match across all similarity metrics and dataset sizes.

### Overview

Current benchmark analysis shows pl-fuzzy-frame-match is faster in some scenarios:
- **Jaro-Winkler on medium datasets (1M comparisons):** pl-fuzzy-frame-match 15% faster
- **Levenshtein on large datasets (4M comparisons):** pl-fuzzy-frame-match 10% faster
- **Damerau-Levenshtein across many sizes:** pl-fuzzy-frame-match 2-10% faster

Root causes identified:
1. **Single-pair processing:** Current implementation processes one string pair at a time in SIMD
2. **Thread-local overhead:** Buffer allocation through `thread_local!` adds overhead
3. **Suboptimal dispatch for medium strings:** 15-30 char strings don't hit optimal code paths
4. **SIMD width limitations:** Using 8-wide vectors when 16-wide AVX-512 available

### High Priority Optimizations

**Task 81: Batch-Level SIMD for Fuzzy Join (CRITICAL PRIORITY)**

- **Priority:** Critical
- **Dependencies:** Phase 8 completion
- **Description:** Process multiple string pairs simultaneously using SIMD instead of one pair at a time
- **Expected Impact:** 2-4x speedup for fuzzy join operations
- **Implementation:**
  - Create `compute_batch_similarities_simd8()` function that processes 8 pairs at once
  - Group string pairs into SIMD-width batches for parallel processing
  - Compute 8x8 = 64 similarities simultaneously using SIMD vectors
  - Vectorize threshold filtering for batch results
  - Integrate with existing batch processing infrastructure
  - Key insight: Current approach processes one pair, applies SIMD within pair. New approach processes 8 pairs, applies SIMD across pairs for better ILP.
- **Subtasks:**
  - Subtask 81.1: Create `StringBatchSIMD8` struct for 8-pair batching
  - Subtask 81.2: Implement `jaro_winkler_batch8()` for 8 concurrent Jaro-Winkler calculations
  - Subtask 81.3: Implement `levenshtein_batch8()` for 8 concurrent Levenshtein calculations
  - Subtask 81.4: Implement `damerau_levenshtein_batch8()` for 8 concurrent DL calculations
  - Subtask 81.5: Integrate batch SIMD with fuzzy join pipeline
  - Subtask 81.6: Add fallback path for non-multiple-of-8 batch sizes
- **Test Strategy:** Benchmark batch SIMD vs single-pair SIMD, verify correctness
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`, new `fuzzy_simd_batch.rs`

**Task 82: Stack Allocation for Medium Strings (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** None (independent optimization)
- **Description:** Use stack-allocated buffers instead of thread-local for strings ≤128 chars
- **Expected Impact:** 10-20% reduction in overhead for common string sizes
- **Implementation:**
  - Create `levenshtein_distance_stack_optimized()` with `[usize; 129]` stack arrays
  - Create `jaro_similarity_stack_optimized()` with `[bool; 65]` stack arrays
  - Create `damerau_levenshtein_stack_optimized()` with `[usize; 129]` stack arrays
  - Only use thread-local buffers for strings >128 chars (rare in practice)
  - Eliminates `BUFFER.with()` overhead which adds ~5-10ns per call
  - Integrate into main dispatch functions with length checks
- **Subtasks:**
  - Subtask 82.1: Implement stack-allocated Levenshtein for strings ≤128 chars
  - Subtask 82.2: Implement stack-allocated Jaro-Winkler for strings ≤64 chars
  - Subtask 82.3: Implement stack-allocated Damerau-Levenshtein for strings ≤128 chars
  - Subtask 82.4: Update dispatch functions to prefer stack allocation
  - Subtask 82.5: Benchmark thread-local vs stack allocation overhead
- **Test Strategy:** Compare performance with thread-local version, verify correctness
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Medium Priority Optimizations

**Task 83: Medium String Specialization for Jaro-Winkler (15-30 chars)**

- **Priority:** Medium
- **Dependencies:** Task 82
- **Description:** Specialized Jaro-Winkler implementation optimized for the most common string lengths
- **Expected Impact:** 15-30% speedup for Jaro-Winkler on typical name/company data
- **Implementation:**
  - Create `jaro_similarity_medium_strings()` for 15-30 char strings
  - Use `[bool; 32]` stack-allocated match arrays (fits in L1 cache line)
  - Inline SIMD character search using `u8x16` vectors for search windows
  - Unroll match-finding loop for strings in this range
  - Avoid hash-based path (overhead not worth it for medium strings)
  - Key insight: 15-30 chars is the "sweet spot" where bit-parallel (≤64) is slower due to setup and hash-based (>50) is overkill
- **Subtasks:**
  - Subtask 83.1: Implement `jaro_similarity_medium_strings()` with stack allocation
  - Subtask 83.2: Add inline SIMD character search for 16-byte windows
  - Subtask 83.3: Optimize match-finding loop with manual unrolling
  - Subtask 83.4: Integrate with dispatch logic in `jaro_similarity_bytes()`
  - Subtask 83.5: Benchmark medium string path vs current implementation
- **Test Strategy:** Profile on real-world name datasets, verify speedup
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 84: AVX-512 16-Wide Vectors for Levenshtein**

- **Priority:** Medium
- **Dependencies:** Task 28 (existing SIMD banded Levenshtein)
- **Description:** Use `u32x16` vectors on AVX-512 capable CPUs for 2x wider SIMD
- **Expected Impact:** 2x speedup on AVX-512 systems (Intel Xeon, AMD Zen4+)
- **Implementation:**
  - Create `levenshtein_distance_banded_avx512()` using `Simd<u32, 16>`
  - Process 16 DP cells in parallel instead of current 8
  - Runtime CPU feature detection using `is_x86_feature_detected!("avx512f")`
  - Fallback to 8-wide implementation on non-AVX-512 systems
  - Also implement AVX-512 for Damerau-Levenshtein
- **Subtasks:**
  - Subtask 84.1: Implement `levenshtein_distance_banded_avx512()` with u32x16
  - Subtask 84.2: Implement `damerau_levenshtein_avx512()` with u32x16
  - Subtask 84.3: Add runtime CPU feature detection for AVX-512
  - Subtask 84.4: Create dispatch functions for optimal SIMD width selection
  - Subtask 84.5: Benchmark AVX-512 vs AVX2 on supported hardware
- **Test Strategy:** Test on AVX-512 and non-AVX-512 systems, verify correctness
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Low Priority Optimizations

**Task 85: Transposition Pre-computation for Damerau-Levenshtein**

- **Priority:** Low
- **Dependencies:** Task 32 (existing DL SIMD)
- **Description:** Pre-compute transposition positions to reduce inner loop comparisons
- **Expected Impact:** 5-10% speedup for Damerau-Levenshtein
- **Implementation:**
  - Create `TranspositionMap` struct with bitmap storage
  - Pre-compute all (i, j) positions where transposition is possible
  - Replace 4 character comparisons per cell with O(1) bitmap lookup
  - Trade memory (O(m*n) bits) for reduced comparisons
  - Only use for medium-sized strings (10-100 chars) where benefit exceeds overhead
- **Subtasks:**
  - Subtask 85.1: Implement `TranspositionMap` struct with bitmap storage
  - Subtask 85.2: Implement `precompute_transpositions()` function
  - Subtask 85.3: Integrate pre-computed map with DL SIMD implementation
  - Subtask 85.4: Add length-based dispatch to enable/disable pre-computation
  - Subtask 85.5: Benchmark pre-computed vs inline transposition checks
- **Test Strategy:** Profile on various string lengths, verify memory/speed tradeoff
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 86: Cache-Optimized Memory Access Patterns**

- **Priority:** Low
- **Dependencies:** None
- **Description:** Optimize memory access patterns for better cache utilization
- **Expected Impact:** 5-15% speedup from reduced cache misses
- **Implementation:**
  - Ensure DP row arrays are 64-byte aligned (cache line aligned)
  - Process strings in cache-friendly order
  - Prefetch next row data during current row computation
  - Use cache blocking for very long strings
- **Subtasks:**
  - Subtask 86.1: Add cache line alignment to buffer allocations
  - Subtask 86.2: Implement prefetching for next row data
  - Subtask 86.3: Add cache blocking for strings >1024 chars
  - Subtask 86.4: Profile cache miss rates before/after optimization
- **Test Strategy:** Use perf/cachegrind to measure cache miss reduction
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Phase 9 Success Criteria
- Beat pl-fuzzy-frame-match on ALL similarity metrics and dataset sizes
- Batch-level SIMD achieves 2-4x speedup for fuzzy join operations
- Stack allocation reduces overhead by 10-20% for medium strings
- Medium string specialization improves Jaro-Winkler by 15-30%
- AVX-512 implementation achieves 2x speedup on supported hardware
- All existing tests still pass
- Benchmark results documented and tracked

### Phase 9 Dependencies
- Phase 9 (Tasks 81-86) depends on Phase 8 completion (Tasks 73-80)
- Task 81 (Batch SIMD) depends on fuzzy join batch processing infrastructure
- Task 82 (Stack Allocation) is independent
- Task 83 (Medium String) depends on Task 82
- Task 84 (AVX-512) depends on Task 28 (existing SIMD)
- Task 85 (Transposition Pre-compute) depends on Task 32 (DL SIMD)
- Task 86 (Cache Optimization) is independent

---

## Phase 10: Comprehensive Batch SIMD Optimization

After Phase 9's initial batch SIMD implementation, this phase extends batch SIMD to all code paths where it's currently missing, ensuring maximum performance across all fuzzy join scenarios.

### Overview

Current batch SIMD implementation (Task 81) only covers the main fuzzy join path when early termination is disabled. Analysis reveals several critical gaps:
- **Sequential path with early termination**: Falls back to individual processing (2-4x performance loss)
- **Hamming similarity**: Existing batch function not used (2-3x performance loss)
- **Blocking candidate verification**: Candidates verified individually (2-3x performance loss)
- **AVX-512 support**: Only 8-wide vectors used when 16-wide available (1.5-2x potential gain)

This phase addresses all these gaps to achieve comprehensive batch SIMD coverage.

### High Priority Optimizations

**Task 87: Hybrid Early Termination with Batch SIMD (CRITICAL PRIORITY)**

- **Priority:** Critical
- **Dependencies:** Task 81 (existing batch SIMD infrastructure)
- **Description:** Enable batch SIMD in sequential path when early termination is enabled
- **Expected Impact:** 2-4x speedup for early termination scenarios (BestMatch, FirstMatch, AllMatches with limit)
- **Implementation:**
  - Modify `compute_batch_similarities_sequential()` to use batch SIMD
  - Process pairs in batches of 8 using SIMD functions
  - Check early termination conditions **after** each batch completes
  - Maintain correctness: early termination still works, but with SIMD benefits
  - Key insight: Batch SIMD and early termination are compatible - check termination after batch, not during
- **Subtasks:**
  - Subtask 87.1: Create `compute_batch_similarities_with_early_term_simd()` function
  - Subtask 87.2: Group pairs into batches of 8 for SIMD processing
  - Subtask 87.3: Integrate early termination checks after batch computation
  - Subtask 87.4: Handle BestMatch strategy: track best similarity per left index
  - Subtask 87.5: Handle FirstMatch strategy: stop after first match per left index
  - Subtask 87.6: Handle AllMatches with limit: stop after reaching max matches
  - Subtask 87.7: Update `compute_batch_similarities_with_termination()` to use new function
  - Subtask 87.8: Benchmark early termination with/without batch SIMD
- **Test Strategy:** Verify early termination correctness, measure speedup vs sequential processing
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

**Task 88: Use Existing Hamming Batch SIMD Function (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 81 (batch SIMD infrastructure exists)
- **Description:** Replace individual Hamming processing with existing `compute_hamming_batch8()` function
- **Expected Impact:** 2-3x speedup for Hamming similarity in fuzzy joins
- **Implementation:**
  - Update `process_simd8_batch()` to use `compute_hamming_batch8()` for Hamming similarity
  - Filter equal-length pairs first (Hamming requires equal lengths)
  - Use batch SIMD when all 8 pairs have equal lengths
  - Fallback to individual processing only for mixed-length batches
  - Remove redundant individual Hamming computation in batch path
- **Subtasks:**
  - Subtask 88.1: Update `process_simd8_batch()` to check for equal-length pairs
  - Subtask 88.2: Call `compute_hamming_batch8()` when all pairs have equal lengths
  - Subtask 88.3: Implement efficient equal-length filtering for batch
  - Subtask 88.4: Update `process_remainder_batch()` to use batch Hamming when possible
  - Subtask 88.5: Remove redundant `hamming_similarity_direct_bytes()` calls in batch path
  - Subtask 88.6: Benchmark Hamming with/without batch SIMD
- **Test Strategy:** Verify Hamming correctness, test with mixed-length batches, measure speedup
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

### Medium Priority Optimizations

**Task 89: Batch SIMD for Blocking Candidate Verification (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 52 (blocking infrastructure), Task 81 (batch SIMD)
- **Description:** Use batch SIMD to verify candidate pairs from blocking strategies
- **Expected Impact:** 2-3x speedup for blocked fuzzy joins
- **Implementation:**
  - Modify `compute_fuzzy_matches_from_candidates()` to group candidates into batches of 8
  - Use batch SIMD functions to verify candidate pairs
  - Process full batches (8 pairs) with SIMD, handle remainders individually
  - Maintain candidate pair ordering for correct result assembly
  - Integrate with all blocking strategies (FirstNChars, NGram, Length, LSH, SparseVector)
- **Subtasks:**
  - Subtask 89.1: Create `verify_candidates_batch_simd()` function
  - Subtask 89.2: Group candidate pairs into batches of 8
  - Subtask 89.3: Extract string pairs for batch processing
  - Subtask 89.4: Call appropriate batch SIMD function based on similarity type
  - Subtask 89.5: Filter candidates above threshold using SIMD threshold filtering
  - Subtask 89.6: Handle remainder candidates (< 8) efficiently
  - Subtask 89.7: Update `compute_fuzzy_matches_from_candidates()` to use batch verification
  - Subtask 89.8: Benchmark blocked joins with/without batch SIMD candidate verification
- **Test Strategy:** Verify blocking correctness, test with various blocking strategies, measure speedup
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

**Task 90: AVX-512 16-Wide Batch SIMD Support (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Task 81 (8-wide batch SIMD), Task 84 (AVX-512 for individual similarity)
- **Description:** Extend batch SIMD to use 16-wide AVX-512 vectors when available
- **Expected Impact:** 1.5-2x speedup on AVX-512 capable CPUs (Intel Xeon, AMD Zen4+)
- **Implementation:**
  - Create 16-wide batch SIMD functions: `compute_*_batch16_with_threshold()`
  - Implement for all similarity metrics: Jaro-Winkler, Levenshtein, Damerau-Levenshtein, Hamming
  - Runtime CPU feature detection using `is_x86_feature_detected!("avx512f")`
  - Auto-select 16-wide when AVX-512 available, fallback to 8-wide otherwise
  - Update batch processing to handle 16-pair batches
  - Process remainders efficiently (use 8-wide for 8-15 remainders)
- **Subtasks:**
  - Subtask 90.1: Implement `compute_jaro_winkler_batch16_with_threshold()` using Simd<f32, 16>
  - Subtask 90.2: Implement `compute_levenshtein_batch16_with_threshold()` using Simd<f32, 16>
  - Subtask 90.3: Implement `compute_damerau_levenshtein_batch16_with_threshold()` using Simd<f32, 16>
  - Subtask 90.4: Implement `compute_hamming_batch16()` using Simd<f32, 16>
  - Subtask 90.5: Add CPU feature detection for AVX-512
  - Subtask 90.6: Create dispatch function to select 8-wide vs 16-wide based on CPU
  - Subtask 90.7: Update `process_simd8_batch()` to handle 16-wide batches
  - Subtask 90.8: Update batch grouping logic for 16-pair batches
  - Subtask 90.9: Benchmark AVX-512 vs AVX2 on supported hardware
- **Test Strategy:** Test on AVX-512 and non-AVX-512 systems, verify correctness, measure speedup
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`, `fuzzy.rs`

### Low Priority Optimizations

**Task 91: Optimize Remainder Batch Processing (LOW PRIORITY)**

- **Priority:** Low
- **Dependencies:** Task 81 (batch SIMD infrastructure)
- **Description:** Use smaller SIMD batches or masked operations for remainder pairs (< 8)
- **Expected Impact:** 10-20% speedup for remainder processing
- **Implementation:**
  - For remainders of size 4-7: use 4-wide or 8-wide with masking
  - For remainders of size 1-3: process individually (overhead too high)
  - Use SIMD mask operations to handle partial batches
  - Or create specialized 4-wide batch functions for 4-7 remainders
- **Subtasks:**
  - Subtask 91.1: Implement masked SIMD operations for partial batches
  - Subtask 91.2: Create 4-wide batch functions for 4-7 remainders
  - Subtask 91.3: Update `process_remainder_batch()` to use optimized paths
  - Subtask 91.4: Benchmark remainder processing with/without optimization
- **Test Strategy:** Verify correctness for all remainder sizes, measure speedup
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

### Phase 10 Success Criteria
- Batch SIMD used in all fuzzy join code paths (early termination, blocking, Hamming)
- Early termination scenarios achieve 2-4x speedup with batch SIMD
- Hamming similarity achieves 2-3x speedup using batch SIMD
- Blocked joins achieve 2-3x speedup with batch SIMD candidate verification
- AVX-512 support provides 1.5-2x additional speedup on supported CPUs
- Remainder processing optimized for 10-20% improvement
- All existing tests still pass
- Comprehensive batch SIMD coverage across all similarity metrics and join scenarios

### Phase 10 Dependencies
- Phase 10 (Tasks 87-91) depends on Phase 9 completion (Tasks 81-86)
- Task 87 (Hybrid Early Termination) depends on Task 81 (batch SIMD infrastructure)
- Task 88 (Hamming Batch) depends on Task 81 (batch SIMD infrastructure)
- Task 89 (Blocking Candidates) depends on Task 52 (blocking) and Task 81 (batch SIMD)
- Task 90 (AVX-512 Batch) depends on Task 81 (8-wide batch) and Task 84 (AVX-512 individual)
- Task 91 (Remainder Optimization) depends on Task 81 (batch SIMD infrastructure)

---

## Phase 11: Memory and Dispatch Optimizations for Peak Performance

After Phase 10's comprehensive batch SIMD coverage, this phase implements memory access and dispatch optimizations to match or exceed pl-fuzzy-frame-match performance without using external dependencies like polars-simed.

### Overview

Analysis of pl-fuzzy-frame-match reveals their performance advantages come from:
1. **Aggressive blocking filtering** (99%+ filter rate at large scales)
2. **Optimized memory layout** (contiguous string storage, better cache locality)
3. **Better dispatch logic** (algorithm selection based on batch characteristics)
4. **Lower overhead** (minimal abstraction layers, aggressive inlining)

Phase 11 addresses these within pure Polars, focusing on:
- Memory access pattern optimization
- Better batch-level algorithm dispatch
- Function call overhead reduction
- Profile-guided optimization

### High Priority Optimizations

**Task 94: Contiguous Memory Layout for String Batches (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 81 (batch processing infrastructure)
- **Description:** Use contiguous memory buffers for string pairs to improve cache locality and reduce pointer indirection
- **Expected Impact:** 10-20% speedup from better cache utilization
- **Implementation:**
  - Create `ContiguousStringBatch` struct:
    ```rust
    pub struct ContiguousStringBatch {
        data: Vec<u8>,              // Contiguous buffer for all string data
        offsets: Vec<usize>,        // Byte offsets for each string
        indices: Vec<usize>,        // Original row indices
        lengths: Vec<usize>,        // Pre-computed lengths
    }
    ```
  - Pre-allocate single buffer with estimated total size
  - Copy string data into contiguous buffer
  - Benefits:
    - Sequential memory access improves prefetching
    - Reduced pointer chasing
    - Better SIMD memory access patterns
    - Improved cache line utilization
  - Use in batch processing paths for both full scan and blocked joins
  - Add `use_contiguous_layout: bool` parameter (default: true for batches >32 pairs)
- **Subtasks:**
  - Subtask 94.1: Implement `ContiguousStringBatch` struct with buffer management
  - Subtask 94.2: Create `from_string_pairs()` constructor that copies strings contiguously
  - Subtask 94.3: Update batch SIMD functions to accept contiguous layout
  - Subtask 94.4: Add memory estimation for optimal buffer pre-allocation
  - Subtask 94.5: Integrate with `compute_batch_similarities_simd8()`
  - Subtask 94.6: Add fallback to pointer-based when contiguous layout overhead too high
  - Subtask 94.7: Benchmark cache miss rates and overall performance improvement
- **Test Strategy:** Profile cache misses, measure memory access latency, verify correctness
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

**Task 95: Batch-Level Algorithm Dispatch (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** None (independent optimization)
- **Description:** Analyze batch characteristics and select optimal algorithm variant for entire batch instead of per-pair dispatch
- **Expected Impact:** 15-30% speedup from better algorithm selection and reduced dispatch overhead
- **Implementation:**
  - Create `BatchCharacteristics` struct:
    ```rust
    pub struct BatchCharacteristics {
        homogeneous_length: bool,    // All strings same length
        avg_length: usize,
        min_length: usize,
        max_length: usize,
        all_ascii: bool,
        high_diversity: bool,        // Many unique characters
    }
    ```
  - Analyze batch once before processing:
    - Scan all strings in batch for characteristics
    - Select optimal algorithm variant for entire batch
    - Avoid per-pair dispatch overhead
  - Specialized batch paths:
    - `process_homogeneous_batch()`: All same length (can use optimized loops)
    - `process_short_batch()`: All strings ≤16 bytes (use inline comparison)
    - `process_long_batch()`: All strings >50 bytes (use hash-based for Jaro-Winkler)
    - `process_mixed_batch()`: Mixed lengths (current general path)
  - Dispatch once per batch instead of once per pair (8-16x fewer dispatch calls)
- **Subtasks:**
  - Subtask 95.1: Implement `BatchCharacteristics` analysis
  - Subtask 95.2: Create specialized batch processing functions
  - Subtask 95.3: Implement `select_batch_algorithm()` dispatch function
  - Subtask 95.4: Add homogeneous batch fast path
  - Subtask 95.5: Integrate batch dispatch with SIMD batch processing
  - Subtask 95.6: Benchmark dispatch overhead reduction
- **Test Strategy:** Test on homogeneous and mixed batches, verify algorithm selection, measure speedup
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

**Task 96: Aggressive Function Inlining and Call Overhead Reduction (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** None
- **Description:** Reduce function call overhead in hot paths through aggressive inlining and flattening
- **Expected Impact:** 5-15% speedup from reduced call overhead
- **Implementation:**
  - Add `#[inline(always)]` to all batch SIMD helper functions
  - Inline small helper functions into batch processing loops
  - Use macros for repeated patterns instead of function calls
  - Flatten call stack in similarity computation:
    ```rust
    // ❌ Current: Multiple function calls
    fn process_batch() {
        let sim = compute_similarity();  // Call 1
        filter_results(sim);             // Call 2
    }
    
    // ✅ Optimized: Inline everything
    #[inline(always)]
    fn process_batch() {
        // Inline similarity computation
        // Inline result filtering
        // Single function call from caller
    }
    ```
  - Key targets:
    - `process_simd8_batch()`: Inline similarity computation
    - `process_remainder_batch()`: Inline individual processing
    - `compute_batch_similarities_simd8_impl()`: Inline validation checks
- **Subtasks:**
  - Subtask 96.1: Add `#[inline(always)]` to batch SIMD helper functions
  - Subtask 96.2: Inline small helper functions into main loops
  - Subtask 96.3: Convert repeated patterns to macros
  - Subtask 96.4: Flatten call stack in hot paths
  - Subtask 96.5: Profile and verify call overhead reduction
- **Test Strategy:** Profile call overhead, measure instruction count reduction, verify no code bloat
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`, `similarity.rs`

### Medium Priority Optimizations

**Task 97: SmallVec for Batch Buffers (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Task 94 (contiguous layout)
- **Description:** Use `SmallVec` for batch buffers to avoid heap allocation for small batches
- **Expected Impact:** 5-10% speedup for small-medium batch sizes
- **Implementation:**
  - Replace `Vec<u8>` with `SmallVec<[u8; 256]>` for small batches
  - Replace `Vec<(usize, usize, f32)>` with `SmallVec<[(usize, usize, f32); 32]>` for results
  - Benefits:
    - Zero heap allocations for batches ≤32 pairs
    - Faster for common batch sizes
    - Reduces allocator pressure
  - Add `smallvec` dependency to `polars-ops/Cargo.toml` (already used elsewhere in Polars)
  - Use in batch processing and remainder processing
- **Subtasks:**
  - Subtask 97.1: Add `smallvec` dependency if not present
  - Subtask 97.2: Replace batch buffer Vec with SmallVec
  - Subtask 97.3: Replace result Vec with SmallVec for small batches
  - Subtask 97.4: Tune SmallVec inline capacity based on profiling
  - Subtask 97.5: Benchmark allocation overhead reduction
- **Test Strategy:** Measure allocation counts, verify performance improvement for small batches
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

**Task 98: Pre-computed String Length Lookups (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Task 94 (contiguous layout)
- **Description:** Pre-compute and cache string lengths to avoid repeated `str.len()` calls in inner loops
- **Expected Impact:** 5-10% speedup from eliminated length computations
- **Implementation:**
  - Add `lengths` field to `StringBatch` and `ContiguousStringBatch`
  - Pre-compute all lengths during batch construction
  - Use cached lengths in:
    - Length-based pre-filtering
    - Algorithm dispatch
    - Similarity computation (where needed)
  - Single pass through strings instead of multiple `.len()` calls
  - Particularly beneficial for UTF-8 strings where `.len()` is not O(1) in bytes
- **Subtasks:**
  - Subtask 98.1: Add `lengths: Vec<usize>` to batch structs
  - Subtask 98.2: Pre-compute lengths during batch construction
  - Subtask 98.3: Update `can_reach_threshold()` to use cached lengths
  - Subtask 98.4: Update dispatch logic to use cached lengths
  - Subtask 98.5: Benchmark performance with pre-computed lengths
- **Test Strategy:** Measure `.len()` call elimination, verify correctness
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

**Task 99: Specialized Fast Path for High Thresholds (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** None
- **Description:** Add specialized fast paths for high similarity thresholds (≥0.9)
- **Expected Impact:** 10-30% speedup for high-threshold queries (common use case)
- **Implementation:**
  - For thresholds ≥0.9:
    - Use more aggressive length-based pre-filtering
    - Check prefix match early (first N characters must match for high similarity)
    - Use early exit more aggressively
    - Specialized algorithms that exploit high similarity assumption
  - Create `*_similarity_high_threshold()` variants:
    ```rust
    fn levenshtein_similarity_high_threshold(s1: &[u8], s2: &[u8], threshold: f32) -> Option<f32> {
        // Assume threshold >= 0.9
        // Max edit distance = (1 - 0.9) * max_len = 0.1 * max_len
        // Very narrow diagonal band
        // Can use faster algorithm
    }
    ```
  - Integrate into batch processing with threshold-based dispatch
- **Subtasks:**
  - Subtask 99.1: Implement prefix match check for high thresholds
  - Subtask 99.2: Create narrow-band Levenshtein for high thresholds
  - Subtask 99.3: Optimize Jaro-Winkler for high threshold (fewer candidates)
  - Subtask 99.4: Add threshold-based dispatch in batch processing
  - Subtask 99.5: Benchmark high-threshold queries
- **Test Strategy:** Test correctness at various high thresholds, measure speedup
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`, `fuzzy.rs`

### Low Priority Optimizations

**Task 100: Profile-Guided Optimization (PGO) Build Configuration (LOW PRIORITY)**

- **Priority:** Low
- **Dependencies:** None (build configuration)
- **Description:** Configure and document PGO builds for optimal performance
- **Expected Impact:** 10-20% speedup from better branch prediction and code layout
- **Implementation:**
  - Document PGO build process in build documentation:
    ```bash
    # Step 1: Build with instrumentation
    RUSTFLAGS="-Cprofile-generate=/tmp/pgo-data" cargo build --release
    
    # Step 2: Run representative benchmarks
    ./target/release/benchmark_fuzzy_join
    
    # Step 3: Build with PGO
    RUSTFLAGS="-Cprofile-use=/tmp/pgo-data -Cllvm-args=-pgo-warn-missing-function" cargo build --release
    ```
  - Add PGO configuration to `Cargo.toml`:
    ```toml
    [profile.release-pgo]
    inherits = "release"
    lto = "fat"
    codegen-units = 1
    ```
  - Create benchmark workload for PGO training that covers:
    - Various dataset sizes
    - All similarity metrics
    - Different threshold ranges
    - Mix of blocking strategies
  - Document PGO benefits and when to use
- **Subtasks:**
  - Subtask 100.1: Document PGO build process
  - Subtask 100.2: Create PGO training workload script
  - Subtask 100.3: Add PGO profile to Cargo.toml
  - Subtask 100.4: Benchmark PGO vs non-PGO builds
  - Subtask 100.5: Document when PGO is worth the build time
- **Test Strategy:** Build with PGO, run benchmarks, measure improvement
- **Code Location:** Build scripts, `BUILD_OPTIMIZATION.md` documentation

**Task 101: LTO (Link-Time Optimization) Configuration (LOW PRIORITY)**

- **Priority:** Low
- **Dependencies:** None
- **Description:** Enable and configure LTO for cross-module optimization
- **Expected Impact:** 5-15% speedup from better cross-module inlining
- **Implementation:**
  - Already enabled in release profile, but document optimal settings:
    ```toml
    [profile.release]
    lto = "fat"          # Full LTO across all crates
    codegen-units = 1    # Single codegen unit for maximum optimization
    opt-level = 3        # Maximum optimization level
    ```
  - Document trade-offs:
    - Longer compile times (3-5x slower)
    - Better runtime performance (5-15% faster)
    - When to use: production builds, benchmarking
  - Create separate profile for fast development builds:
    ```toml
    [profile.dev-fast]
    inherits = "dev"
    opt-level = 1
    ```
- **Subtasks:**
  - Subtask 101.1: Document LTO configuration and trade-offs
  - Subtask 101.2: Verify LTO is enabled for release builds
  - Subtask 101.3: Create fast development profile
  - Subtask 101.4: Benchmark LTO vs non-LTO performance
- **Test Strategy:** Compare build times and runtime performance with/without LTO
- **Code Location:** `Cargo.toml`, `BUILD_OPTIMIZATION.md`

### Low Priority Optimizations

**Task 102: Cache Line Alignment for DP Buffers (LOW PRIORITY)**

- **Priority:** Low
- **Dependencies:** Task 82 (stack allocation)
- **Description:** Align DP matrix buffers to 64-byte cache lines for optimal memory access
- **Expected Impact:** 5-10% speedup from reduced cache misses
- **Implementation:**
  - Use `#[repr(align(64))]` for buffer structs
  - Ensure DP row arrays start on cache line boundaries
  - Benefits:
    - Each row fits in whole cache lines
    - Reduced false sharing in parallel execution
    - Better prefetching
  - Apply to:
    - Levenshtein DP buffers
    - Damerau-Levenshtein DP buffers
    - Jaro-Winkler match arrays (when on heap)
- **Subtasks:**
  - Subtask 102.1: Add cache line alignment to buffer structs
  - Subtask 102.2: Verify alignment with testing
  - Subtask 102.3: Profile cache miss rates before/after
  - Subtask 102.4: Measure performance impact
- **Test Strategy:** Use perf/cachegrind to measure cache miss reduction
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 103: Prefetching for Next String Pair (LOW PRIORITY)**

- **Priority:** Low
- **Dependencies:** Task 94 (contiguous layout)
- **Description:** Add software prefetching hints for next string pair during current computation
- **Expected Impact:** 5-10% speedup from improved memory latency hiding
- **Implementation:**
  - Use `core::intrinsics::prefetch_read_data()` to prefetch next strings
  - Prefetch during current similarity computation
  - Tune prefetch distance based on algorithm complexity
  - Only enable for large batches (>32 pairs) where benefit exceeds overhead
  - Target architectures with aggressive prefetchers (x86_64)
- **Subtasks:**
  - Subtask 103.1: Add prefetch intrinsics for next string data
  - Subtask 103.2: Tune prefetch distance based on algorithm
  - Subtask 103.3: Add architecture-specific prefetch configuration
  - Subtask 103.4: Benchmark with hardware performance counters
- **Test Strategy:** Measure memory stall cycles, verify latency hiding
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

**Task 104: Compile-Time Optimization Flags (LOW PRIORITY)**

- **Priority:** Low
- **Dependencies:** None
- **Description:** Document and optimize RUSTFLAGS for maximum performance
- **Expected Impact:** 5-10% speedup from better code generation
- **Implementation:**
  - Document optimal RUSTFLAGS:
    ```bash
    RUSTFLAGS="-C target-cpu=native -C opt-level=3 -C codegen-units=1"
    ```
  - Explain each flag:
    - `target-cpu=native`: Use all CPU features available on build machine
    - `opt-level=3`: Maximum optimization
    - `codegen-units=1`: Better optimization at cost of compile time
  - Add architecture-specific optimizations:
    - x86_64: Enable AVX-512, AVX2, FMA
    - ARM: Enable NEON, SVE if available
  - Create build scripts for different target architectures
  - Document when to use native vs portable builds
- **Subtasks:**
  - Subtask 104.1: Document optimal RUSTFLAGS
  - Subtask 104.2: Create architecture-specific build scripts
  - Subtask 104.3: Add target CPU configuration to build guide
  - Subtask 104.4: Benchmark native vs generic builds
- **Test Strategy:** Compare performance on different CPUs, verify correctness
- **Code Location:** `BUILD_OPTIMIZATION.md`, build scripts

### Phase 11 Success Criteria
- Contiguous memory layout reduces cache misses by 30-50%
- Batch-level dispatch reduces overhead by 15-30%
- Function call overhead reduced by 5-15%
- SmallVec eliminates heap allocations for small batches
- PGO builds achieve 10-20% additional speedup
- Overall: 1.5-2.5x additional speedup on top of Phase 10
- **Target: Match or exceed pl-fuzzy-frame-match on ALL metrics and dataset sizes**
- All existing tests still pass
- Performance improvements documented with benchmarks

### Phase 11 Dependencies
- Phase 11 (Tasks 94-104) can proceed after Phase 10 (Tasks 87-93)
- Task 94 (Contiguous Layout) depends on Task 81 (batch infrastructure)
- Task 95 (Batch Dispatch) is independent
- Task 96 (Inlining) is independent
- Task 97 (SmallVec) depends on Task 94
- Task 98 (Length Cache) depends on Task 94
- Task 99 (High Threshold) is independent
- Tasks 100-104 are build/configuration tasks (independent)

---

## Phase 12: Novel Optimizations from polars_sim Analysis

After analyzing the `polars_sim` repository (https://github.com/schemaitat/polars_sim), this phase implements novel optimization techniques not yet present in our implementation that could provide alternative approaches or complementary benefits.

### Overview

The `polars_sim` library uses different architectural approaches that offer unique trade-offs:
1. **On-the-fly vectorization** instead of pre-computed indices
2. **Integer-based sparse matrices** (u16) for memory efficiency
3. **Top-N heap-based algorithms** to avoid full matrix materialization
4. **Dynamic parallelization axis selection** based on DataFrame size asymmetry
5. **Zero-copy Arrow buffer access** for reduced conversion overhead
6. **Compile-time SIMD width selection** via feature flags
7. **Cache-oblivious algorithms** for datasets larger than cache
8. **Hybrid dense/sparse representation** based on vector density

### Core Alternative Approaches

**Task 105: On-the-Fly Vectorization for Medium Datasets (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Task 73 (existing sparse vector blocking)
- **Description:** Implement streaming sparse vectorization as a lighter-weight alternative to pre-computed indices for medium-sized datasets (100K-1M rows)
- **Expected Impact:** 10-20% memory reduction, 5-15% speedup for medium datasets where index overhead exceeds lookup benefits
- **Implementation:**
  - Create `StreamingVectorizer` that vectorizes strings on-the-fly during similarity computation
  - Instead of: Build inverted index → Query index → Compute similarities
  - Do: Stream through right column → Vectorize each string → Sparse dot product
  - Pre-compute left vectors once, vectorize right strings on-demand
  - Benefits:
    - No index building overhead
    - Lower memory footprint (only left vectors stored)
    - Better for medium datasets where index building cost > query savings
    - Simpler implementation for one-time joins
  - Add `vectorization_mode` parameter:
    - `"indexed"`: Pre-build inverted index (current approach, best for large datasets)
    - `"streaming"`: On-the-fly vectorization (best for medium datasets)
    - `"auto"`: Automatically select based on dataset size and reuse pattern
- **Subtasks:**
  - Subtask 105.1: Implement `StreamingVectorizer` struct with on-the-fly n-gram generation
  - Subtask 105.2: Create `compute_streaming_similarities()` function
  - Subtask 105.3: Add cost model to choose indexed vs streaming
  - Subtask 105.4: Integrate with auto-selector based on dataset characteristics
  - Subtask 105.5: Benchmark indexed vs streaming on medium datasets (100K-1M rows)
- **Test Strategy:** Compare memory usage and performance vs indexed approach, verify correctness
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_sparse_vector.rs`

**Task 106: U16 Sparse Matrix Storage for Memory Efficiency (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 73 (sparse vector blocking)
- **Description:** Use u16 integer storage for sparse vector values when normalization isn't required, reducing memory by 2x and improving cache performance
- **Expected Impact:** 50% memory reduction, 20-30% speedup from better cache utilization
- **Implementation:**
  - Create `SparseVectorStorage` enum:
    ```rust
    pub enum SparseVectorStorage {
        U16 {
            indices: Vec<u64>,      // N-gram hashes
            values: Vec<u16>,       // Integer term counts
            scale_factor: f32,      // For converting back to float
        },
        F32 {
            indices: Vec<u64>,      // N-gram hashes
            values: Vec<f32>,       // Normalized TF-IDF weights
        },
    }
    ```
  - Use U16 variant when:
    - No L2 normalization required
    - Cosine similarity computed via integer dot product
    - Convert to f32 only at final step
  - Benefits:
    - 2x memory reduction per vector
    - Better cache line utilization (more vectors fit in cache)
    - SIMD integer operations on x86 (pmaddwd instruction)
    - Lower memory bandwidth requirements
  - Automatically select based on whether normalization is needed
  - Add `use_integer_storage: bool` parameter (default: auto)
- **Subtasks:**
  - Subtask 106.1: Implement `SparseVectorStorage` enum with U16 and F32 variants
  - Subtask 106.2: Create `dot_product_u16()` for integer dot products
  - Subtask 106.3: Add SIMD integer dot product using pmaddwd (x86) or similar (ARM)
  - Subtask 106.4: Implement auto-selection logic based on normalization requirements
  - Subtask 106.5: Benchmark memory usage and cache performance
- **Test Strategy:** Verify correctness vs f32 version, measure memory reduction, profile cache misses
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_sparse_vector.rs`

**Task 107: Top-N Heap-Based Sparse Matrix Multiplication (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 73 (sparse vector implementation)
- **Description:** Implement specialized sparse matrix multiplication that computes top-N matches per row without materializing the full similarity matrix
- **Expected Impact:** O(n×m×log(k)) vs O(n×m×log(n×m)) complexity, more memory-efficient for large datasets
- **Implementation:**
  - Create `sparse_top_n_per_row()` function:
    ```rust
    fn sparse_top_n_per_row(
        left_vectors: &[SparseVector],
        right_vectors: &[SparseVector],
        top_n: usize,
        threshold: f32,
    ) -> Vec<Vec<(usize, f32)>> {
        // Min-heap of size top_n per left row
        // Only keeps top-N highest similarities
        // Never materializes full n×m matrix
    }
    ```
  - Use `BinaryHeap` with fixed capacity `top_n` for each left row
  - Benefits:
    - Memory usage: O(n × top_n) instead of O(n × m)
    - Faster for `keep="best"` with small N (e.g., top 1-10 matches)
    - No need to sort full result set
    - Cache-friendly (small heaps fit in cache)
  - Parallel implementation with Rayon for per-row processing
  - Integrate with existing fuzzy join for `keep="best"` strategy
- **Subtasks:**
  - Subtask 107.1: Implement `TopNHeap` struct with min-heap semantics
  - Subtask 107.2: Create `sparse_top_n_per_row()` function
  - Subtask 107.3: Add parallel processing with Rayon for rows
  - Subtask 107.4: Integrate with `compute_fuzzy_matches()` for best match strategy
  - Subtask 107.5: Benchmark vs full matrix approach for different N values
- **Test Strategy:** Verify correctness vs full computation, measure memory usage reduction
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_sparse_vector.rs`

### Parallelization Optimizations

**Task 108: Dynamic Parallelization Axis Selection (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Task 55 (parallel fuzzy join)
- **Description:** Automatically choose whether to parallelize over left or right DataFrame based on size asymmetry and normalization requirements
- **Expected Impact:** 20-40% speedup for asymmetric joins (one DataFrame >> other)
- **Implementation:**
  - Create `ParallelizationAxis` enum:
    ```rust
    pub enum ParallelizationAxis {
        Left,   // Parallelize over left rows (current default)
        Right,  // Parallelize over right rows (better for right >> left)
        Auto,   // Automatically select based on characteristics
    }
    ```
  - Selection logic:
    ```rust
    fn select_parallelization_axis(
        left_len: usize,
        right_len: usize,
        normalize: bool,
        blocking_enabled: bool,
    ) -> ParallelizationAxis {
        if normalize || blocking_enabled {
            // Normalization/blocking requires consistent access pattern
            return ParallelizationAxis::Left;
        }
        
        let size_ratio = right_len as f32 / left_len as f32;
        
        if size_ratio > 10.0 {
            // Right is 10x larger - parallelize over right
            // Build left index once, query from all right threads
            ParallelizationAxis::Right
        } else if size_ratio < 0.1 {
            // Left is 10x larger - standard left parallelization
            ParallelizationAxis::Left
        } else {
            // Similar sizes - use standard approach
            ParallelizationAxis::Left
        }
    }
    ```
  - Implement right-side parallelization:
    - Build blocking index for left DataFrame once
    - Parallelize over right rows using Rayon
    - Each thread queries left index independently
    - Better load balancing when right >> left
  - Add `parallelization_axis` parameter (default: "auto")
- **Subtasks:**
  - Subtask 108.1: Implement `ParallelizationAxis` enum and selection logic
  - Subtask 108.2: Create `compute_fuzzy_matches_parallel_right()` function
  - Subtask 108.3: Build left-side blocking index for right-side parallelization
  - Subtask 108.4: Integrate with auto-selector based on size ratio
  - Subtask 108.5: Benchmark on asymmetric datasets (e.g., 100 × 100K, 100K × 100)
- **Test Strategy:** Test on various size ratios, verify load balancing improvement
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

### Memory Access Optimizations

**Task 109: Zero-Copy Arrow String Access (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 94 (contiguous memory layout)
- **Description:** Access Arrow string buffers directly without conversion overhead, reducing memory copies
- **Expected Impact:** 10-20% speedup from eliminated string reference conversions
- **Implementation:**
  - Create `ArrowStringBatch` struct:
    ```rust
    pub struct ArrowStringBatch<'a> {
        offsets: &'a [i32],          // Arrow offset buffer (direct reference)
        data: &'a [u8],              // Arrow data buffer (direct reference)
        null_bitmap: Option<&'a [u8]>, // Arrow null bitmap
        len: usize,
    }
    ```
  - Implement zero-copy accessors:
    ```rust
    #[inline(always)]
    fn get_unchecked(&self, idx: usize) -> &'a [u8] {
        let start = self.offsets[idx] as usize;
        let end = self.offsets[idx + 1] as usize;
        &self.data[start..end]  // Zero-copy slice
    }
    
    #[inline(always)]
    fn is_null(&self, idx: usize) -> bool {
        self.null_bitmap
            .map(|bitmap| (bitmap[idx / 8] & (1 << (idx % 8))) == 0)
            .unwrap_or(false)
    }
    ```
  - Benefits:
    - No intermediate string reference creation
    - Direct access to Arrow buffers (contiguous memory)
    - Reduced allocations and copies
    - Better cache locality
  - Integrate with batch processing infrastructure
  - Use in all batch SIMD functions
- **Subtasks:**
  - Subtask 109.1: Implement `ArrowStringBatch` struct with zero-copy accessors
  - Subtask 109.2: Create `from_string_chunked()` constructor
  - Subtask 109.3: Update batch SIMD functions to accept Arrow batches
  - Subtask 109.4: Add null handling with bitmap access
  - Subtask 109.5: Benchmark vs current string reference approach
- **Test Strategy:** Verify correctness, measure allocation reduction, profile memory bandwidth
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy.rs`

### Build-Time Optimizations

**Task 110: Compile-Time SIMD Width Selection via Feature Flags (LOW PRIORITY)**

- **Priority:** Low
- **Dependencies:** Task 90 (AVX-512 runtime detection)
- **Description:** Use compile-time feature flags for SIMD width selection instead of runtime detection for optimal performance
- **Expected Impact:** 5-10% speedup from eliminated runtime checks and better compiler optimization
- **Implementation:**
  - Add feature flags to `Cargo.toml`:
    ```toml
    [features]
    default = ["simd_avx2"]
    simd_avx2 = []
    simd_avx512 = []
    simd_neon = []  # ARM NEON
    simd_sve = []   # ARM SVE
    ```
  - Compile-time SIMD width selection:
    ```rust
    #[cfg(all(target_arch = "x86_64", feature = "simd_avx512"))]
    const SIMD_WIDTH: usize = 16;
    
    #[cfg(all(target_arch = "x86_64", feature = "simd_avx2"))]
    const SIMD_WIDTH: usize = 8;
    
    #[cfg(target_arch = "aarch64")]
    const SIMD_WIDTH: usize = 4;  // NEON is 128-bit
    ```
  - Benefits:
    - Better compiler optimization (constant SIMD width)
    - No runtime CPU feature detection overhead
    - Can distribute multiple binaries for different CPUs
    - Monomorphization enables better inlining
  - Document build process:
    ```bash
    # AVX2 build (default)
    cargo build --release
    
    # AVX-512 build
    cargo build --release --features simd_avx512
    
    # ARM NEON build
    cargo build --release --target aarch64-unknown-linux-gnu
    ```
  - Keep runtime detection as fallback for dynamic library builds
- **Subtasks:**
  - Subtask 110.1: Add SIMD feature flags to Cargo.toml
  - Subtask 110.2: Implement compile-time SIMD width constants
  - Subtask 110.3: Update batch functions to use compile-time width
  - Subtask 110.4: Document build process for different SIMD targets
  - Subtask 110.5: Benchmark compile-time vs runtime dispatch
- **Test Strategy:** Build with different features, verify correctness, measure overhead reduction
- **Code Location:** `polars/crates/polars-ops/Cargo.toml`, `similarity.rs`, `fuzzy.rs`

### Advanced Algorithmic Optimizations

**Task 111: Cache-Oblivious Algorithm for Very Large Matrices (LOW PRIORITY)**

- **Priority:** Low
- **Dependencies:** Task 107 (sparse matrix multiplication)
- **Description:** Implement recursive cache-oblivious sparse matrix multiplication for datasets that don't fit in cache
- **Expected Impact:** 15-30% speedup for billion-scale datasets through automatic cache optimization
- **Implementation:**
  - Create recursive divide-and-conquer algorithm:
    ```rust
    fn sparse_multiply_cache_oblivious(
        left_start: usize,
        left_end: usize,
        right_start: usize,
        right_end: usize,
        left_vectors: &[SparseVector],
        right_vectors: &[SparseVector],
        threshold: f32,
    ) -> Vec<(usize, usize, f32)> {
        let left_size = left_end - left_start;
        let right_size = right_end - right_start;
        
        // Base case: block fits in L3 cache (~1MB = ~1K pairs)
        if left_size * right_size <= 1024 {
            return compute_block_direct(left_start, left_end, right_start, right_end, ...);
        }
        
        // Recursive case: divide larger dimension
        if left_size > right_size {
            let mid = left_start + left_size / 2;
            rayon::join(
                || sparse_multiply_cache_oblivious(left_start, mid, ...),
                || sparse_multiply_cache_oblivious(mid, left_end, ...),
            )
        } else {
            let mid = right_start + right_size / 2;
            rayon::join(
                || sparse_multiply_cache_oblivious(..., right_start, mid, ...),
                || sparse_multiply_cache_oblivious(..., mid, right_end, ...),
            )
        }
    }
    ```
  - Benefits:
    - Automatic cache optimization for any cache size
    - Better locality for large datasets (100M+ comparisons)
    - Natural parallelization via recursive splitting
    - No manual cache blocking tuning required
  - Use only for very large datasets where cache misses dominate
  - Tune base case size based on L3 cache size
- **Subtasks:**
  - Subtask 111.1: Implement recursive cache-oblivious algorithm
  - Subtask 111.2: Add base case direct computation function
  - Subtask 111.3: Tune base case threshold based on cache size
  - Subtask 111.4: Integrate with fuzzy join for large datasets
  - Subtask 111.5: Benchmark on billion-scale datasets (1B+ comparisons)
- **Test Strategy:** Profile cache misses, measure performance on datasets larger than cache
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_sparse_vector.rs`

**Task 112: Hybrid Dense/Sparse Vector Representation (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Task 106 (U16 storage)
- **Description:** Automatically switch between dense and sparse vector representations based on vector density
- **Expected Impact:** 2-3x speedup for high-density vectors (long strings with diverse characters)
- **Implementation:**
  - Add `VectorStorage` enum:
    ```rust
    pub enum VectorStorage {
        Sparse {
            indices: Vec<u64>,
            values: Vec<u16>,
        },
        Dense {
            values: Vec<u16>,  // Direct index by n-gram hash
            offset: u64,       // Base offset for indexing
        },
    }
    ```
  - Density threshold:
    ```rust
    fn select_storage(ngrams: &[u64], alphabet_size: u64) -> VectorStorage {
        const DENSITY_THRESHOLD: f32 = 0.3;  // 30% of possible n-grams
        
        let unique_ngrams = ngrams.len();
        let density = unique_ngrams as f32 / alphabet_size as f32;
        
        if density > DENSITY_THRESHOLD || unique_ngrams > 1000 {
            // Dense representation more efficient
            // Use direct indexing by n-gram hash
            VectorStorage::Dense { ... }
        } else {
            // Sparse representation
            VectorStorage::Sparse { ... }
        }
    }
    ```
  - Benefits:
    - Dense: O(1) lookup, faster dot product for high-density vectors
    - Sparse: Lower memory, faster for low-density vectors
    - Automatic selection eliminates manual tuning
  - Use dense for:
    - Very long strings (>100 chars)
    - High character diversity strings
    - Multiple language text (Unicode)
  - Use sparse for:
    - Short to medium strings (<50 chars)
    - Limited character set (names, codes, IDs)
- **Subtasks:**
  - Subtask 112.1: Implement `VectorStorage` enum with dense and sparse variants
  - Subtask 112.2: Create density analysis function
  - Subtask 112.3: Implement dense dot product optimized path
  - Subtask 112.4: Add auto-selection logic based on density threshold
  - Subtask 112.5: Benchmark dense vs sparse on various string types
- **Test Strategy:** Test on high/low density datasets, verify performance crossover point
- **Code Location:** `polars/crates/polars-ops/src/frame/join/fuzzy_sparse_vector.rs`

### Phase 12 Success Criteria
- Streaming vectorization provides 10-20% memory reduction for medium datasets
- U16 sparse storage achieves 50% memory reduction with 20-30% speedup
- Top-N heap algorithm reduces memory for best-match queries to O(n×k) instead of O(n×m)
- Dynamic parallelization improves performance 20-40% for asymmetric joins
- Zero-copy Arrow access reduces overhead by 10-20%
- Compile-time SIMD selection eliminates 5-10% runtime overhead
- Cache-oblivious algorithm optimizes billion-scale datasets automatically
- Hybrid dense/sparse achieves 2-3x speedup for high-density vectors
- All existing tests still pass
- New optimizations documented with benchmarks

### Phase 12 Dependencies
- Phase 12 (Tasks 105-112) can proceed independently after Phase 11
- Task 105 (Streaming) depends on Task 73 (sparse vector)
- Task 106 (U16 storage) depends on Task 73 (sparse vector)
- Task 107 (Top-N heap) depends on Task 73 (sparse vector)
- Task 108 (Dynamic parallelization) depends on Task 55 (parallel join)
- Task 109 (Zero-copy Arrow) depends on Task 94 (contiguous layout)
- Task 110 (Compile-time SIMD) depends on Task 90 (AVX-512)
- Task 111 (Cache-oblivious) depends on Task 107 (sparse matrix)
- Task 112 (Hybrid dense/sparse) depends on Task 106 (U16 storage)

---

## Phase 13: Quick Win Optimizations for polars-distance Plugin

After analyzing the polars-distance repository, this phase implements high-impact, low-complexity optimizations that provide immediate performance gains with minimal implementation effort.

### Overview

The polars-distance plugin (https://github.com/ion-elgreco/polars-distance) provides distance metrics for Polars but lacks several key optimizations present in our implementation. This phase ports our optimization techniques to benefit the polars-distance ecosystem.

### High Priority Quick Wins

**Task 113: ARM NEON SIMD Support (CRITICAL PRIORITY)**

- **Priority:** Critical
- **Dependencies:** None (standalone optimization)
- **Description:** Port existing AVX2/AVX-512 SIMD code to ARM NEON for Apple Silicon (M1/M2/M3/M4) support
- **Expected Impact:** 2-4x speedup on Apple Silicon Macs
- **Target Repository:** polars-distance
- **Implementation:**
  - Port x86 SIMD code to ARM NEON intrinsics
  - Use `#[cfg(target_arch = "aarch64")]` for ARM-specific code
  - NEON has 128-bit vectors (vs AVX2's 256-bit)
  - Implement for all distance metrics:
    - Hamming: `uint8x16_t` for 16-byte XOR + popcount
    - Levenshtein: `uint32x4_t` for 4-wide min operations
    - Jaro-Winkler: `uint8x16_t` for character matching
    - Cosine: `float64x2_t` for 2-wide dot products
  - Runtime CPU feature detection using `is_aarch64_feature_detected!()`
  - Fallback to scalar for older ARM without NEON
- **Subtasks:**
  - Subtask 113.1: Implement NEON Hamming distance with `vaddvq_u8()` for popcount
  - Subtask 113.2: Implement NEON Levenshtein with `vminq_u32()` for DP min operations
  - Subtask 113.3: Implement NEON Jaro-Winkler character search with `vceqq_u8()`
  - Subtask 113.4: Implement NEON Cosine similarity with `vfmaq_f64()` (FMA)
  - Subtask 113.5: Add ARM feature detection and dispatch logic
  - Subtask 113.6: Create CI pipeline for ARM testing (GitHub Actions with M1 runner)
  - Subtask 113.7: Benchmark on Apple Silicon vs scalar implementation
- **Test Strategy:** Test on M1/M2 hardware, verify correctness vs x86 results, measure speedup
- **Documentation:** Add ARM-specific build instructions and performance notes

**Task 114: FMA (Fused Multiply-Add) Instructions (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** None
- **Description:** Use FMA instructions for floating-point operations to reduce instruction count and improve accuracy
- **Expected Impact:** 20-40% speedup for cosine similarity, 5-10% for other metrics
- **Target Repository:** polars-distance
- **Implementation:**
  - Replace separate multiply + add with single FMA instruction
  - Use `.mul_add(b, c)` method for `a * b + c` operations
  - Apply to:
    - Cosine similarity: `dot = a.mul_add(b, dot)`
    - Levenshtein/Jaro min operations: Use FMA for distance calculations
  - Benefits:
    - Single instruction instead of two (multiply + add)
    - Better accuracy (no intermediate rounding)
    - Available on all modern CPUs (x86: FMA3, ARM: built-in)
  - Example transformation:
    ```rust
    // ❌ Before: Two instructions
    let result = a * b + c;
    
    // ✅ After: One FMA instruction
    let result = a.mul_add(b, c);
    ```
- **Subtasks:**
  - Subtask 114.1: Replace multiply-add patterns in cosine similarity
  - Subtask 114.2: Apply FMA to Levenshtein diagonal band min operations
  - Subtask 114.3: Use FMA in Jaro-Winkler scoring calculations
  - Subtask 114.4: Add FMA to vector normalization (magnitude calculations)
  - Subtask 114.5: Benchmark FMA vs separate operations
- **Test Strategy:** Verify numerical accuracy, measure instruction count reduction
- **Documentation:** Document FMA usage and benefits in code comments

**Task 115: Profile-Guided Optimization (PGO) Build Configuration (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** None (build configuration)
- **Description:** Set up and document PGO builds for 10-25% free performance gain
- **Expected Impact:** 10-25% overall speedup from better branch prediction and code layout
- **Target Repository:** polars-distance
- **Implementation:**
  - Create PGO build profile in `Cargo.toml`:
    ```toml
    [profile.release-pgo]
    inherits = "release"
    lto = "fat"
    codegen-units = 1
    ```
  - Document three-step PGO process:
    ```bash
    # Step 1: Build with instrumentation
    RUSTFLAGS="-Cprofile-generate=/tmp/pgo-data" cargo build --release
    
    # Step 2: Run benchmarks to collect profile
    cargo bench  # Or run representative workload
    
    # Step 3: Rebuild with profile data
    rustup run nightly bash -c \
      'RUSTFLAGS="-Cprofile-use=/tmp/pgo-data -Cllvm-args=-pgo-warn-missing-function" \
       cargo build --release'
    ```
  - Create benchmark workload for PGO training:
    - Various string lengths (short, medium, long)
    - All distance metrics
    - Mix of similar and dissimilar strings
    - Representative of production use
  - Add CI job for PGO builds
  - Document when PGO is worth the build time overhead
- **Subtasks:**
  - Subtask 115.1: Add PGO profile to Cargo.toml
  - Subtask 115.2: Create PGO training workload script
  - Subtask 115.3: Document PGO build process in BUILD.md
  - Subtask 115.4: Add CI job for PGO builds
  - Subtask 115.5: Benchmark PGO vs non-PGO performance
  - Subtask 115.6: Measure compile time overhead vs runtime benefit
- **Test Strategy:** Compare PGO vs non-PGO builds, verify consistent results
- **Documentation:** Create `docs/PGO_BUILDS.md` with detailed instructions

**Task 116: Custom Memory Allocator (jemalloc/mimalloc) (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** None
- **Description:** Replace system allocator with jemalloc or mimalloc for 5-15% speedup
- **Expected Impact:** 5-15% speedup from reduced allocation overhead and better memory layout
- **Target Repository:** polars-distance
- **Implementation:**
  - Add allocator dependency to `Cargo.toml`:
    ```toml
    [dependencies]
    mimalloc = { version = "0.1", optional = true }
    
    [features]
    default = ["mimalloc"]
    mimalloc = ["dep:mimalloc"]
    ```
  - In `lib.rs`:
    ```rust
    #[cfg(feature = "mimalloc")]
    #[global_allocator]
    static GLOBAL: mimalloc::MiMalloc = mimalloc::MiMalloc;
    ```
  - Why mimalloc over jemalloc:
    - Simpler build (no C dependencies)
    - Better Windows support
    - Comparable or better performance
    - Already used in Polars ecosystem
  - Make it optional via feature flag (default enabled)
  - Document how to disable if conflicts with other allocators
- **Subtasks:**
  - Subtask 116.1: Add mimalloc dependency to Cargo.toml
  - Subtask 116.2: Set global allocator with feature flag
  - Subtask 116.3: Test on multiple platforms (Linux, macOS, Windows)
  - Subtask 116.4: Benchmark vs system allocator
  - Subtask 116.5: Document allocator selection rationale
- **Test Strategy:** Verify correctness, measure allocation overhead reduction
- **Documentation:** Document custom allocator usage and how to opt-out

### Phase 13 Success Criteria
- ARM NEON provides 2-4x speedup on Apple Silicon
- FMA instructions provide 20-40% speedup for cosine, 5-10% for others
- PGO builds provide 10-25% overall speedup
- Custom allocator provides 5-15% speedup
- All changes maintain correctness (tests pass)
- Documentation complete for all optimizations
- **Combined impact: 50-80% overall performance improvement**

### Phase 13 Dependencies
- All tasks in Phase 13 are independent and can proceed in parallel
- Task 113 (ARM NEON) is highest priority due to growing Apple Silicon user base
- Task 114 (FMA) and Task 115 (PGO) are quick wins with high ROI
- Task 116 (Allocator) is lowest priority but easiest (one-line change)

---

## Phase 14: Core Performance Optimizations for polars-distance

After Phase 13's quick wins, this phase implements deeper algorithmic and implementation optimizations that require more effort but provide substantial performance gains.

### Overview

These optimizations build on Phase 13 to further improve polars-distance performance through better algorithms, memory access patterns, and SIMD utilization.

### High Priority Core Optimizations

**Task 117: Vectorized Threshold Filtering (CRITICAL PRIORITY)**

- **Priority:** Critical
- **Dependencies:** Task 113 (SIMD infrastructure)
- **Description:** Use SIMD for threshold comparisons instead of scalar comparison per pair
- **Expected Impact:** 20-40% speedup for threshold-based distance queries (common use case)
- **Target Repository:** polars-distance
- **Implementation:**
  - Compute 8 distances with SIMD, then threshold filter in parallel:
    ```rust
    // Compute 8 distances using SIMD
    let distances = compute_batch8_distances(strings);
    
    // Vectorized threshold comparison
    let threshold_vec = f32x8::splat(threshold);
    let mask = distances.simd_ge(threshold_vec);
    
    // Extract only passing distances using bitmask
    let passing_mask = mask.to_bitmask();
    for i in 0..8 {
        if (passing_mask >> i) & 1 != 0 {
            results.push((indices[i], distances.extract(i)));
        }
    }
    ```
  - Apply to all distance metrics
  - Particularly beneficial for filtering scenarios (only keep matches above threshold)
  - Integrate with existing SIMD distance computation
- **Subtasks:**
  - Subtask 117.1: Implement `filter_by_threshold_simd()` for f32x8 vectors
  - Subtask 117.2: Integrate with Hamming distance computation
  - Subtask 117.3: Integrate with Levenshtein/Jaro-Winkler distance computation
  - Subtask 117.4: Add vectorized threshold filtering to batch processing
  - Subtask 117.5: Benchmark threshold filtering speedup
- **Test Strategy:** Verify correctness, test at various thresholds, measure speedup
- **Documentation:** Document vectorized filtering approach

**Task 118: Branchless/Branch-Free Implementations (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** None
- **Description:** Eliminate conditional branches in hot paths for better CPU pipelining
- **Expected Impact:** 10-20% speedup from improved branch prediction
- **Target Repository:** polars-distance
- **Implementation:**
  - Replace conditional logic with arithmetic:
    ```rust
    // ❌ Branching version
    let result = if a > b { a } else { b };
    
    // ✅ Branchless version
    let is_greater = (a > b) as u32;
    let result = (a * is_greater) + (b * (1 - is_greater));
    
    // Or use conditional move intrinsic
    let result = if_then_else(a > b, a, b);  // Maps to CMOV instruction
    ```
  - Apply to inner loops of distance calculations:
    - Levenshtein min operations: `min(a, min(b, c))`
    - Character comparison: `(ch1 == ch2) as usize`
    - Hamming difference counting
  - Use SIMD select operations where possible:
    ```rust
    let mask = a.simd_gt(b);
    let result = mask.select(a, b);  // Branchless SIMD max
    ```
  - Focus on hot paths identified by profiling
- **Subtasks:**
  - Subtask 118.1: Implement branchless min/max for Levenshtein DP
  - Subtask 118.2: Branchless character equality checks
  - Subtask 118.3: Branchless Hamming difference counting
  - Subtask 118.4: Use SIMD select for conditional operations
  - Subtask 118.5: Profile branch misprediction reduction
- **Test Strategy:** Profile branch mispredictions, verify correctness, measure speedup
- **Documentation:** Add comments explaining branchless techniques

**Task 119: Advanced Multi-Level Prefetching (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** None
- **Description:** Implement sophisticated prefetching strategy to hide memory latency
- **Expected Impact:** 10-20% speedup from reduced memory stalls
- **Target Repository:** polars-distance
- **Implementation:**
  - Prefetch multiple strings ahead with different cache levels:
    ```rust
    use std::intrinsics::prefetch_read_data;
    
    for i in 0..strings.len() {
        // Prefetch 4 strings ahead into L1 cache
        if i + 4 < strings.len() {
            unsafe {
                prefetch_read_data(
                    strings[i + 4].as_ptr(),
                    3  // High temporal locality (L1)
                );
            }
        }
        
        // Prefetch 16 strings ahead into L2 cache
        if i + 16 < strings.len() {
            unsafe {
                prefetch_read_data(
                    strings[i + 16].as_ptr(),
                    2  // Medium temporal locality (L2)
                );
            }
        }
        
        // Compute distance for current string
        compute_distance(strings[i], target);
    }
    ```
  - Tune prefetch distance based on:
    - String length (longer = more computation = further prefetch)
    - Cache sizes (adapt to L1/L2/L3)
    - Algorithm complexity
  - Use architecture-specific prefetch instructions:
    - x86: `_mm_prefetch` with different hints (T0, T1, T2, NTA)
    - ARM: `__builtin_prefetch`
  - Measure effectiveness with hardware performance counters
- **Subtasks:**
  - Subtask 119.1: Implement multi-level prefetching for string access
  - Subtask 119.2: Tune prefetch distances based on profiling
  - Subtask 119.3: Add architecture-specific prefetch intrinsics
  - Subtask 119.4: Measure cache miss reduction with perf counters
  - Subtask 119.5: Adaptive prefetch distance based on string length
- **Test Strategy:** Profile cache misses, measure memory stall cycles, verify benefit
- **Documentation:** Document prefetching strategy and tuning parameters

### Medium Priority Optimizations

**Task 120: Loop Fusion for Multiple Metrics (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** None
- **Description:** Compute multiple distance metrics in single pass for 30-50% speedup when multiple needed
- **Expected Impact:** 30-50% speedup when computing 2+ metrics on same data
- **Target Repository:** polars-distance
- **Implementation:**
  - Create fused computation functions:
    ```rust
    fn compute_levenshtein_and_jaro(s1: &str, s2: &str) -> (f32, f32) {
        // Share:
        // - String iteration
        // - Character comparisons
        // - Length calculations
        // Compute both metrics in single pass
        
        // Common setup
        let len1 = s1.len();
        let len2 = s2.len();
        let chars1: Vec<char> = s1.chars().collect();
        let chars2: Vec<char> = s2.chars().collect();
        
        // Parallel computation
        let lev = compute_lev_with_chars(&chars1, &chars2);
        let jaro = compute_jaro_with_chars(&chars1, &chars2);
        
        (lev, jaro)
    }
    ```
  - Common combinations to fuse:
    - Levenshtein + Jaro-Winkler (both character-based)
    - Hamming + Levenshtein (when applicable)
    - Multiple cosine variants (different norms)
  - Add API for batch metric computation:
    ```python
    # Python API
    df.select([
        pl.col("name").dist.multi_metric(
            other,
            metrics=["levenshtein", "jaro_winkler", "hamming"]
        )
    ])
    ```
  - Benefit: Amortize string processing overhead across multiple metrics
- **Subtasks:**
  - Subtask 120.1: Implement `compute_lev_and_jaro()` fused function
  - Subtask 120.2: Create generic `compute_multiple_metrics()` API
  - Subtask 120.3: Add Python bindings for multi-metric computation
  - Subtask 120.4: Benchmark fused vs separate computation
  - Subtask 120.5: Document loop fusion benefits and API usage
- **Test Strategy:** Verify correctness vs separate computation, measure speedup
- **Documentation:** Add examples for multi-metric use cases

**Task 121: Custom Memory Allocators for Specific Types (MEDIUM PRIORITY)**

- **Priority:** Medium  
- **Dependencies:** Task 116 (global allocator)
- **Description:** Use specialized allocators for frequent allocation patterns beyond global allocator
- **Expected Impact:** Additional 5-10% speedup on top of global allocator improvement
- **Target Repository:** polars-distance
- **Implementation:**
  - Type-specific allocators:
    - **Arena allocator** for temporary DP matrices (freed in bulk)
    - **Pool allocator** for fixed-size buffers (reuse without deallocation)
    - **Stack allocator** for small temporary vectors (<= 128 bytes)
  - Example arena allocator usage:
    ```rust
    struct ArenaAllocator {
        buffer: Vec<u8>,
        offset: usize,
    }
    
    impl ArenaAllocator {
        fn alloc<T>(&mut self, count: usize) -> &mut [T] {
            // Bump allocator - no individual frees
            // All freed at once when arena dropped
        }
    }
    
    fn compute_distance_with_arena(s1: &str, s2: &str) -> f32 {
        let mut arena = ArenaAllocator::new(1024);
        let dp = arena.alloc::<u32>(s1.len() * s2.len());
        // ... computation ...
        // Arena automatically freed at end of scope
    }
    ```
  - Benefits:
    - Lower allocation overhead
    - Better cache locality
    - Predictable memory layout
  - Use thread-local allocators to avoid contention
- **Subtasks:**
  - Subtask 121.1: Implement arena allocator for DP matrices
  - Subtask 121.2: Implement pool allocator for fixed-size buffers
  - Subtask 121.3: Add thread-local allocator support
  - Subtask 121.4: Integrate specialized allocators with distance functions
  - Subtask 121.5: Benchmark allocation overhead reduction
- **Test Strategy:** Measure allocation counts, verify memory safety
- **Documentation:** Document when to use each allocator type

### Phase 14 Success Criteria
- Vectorized threshold filtering provides 20-40% speedup for filtered queries
- Branchless implementations provide 10-20% speedup from better pipelining
- Advanced prefetching provides 10-20% speedup from hidden latency
- Loop fusion provides 30-50% speedup for multi-metric scenarios
- Specialized allocators provide additional 5-10% speedup
- All optimizations maintain correctness (tests pass)
- **Combined impact: 30-60% additional performance improvement on top of Phase 13**

### Phase 14 Dependencies
- Phase 14 (Tasks 117-121) should proceed after Phase 13 completion
- Task 117 (Vectorized filtering) depends on Task 113 (SIMD infrastructure)
- Tasks 118, 119, 121 are independent
- Task 120 (Loop fusion) is independent
- Task 121 (Specialized allocators) builds on Task 116 (global allocator)

---

## Overall Success Metrics

### Performance Targets (All Phases Complete)
- **Beat RapidFuzz on ALL metrics:**
  - Levenshtein: 1.2-2x faster ✅ ACHIEVED (1.24-1.63x)
  - Damerau-Levenshtein: 1.5-3x faster ✅ ACHIEVED (1.98-2.35x)
  - Jaro-Winkler: 1.5-6x faster ✅ ACHIEVED (1.19-6.00x)
  - Hamming: 2-3x faster ✅ ACHIEVED (2.34-2.56x)
  - Cosine: 20-50x faster ✅ ACHIEVED (15.50-38.68x)

- **Match or exceed pl-fuzzy-frame-match:**
  - Small datasets (<10M): Match performance ✅ Currently tied
  - Medium datasets (10-100M): Exceed by 5-10% ⚠️ Close but not consistently faster
  - Large datasets (100M+): Exceed by 10-20% with Phase 11 optimizations
  - **Target after Phase 11:** 1.1-1.5x faster across all sizes

### Feature Completeness
- ✅ All 5 similarity metrics implemented
- ✅ Full fuzzy join API with all join types
- ✅ Comprehensive blocking strategies (8 variants)
- ✅ Batch processing and parallelization
- ✅ Python bindings complete
- ✅ 177+ tests all passing
- ✅ Comprehensive documentation

### Code Quality
- All code follows Polars patterns and conventions
- Comprehensive test coverage (unit, integration, edge cases)
- Clear documentation with examples
- Performance characteristics documented
- No external dependencies except standard Polars stack

---

## Phase 15: Advanced SIMD Optimizations for Cosine Similarity and Jaro-Winkler

After analyzing the current implementations, this phase implements missing SIMD optimizations that could provide significant additional speedups for cosine similarity and Jaro-Winkler.

### Overview

Current implementation analysis reveals several optimization opportunities:

**Cosine Similarity (`polars/crates/polars-ops/src/chunked_array/array/similarity.rs`):**
- ✅ Has ARM NEON, AVX2, AVX-512 support
- ✅ Has FMA instructions
- ❌ Uses single accumulator chain (limited ILP)
- ❌ Always casts f32 embeddings to f64 (wastes throughput)
- ❌ No batch row processing (one row at a time)
- ❌ No prefetch hints for large vectors

**Jaro-Winkler (`polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`):**
- ✅ Has bit-parallel match tracking (≤64 chars)
- ✅ Has SIMD character search (32 bytes/iteration)
- ✅ Has column-level ASCII detection
- ❌ Inner loop in bit-parallel path is scalar (not SIMD)
- ❌ Transposition counting uses sequential bit scanning

### High Priority Optimizations

**Task 122: Multiple Accumulators for Cosine Similarity ILP (CRITICAL PRIORITY)**

- **Priority:** Critical
- **Dependencies:** None (enhancement to existing implementation)
- **Description:** Use 4 independent accumulators to hide FMA latency and achieve better instruction-level parallelism
- **Expected Impact:** 2-3x speedup for vectors >128 elements
- **Implementation:**
  - Current code uses single accumulator chain where each FMA waits for previous:
    ```rust
    // ❌ Current: single accumulator chain
    for i in 0..chunks {
        dot_acc = va.mul_add(vb, dot_acc);  // Each FMA waits for previous
    }
    ```
  - Use 4 independent accumulators that can execute in parallel:
    ```rust
    // ✅ Optimized: 4 independent accumulators (2-3x faster for long vectors)
    let (mut dot0, mut dot1, mut dot2, mut dot3) = (splat(0.0), splat(0.0), splat(0.0), splat(0.0));
    let (mut norm_a0, mut norm_a1, mut norm_a2, mut norm_a3) = (splat(0.0), splat(0.0), splat(0.0), splat(0.0));
    let (mut norm_b0, mut norm_b1, mut norm_b2, mut norm_b3) = (splat(0.0), splat(0.0), splat(0.0), splat(0.0));

    for i in (0..chunks).step_by(4) {
        let va0 = load(&a[i*W..]);
        let va1 = load(&a[(i+1)*W..]);
        let va2 = load(&a[(i+2)*W..]);
        let va3 = load(&a[(i+3)*W..]);
        let vb0 = load(&b[i*W..]);
        let vb1 = load(&b[(i+1)*W..]);
        let vb2 = load(&b[(i+2)*W..]);
        let vb3 = load(&b[(i+3)*W..]);
        
        // These 4 FMAs can execute in parallel on modern CPUs
        dot0 = va0.mul_add(vb0, dot0);
        dot1 = va1.mul_add(vb1, dot1);
        dot2 = va2.mul_add(vb2, dot2);
        dot3 = va3.mul_add(vb3, dot3);
        
        norm_a0 = va0.mul_add(va0, norm_a0);
        norm_a1 = va1.mul_add(va1, norm_a1);
        norm_a2 = va2.mul_add(va2, norm_a2);
        norm_a3 = va3.mul_add(va3, norm_a3);
        
        norm_b0 = vb0.mul_add(vb0, norm_b0);
        norm_b1 = vb1.mul_add(vb1, norm_b1);
        norm_b2 = vb2.mul_add(vb2, norm_b2);
        norm_b3 = vb3.mul_add(vb3, norm_b3);
    }
    
    // Reduce at the end (single reduction)
    let dot = (dot0 + dot1) + (dot2 + dot3);
    let norm_a_sq = (norm_a0 + norm_a1) + (norm_a2 + norm_a3);
    let norm_b_sq = (norm_b0 + norm_b1) + (norm_b2 + norm_b3);
    ```
  - FMA latency is 4-5 cycles, but throughput is 2 per cycle on modern CPUs
  - 4 accumulators allow full utilization of FMA units
  - Apply to all architecture paths: AVX2, AVX-512, NEON
- **Subtasks:**
  - Subtask 122.1: Implement 4-accumulator version of `dot_product_and_norms_avx2_fma()`
  - Subtask 122.2: Implement 4-accumulator version of `dot_product_and_norms_avx512()`
  - Subtask 122.3: Implement 4-accumulator version of `dot_product_and_norms_neon()`
  - Subtask 122.4: Add handling for vectors not divisible by 4*SIMD_WIDTH
  - Subtask 122.5: Benchmark single vs 4-accumulator on various vector sizes
- **Test Strategy:** Verify numerical correctness, benchmark on vectors 64-4096 elements
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/array/similarity.rs`

**Task 123: Direct f32 SIMD Path for Embeddings (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 122 (applies same multi-accumulator technique)
- **Description:** Add direct f32 SIMD path to avoid f64 casting overhead for embedding vectors
- **Expected Impact:** 2x speedup for f32 embedding vectors (common in ML)
- **Implementation:**
  - Currently all inputs are cast to f64, wasting half of SIMD throughput:
    ```rust
    // ❌ Current: always cast to f64
    let a_f64 = a.cast(&DataType::Float64).ok()?;
    ```
  - Add direct f32 path using f32x8 (AVX2) or f32x16 (AVX-512):
    ```rust
    // ✅ Optimized: detect f32 and use native SIMD
    if a.dtype() == &DataType::Float32 && b.dtype() == &DataType::Float32 {
        return cosine_similarity_f32(a, b);
    }
    
    fn dot_product_and_norms_f32_avx2(a: &[f32], b: &[f32]) -> (f32, f32, f32) {
        const SIMD_WIDTH: usize = 8;  // f32x8 for AVX2 (vs f64x4)
        type SimdF32 = Simd<f32, SIMD_WIDTH>;
        
        // Process 8 floats per iteration instead of 4 doubles
        // Double throughput for f32 embeddings
    }
    
    fn dot_product_and_norms_f32_avx512(a: &[f32], b: &[f32]) -> (f32, f32, f32) {
        const SIMD_WIDTH: usize = 16;  // f32x16 for AVX-512
        type SimdF32 = Simd<f32, SIMD_WIDTH>;
        // Process 16 floats per iteration
    }
    ```
  - f32 is sufficient precision for most ML embeddings
  - Most embedding models (BERT, GPT, etc.) output f32
- **Subtasks:**
  - Subtask 123.1: Implement `dot_product_and_norms_f32_avx2()` with f32x8 vectors
  - Subtask 123.2: Implement `dot_product_and_norms_f32_avx512()` with f32x16 vectors
  - Subtask 123.3: Implement `dot_product_and_norms_f32_neon()` with f32x4 vectors
  - Subtask 123.4: Add dtype detection in `compute_cosine_similarity()` to choose path
  - Subtask 123.5: Benchmark f32 native vs f64 cast on typical embedding sizes (384, 768, 1536)
- **Test Strategy:** Verify numerical accuracy is sufficient for similarity, benchmark vs f64 path
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/array/similarity.rs`

**Task 124: SIMD Inner Loop for Jaro-Winkler Bit-Parallel Path (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** None (enhancement to existing implementation)
- **Description:** Vectorize the character search inner loop in `jaro_similarity_bitparallel()` which currently uses scalar comparison
- **Expected Impact:** 30-50% speedup for strings 32-64 characters
- **Implementation:**
  - Current scalar inner loop:
    ```rust
    // ❌ Current: scalar inner loop
    for j in start..end {
        if (s2_matches & (1u64 << j)) == 0 && s2[j] == needle {
            s1_matches |= 1u64 << i;
            s2_matches |= 1u64 << j;
            matches += 1;
            break;
        }
    }
    ```
  - SIMD-accelerated character search with bitmask check:
    ```rust
    // ✅ Optimized: SIMD character search with bit mask check
    fn find_unmatched_char_simd(
        needle: u8, 
        s2: &[u8], 
        start: usize, 
        end: usize, 
        s2_matches: u64
    ) -> Option<usize> {
        const W: usize = 32;
        type SimdU8 = Simd<u8, W>;
        
        let needle_vec = SimdU8::splat(needle);
        let search_slice = &s2[start..end.min(s2.len())];
        
        for chunk_start in (0..search_slice.len()).step_by(W) {
            let chunk_end = (chunk_start + W).min(search_slice.len());
            if chunk_end - chunk_start < W { break; } // Handle remainder scalar
            
            let haystack = SimdU8::from_slice(&search_slice[chunk_start..chunk_start + W]);
            let eq_mask = haystack.simd_eq(needle_vec).to_bitmask();
            
            if eq_mask != 0 {
                // Extract positions where s2[j] == needle AND !already_matched
                let base_pos = start + chunk_start;
                let unmatched_mask = if base_pos + W <= 64 {
                    !((s2_matches >> base_pos) as u32) // Extract relevant bits
                } else {
                    u32::MAX // All positions are "unmatched" for positions > 64
                };
                
                let combined = eq_mask as u32 & unmatched_mask;
                if combined != 0 {
                    return Some(base_pos + combined.trailing_zeros() as usize);
                }
            }
        }
        
        // Scalar fallback for remainder
        for j in (start + (search_slice.len() / W) * W)..end {
            if (s2_matches & (1u64 << j)) == 0 && s2[j] == needle {
                return Some(j);
            }
        }
        None
    }
    ```
  - Key insight: Current bit-parallel uses u64 bitmasks for match tracking but scalar search
  - SIMD search finds character positions fast, then check against match bitmask
- **Subtasks:**
  - Subtask 124.1: Implement `find_unmatched_char_simd()` helper function
  - Subtask 124.2: Integrate SIMD search into `jaro_similarity_bitparallel()` match-finding loop
  - Subtask 124.3: Handle edge cases: search window crossing 32-byte boundaries
  - Subtask 124.4: Optimize for common case where needle found in first SIMD chunk
  - Subtask 124.5: Benchmark bit-parallel with SIMD search vs scalar search
- **Test Strategy:** Verify correctness on strings 32-64 chars, benchmark speedup
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 125: Fast Transposition Counting with Popcount (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 124 (both optimize bit-parallel path)
- **Description:** Optimize transposition counting using efficient bit manipulation instead of sequential iteration
- **Expected Impact:** 10-20% speedup for bit-parallel path
- **Implementation:**
  - Current sequential bit scanning:
    ```rust
    // ❌ Current: sequential bit scanning
    for i in 0..len1 {
        if (s1_matches & (1u64 << i)) == 0 { continue; }
        // find next match in s2_matches...
    }
    ```
  - Optimized using `trailing_zeros()` to skip directly to set bits:
    ```rust
    // ✅ Optimized: iterate only through set bits
    fn count_transpositions_fast(s1: &[u8], s2: &[u8], s1_matches: u64, s2_matches: u64) -> u32 {
        let mut trans = 0u32;
        let mut remaining_s2 = s2_matches;
        
        // Iterate only through set bits (skip zeros efficiently)
        let mut s1_bits = s1_matches;
        while s1_bits != 0 {
            let i = s1_bits.trailing_zeros() as usize;
            s1_bits &= s1_bits - 1; // Clear lowest set bit (Kernighan's algorithm)
            
            // Find lowest set bit in remaining_s2
            let j = remaining_s2.trailing_zeros() as usize;
            remaining_s2 &= remaining_s2 - 1; // Clear lowest set bit
            
            // Branchless comparison
            trans += (s1[i] != s2[j]) as u32;
        }
        trans / 2
    }
    ```
  - Key optimizations:
    - `trailing_zeros()` is a single CPU instruction (TZCNT/CTZ)
    - `x &= x - 1` clears lowest bit in single instruction
    - Branchless transposition increment
    - Only iterates through matched positions (typically ~30% of string length)
- **Subtasks:**
  - Subtask 125.1: Implement `count_transpositions_fast()` with bit manipulation
  - Subtask 125.2: Replace existing transposition counting in `jaro_similarity_bitparallel()`
  - Subtask 125.3: Add unit tests for edge cases (all matches, no matches, single match)
  - Subtask 125.4: Benchmark transposition counting speedup in isolation
  - Subtask 125.5: Measure overall Jaro-Winkler improvement
- **Test Strategy:** Verify transposition counts match existing implementation, benchmark
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Medium Priority Optimizations

**Task 126: Batch Row Processing for Cosine Similarity (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Task 122, Task 123
- **Description:** Process multiple row pairs simultaneously to amortize overhead and improve memory bandwidth utilization
- **Expected Impact:** 20-40% speedup for large DataFrame operations
- **Implementation:**
  - Current: Process one row pair at a time
  - Optimized: Process 4 row pairs simultaneously:
    ```rust
    fn cosine_similarity_batch_4(
        a_rows: [&[f64]; 4],
        b_rows: [&[f64]; 4],
    ) -> [f64; 4] {
        // Interleave loads from all 4 pairs for better memory bandwidth
        // Compute 4 similarities with shared loop overhead
        
        let mut dots = [0.0; 4];
        let mut norms_a = [0.0; 4];
        let mut norms_b = [0.0; 4];
        
        // Assuming all rows same length
        let len = a_rows[0].len();
        
        for i in 0..len {
            // Load 4 values from each row (memory streaming)
            let a0 = a_rows[0][i];
            let a1 = a_rows[1][i];
            let a2 = a_rows[2][i];
            let a3 = a_rows[3][i];
            let b0 = b_rows[0][i];
            let b1 = b_rows[1][i];
            let b2 = b_rows[2][i];
            let b3 = b_rows[3][i];
            
            // Parallel accumulation
            dots[0] += a0 * b0;
            dots[1] += a1 * b1;
            dots[2] += a2 * b2;
            dots[3] += a3 * b3;
            
            norms_a[0] += a0 * a0;
            norms_a[1] += a1 * a1;
            // ... etc
        }
        
        // Compute final similarities
        [
            dots[0] / (norms_a[0].sqrt() * norms_b[0].sqrt()),
            dots[1] / (norms_a[1].sqrt() * norms_b[1].sqrt()),
            dots[2] / (norms_a[2].sqrt() * norms_b[2].sqrt()),
            dots[3] / (norms_a[3].sqrt() * norms_b[3].sqrt()),
        ]
    }
    ```
  - Benefits:
    - Amortize loop overhead across 4 rows
    - Better memory bandwidth utilization
    - Can use SIMD across rows instead of within rows
- **Subtasks:**
  - Subtask 126.1: Implement `cosine_similarity_batch_4()` function
  - Subtask 126.2: Update `cosine_similarity_arr()` to use batch processing
  - Subtask 126.3: Handle remainder rows (1-3 rows) efficiently
  - Subtask 126.4: Add SIMD acceleration across rows using f64x4
  - Subtask 126.5: Benchmark batch vs individual row processing
- **Test Strategy:** Verify correctness, benchmark on 1K-100K rows
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/array/similarity.rs`

**Task 127: Prefetch Hints for Large Vectors (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** None
- **Description:** Add software prefetch hints for upcoming data in cosine similarity hot loops
- **Expected Impact:** 10-15% speedup for large vectors with cache misses
- **Implementation:**
  - Add prefetch intrinsics to SIMD loops:
    ```rust
    #[cfg(target_arch = "x86_64")]
    use std::arch::x86_64::{_mm_prefetch, _MM_HINT_T0};
    
    fn dot_product_and_norms_with_prefetch(a: &[f64], b: &[f64]) -> (f64, f64, f64) {
        const SIMD_WIDTH: usize = 4;
        const PREFETCH_DISTANCE: usize = 8; // Prefetch 8 chunks ahead
        
        for i in 0..chunks {
            // Prefetch data for iteration i+PREFETCH_DISTANCE while computing i
            if i + PREFETCH_DISTANCE < chunks {
                let prefetch_offset = (i + PREFETCH_DISTANCE) * SIMD_WIDTH;
                unsafe {
                    _mm_prefetch(
                        a.as_ptr().add(prefetch_offset) as *const i8, 
                        _MM_HINT_T0  // Prefetch to L1 cache
                    );
                    _mm_prefetch(
                        b.as_ptr().add(prefetch_offset) as *const i8, 
                        _MM_HINT_T0
                    );
                }
            }
            
            // Compute current iteration
            let offset = i * SIMD_WIDTH;
            let va = SimdF64::from_slice(&a[offset..offset + SIMD_WIDTH]);
            let vb = SimdF64::from_slice(&b[offset..offset + SIMD_WIDTH]);
            // ... accumulation
        }
    }
    ```
  - Tune prefetch distance based on:
    - Cache line size (64 bytes)
    - Computation latency per iteration
    - Memory latency (~100 cycles)
  - Add ARM-specific prefetch for NEON path
- **Subtasks:**
  - Subtask 127.1: Add prefetch intrinsics to `dot_product_and_norms_avx2_fma()`
  - Subtask 127.2: Add prefetch intrinsics to `dot_product_and_norms_avx512()`
  - Subtask 127.3: Add prefetch intrinsics to `dot_product_and_norms_neon()`
  - Subtask 127.4: Tune prefetch distance based on profiling
  - Subtask 127.5: Benchmark with hardware performance counters (cache misses)
- **Test Strategy:** Use perf to measure cache miss reduction, verify speedup
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/array/similarity.rs`

**Task 128: Pre-Normalized Vector Fast Path (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** None
- **Description:** Add optimized path for pre-normalized vectors where cosine = dot product
- **Expected Impact:** ~33% speedup for pre-normalized embeddings (skip norm computation)
- **Implementation:**
  - Many ML embeddings are already L2-normalized (unit vectors)
  - For normalized vectors: cosine_similarity = dot_product
  - Add parameter to indicate pre-normalized:
    ```rust
    pub fn cosine_similarity_arr_normalized(
        ca: &ArrayChunked, 
        other: &ArrayChunked,
        normalized: bool,  // If true, assume unit vectors
    ) -> PolarsResult<Float64Chunked> {
        if normalized {
            // Fast path: just compute dot product
            return dot_product_arr(ca, other);
        }
        // Regular path with norm computation
        cosine_similarity_arr(ca, other)
    }
    
    fn dot_product_simd(a: &[f64], b: &[f64]) -> f64 {
        // Only compute dot product, skip norm calculations
        // 1/3 of the FLOPs of full cosine similarity
    }
    ```
  - Add Python API:
    ```python
    df.select(
        pl.col("embedding").arr.cosine_similarity(other, normalized=True)
    )
    ```
- **Subtasks:**
  - Subtask 128.1: Implement `dot_product_simd()` for f64 vectors
  - Subtask 128.2: Implement `dot_product_simd_f32()` for f32 vectors
  - Subtask 128.3: Add `normalized` parameter to `cosine_similarity_arr()`
  - Subtask 128.4: Update Python bindings to expose `normalized` parameter
  - Subtask 128.5: Add documentation explaining when to use normalized=True
- **Test Strategy:** Verify results match full computation for unit vectors, benchmark 33% speedup
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/array/similarity.rs`

### Phase 15 Success Criteria
- Multiple accumulators provide 2-3x speedup for large vectors (>128 elements)
- Direct f32 path provides 2x speedup for f32 embeddings
- SIMD inner loop provides 30-50% speedup for 32-64 char strings
- Fast transposition counting provides 10-20% speedup for bit-parallel path
- Batch row processing provides 20-40% speedup for DataFrame operations
- Prefetch hints provide 10-15% speedup for large vectors
- Pre-normalized path provides 33% speedup for unit vectors
- All optimizations maintain correctness (all tests pass)
- **Combined impact: 2-4x additional speedup for cosine, 30-50% for Jaro-Winkler**

### Phase 15 Dependencies
- Phase 15 (Tasks 122-128) can proceed after Phase 14
- Task 122 (Multiple Accumulators) is independent
- Task 123 (f32 Path) can use techniques from Task 122
- Task 124 (SIMD Inner Loop) is independent
- Task 125 (Fast Transposition) depends on Task 124 (both optimize bit-parallel)
- Task 126 (Batch Rows) depends on Tasks 122, 123
- Task 127 (Prefetch) is independent
- Task 128 (Pre-Normalized) is independent

---

## Phase 16: Critical Gap Closure for Lagging Metrics

After Phase 15's SIMD optimizations, this phase addresses the fundamental algorithmic gaps causing Damerau-Levenshtein and Jaro-Winkler to lag behind RapidFuzz at scale.

### Overview

Current benchmark analysis reveals critical optimization gaps:

**Damerau-Levenshtein (RapidFuzz 1.4-1.8x faster):**
- ❌ **No Myers' Bit-Parallel**: RapidFuzz uses bit-parallel for short strings
- ❌ **No Diagonal Band**: This is what makes Levenshtein fast - DL doesn't have it!
- ⚠️ **Transposition dependency breaks SIMD**: The `prev_prev_row[i-2] + 1` check creates data dependency

**Jaro-Winkler (RapidFuzz 1.3-2.7x faster at scale):**
- ⚠️ **Core matching loop is scalar**: SIMD only used for auxiliary operations
- ❌ **No bit-parallel for >64 chars**: Falls back to hash-based which is slower
- ⚠️ **O(n²) match window iteration**: Quadratic behavior for long strings

### Critical Damerau-Levenshtein Optimizations

**Task 129: Myers' Bit-Parallel for Damerau-Levenshtein (CRITICAL PRIORITY)**

- **Priority:** Critical
- **Dependencies:** None (standalone optimization)
- **Description:** Implement Myers' bit-parallel algorithm for Damerau-Levenshtein on short strings (≤64 chars), matching what RapidFuzz does
- **Expected Impact:** 2-3x speedup for short strings (the majority of real-world data)
- **Root Cause:** Current DL uses O(m×n) DP even for short strings where bit-parallel is much faster
- **Implementation:**
  - Extend Myers' bit-parallel to handle transpositions
  - Key insight: Track transposition positions with an additional bitmask
  - Algorithm outline:
    ```rust
    fn damerau_levenshtein_myers(s1: &[u8], s2: &[u8]) -> usize {
        // Standard Myers setup
        let mut pv: u64 = !0;  // Positive vertical delta
        let mut mv: u64 = 0;   // Negative vertical delta
        let mut score = s1.len();
        
        // Character position bitmasks (same as Myers)
        let mut peq = [0u64; 256];
        for (i, &c) in s1.iter().enumerate() {
            peq[c as usize] |= 1 << i;
        }
        
        // Track previous character for transposition detection
        let mut prev_char = 0u8;
        let mut prev_eq = 0u64;
        
        for (j, &c) in s2.iter().enumerate() {
            let eq = peq[c as usize];
            
            // Standard Myers bit operations
            let xv = eq | mv;
            let xh = ((eq & pv).wrapping_add(pv)) ^ pv | eq;
            let ph = mv | !(xh | pv);
            let mh = pv & xh;
            
            // Transposition check: if current char matches position j-1 in s1
            // AND previous char matches position j in s1
            if j > 0 {
                let trans_possible = (prev_eq >> 1) & eq;
                // Adjust score if transposition is cheaper
                // This is the key extension over standard Myers
            }
            
            // Update score and vectors
            score = score.wrapping_add(((ph >> (s1.len() - 1)) & 1) as usize);
            score = score.wrapping_sub(((mh >> (s1.len() - 1)) & 1) as usize);
            
            // Shift for next iteration
            pv = (mh << 1) | !(xv | (ph << 1));
            mv = (ph << 1) & xv;
            
            prev_char = c;
            prev_eq = eq;
        }
        
        score
    }
    ```
  - Integrate into `damerau_levenshtein_distance_bytes()` dispatch:
    - Use Myers for strings ≤64 chars (single u64 word)
    - Fall back to existing SIMD DP for longer strings
- **Subtasks:**
  - Subtask 129.1: Implement core `damerau_levenshtein_myers()` function
  - Subtask 129.2: Add transposition tracking bitmask logic
  - Subtask 129.3: Integrate into main dispatch in `damerau_levenshtein_distance_bytes()`
  - Subtask 129.4: Add unit tests comparing against existing implementation
  - Subtask 129.5: Benchmark Myers vs DP for strings 10-64 chars
- **Test Strategy:** Verify correctness against existing implementation, benchmark on short strings
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 130: Diagonal Band Optimization for Damerau-Levenshtein (CRITICAL PRIORITY)**

- **Priority:** Critical
- **Dependencies:** None
- **Description:** Implement diagonal band algorithm for Damerau-Levenshtein to reduce complexity from O(m×n) to O(m×k) where k is the band width
- **Expected Impact:** 5-10x speedup for medium-long strings (this is the #1 missing optimization!)
- **Root Cause:** Current DL computes full O(m×n) matrix; Levenshtein's diagonal band gives it a huge advantage
- **Implementation:**
  - Similar to `levenshtein_distance_adaptive_band()` but with transposition handling
  - Key challenge: Transposition check looks at `prev_prev_row[i-2]` which may be outside the band
  - Solution: Extend band by 1 on each side to accommodate transposition check
  - Algorithm outline:
    ```rust
    fn damerau_levenshtein_distance_banded(s1: &[u8], s2: &[u8], max_distance: usize) -> Option<usize> {
        let m = s1.len();
        let n = s2.len();
        
        // Length difference check
        if m.abs_diff(n) > max_distance {
            return None;
        }
        
        // Band width = max_distance + 1 (extended for transposition)
        let band_width = max_distance + 2;  // +2 for transposition lookahead
        
        // Use 3 rows for DL (prev_prev, prev, curr)
        let mut prev_prev_row = vec![u32::MAX; band_width * 2 + 3];
        let mut prev_row = vec![u32::MAX; band_width * 2 + 3];
        let mut curr_row = vec![u32::MAX; band_width * 2 + 3];
        
        // Initialize first row within band
        for k in 0..=band_width.min(n) {
            prev_row[k] = k as u32;
        }
        
        for i in 1..=m {
            // Compute band boundaries for this row
            let j_min = i.saturating_sub(band_width);
            let j_max = (i + band_width).min(n);
            
            // Initialize edge of band
            curr_row[0] = if j_min == 0 { i as u32 } else { u32::MAX };
            
            for j in j_min.max(1)..=j_max {
                let k = j - j_min;  // Index into band
                let cost = if s1[i-1] == s2[j-1] { 0u32 } else { 1u32 };
                
                // Standard DL recurrence
                let mut min_cost = u32::MAX;
                
                // Deletion
                if k > 0 {
                    min_cost = min_cost.min(prev_row[k-1].saturating_add(1));
                }
                // Insertion
                min_cost = min_cost.min(curr_row[k].saturating_add(1));
                // Substitution
                if k > 0 {
                    min_cost = min_cost.min(prev_row[k].saturating_add(cost));
                }
                
                // Transposition (the DL-specific operation)
                if i > 1 && j > 1 && s1[i-1] == s2[j-2] && s1[i-2] == s2[j-1] {
                    // Need to access prev_prev_row[j-2] which is k-1 in band coords
                    if k >= 2 {
                        min_cost = min_cost.min(prev_prev_row[k-2].saturating_add(1));
                    }
                }
                
                curr_row[k+1] = min_cost;
            }
            
            // Rotate rows
            std::mem::swap(&mut prev_prev_row, &mut prev_row);
            std::mem::swap(&mut prev_row, &mut curr_row);
        }
        
        let result = prev_row[n - m.saturating_sub(band_width) + 1];
        if result <= max_distance as u32 {
            Some(result as usize)
        } else {
            None
        }
    }
    ```
  - Integrate with existing SIMD infrastructure for band computation
- **Subtasks:**
  - Subtask 130.1: Implement core `damerau_levenshtein_distance_banded()` function
  - Subtask 130.2: Handle transposition across band boundaries
  - Subtask 130.3: Add adaptive band width estimation from length difference
  - Subtask 130.4: Add SIMD min operations within band (reuse from Levenshtein)
  - Subtask 130.5: Integrate into main dispatch for strings >64 chars
  - Subtask 130.6: Benchmark banded vs full matrix on 50-500 char strings
- **Test Strategy:** Verify correctness against full matrix, benchmark speedup
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 131: Transposition-Aware SIMD for Damerau-Levenshtein (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 130 (builds on banded algorithm)
- **Description:** Optimize the transposition check to work efficiently with SIMD by pre-computing transposition positions
- **Expected Impact:** Additional 30-50% speedup on top of diagonal band
- **Root Cause:** Current transposition check breaks SIMD parallelism due to data dependency
- **Implementation:**
  - Pre-compute positions where transposition is possible:
    ```rust
    fn precompute_transposition_positions(s1: &[u8], s2: &[u8]) -> Vec<(usize, usize)> {
        // Find all (i, j) where s1[i] == s2[j-1] AND s1[i-1] == s2[j]
        let mut positions = Vec::new();
        for i in 1..s1.len() {
            for j in 1..s2.len() {
                if s1[i] == s2[j-1] && s1[i-1] == s2[j] {
                    positions.push((i, j));
                }
            }
        }
        positions
    }
    ```
  - Use bitmap to mark transposition-eligible cells:
    ```rust
    // For cells in same SIMD lane, process normally
    // For transposition positions, do scalar update after SIMD pass
    fn damerau_levenshtein_simd_with_transpositions(
        s1: &[u8], s2: &[u8], 
        trans_positions: &[(usize, usize)]
    ) -> usize {
        // Phase 1: Standard Levenshtein SIMD (without transpositions)
        // Phase 2: Apply transposition corrections at marked positions
    }
    ```
  - Alternative: Two-pass algorithm:
    1. First pass: Compute standard Levenshtein with SIMD
    2. Second pass: Check and apply transposition improvements
- **Subtasks:**
  - Subtask 131.1: Implement `precompute_transposition_positions()` function
  - Subtask 131.2: Create two-pass SIMD algorithm (Levenshtein + transposition fix)
  - Subtask 131.3: Optimize transposition position lookup with spatial indexing
  - Subtask 131.4: Benchmark two-pass vs single-pass with dependency
  - Subtask 131.5: Profile to verify SIMD utilization improvement
- **Test Strategy:** Verify correctness, profile SIMD lane utilization
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Critical Jaro-Winkler Optimizations

**Task 132: Fully Vectorized Match-Finding Loop for Jaro-Winkler (CRITICAL PRIORITY)**

- **Priority:** Critical
- **Dependencies:** None
- **Description:** Replace the scalar inner loop in Jaro match-finding with fully vectorized SIMD implementation
- **Expected Impact:** 2-4x speedup for strings >64 chars (where RapidFuzz beats us)
- **Root Cause:** Current code uses SIMD for auxiliary operations but core match-finding is scalar
- **Current Problem Code:**
  ```rust
  // ❌ Current: Scalar inner loop (THIS IS WHY WE'RE SLOW!)
  for j in start..=end {
      if !s2_matched[j] && s1[i] == s2[j] {
          s1_matched[i] = true;
          s2_matched[j] = true;
          matches += 1;
          break;
      }
  }
  ```
- **Implementation:**
  - Vectorize the entire match-finding loop:
    ```rust
    fn jaro_find_matches_simd(
        s1: &[u8], 
        s2: &[u8], 
        match_distance: usize
    ) -> (u32, Vec<u8>, Vec<u8>) {
        const W: usize = 32;
        type SimdU8 = Simd<u8, W>;
        
        let mut matches = 0u32;
        let mut s1_matched = vec![0u8; s1.len()];
        let mut s2_matched = vec![0u8; s2.len()];
        
        for i in 0..s1.len() {
            let needle = s1[i];
            let needle_vec = SimdU8::splat(needle);
            
            let start = i.saturating_sub(match_distance);
            let end = (i + match_distance + 1).min(s2.len());
            
            // SIMD search within match window
            let mut found = false;
            let mut j = start;
            
            while j + W <= end && !found {
                // Load 32 characters from s2
                let haystack = SimdU8::from_slice(&s2[j..j + W]);
                
                // Load 32 match flags
                let matched_flags = SimdU8::from_slice(&s2_matched[j..j + W]);
                
                // Find positions where: s2[j] == needle AND s2_matched[j] == 0
                let char_eq = haystack.simd_eq(needle_vec);
                let not_matched = matched_flags.simd_eq(SimdU8::splat(0));
                let valid = char_eq & not_matched;
                
                let mask = valid.to_bitmask();
                if mask != 0 {
                    // Found a match! Get first position
                    let offset = mask.trailing_zeros() as usize;
                    let match_pos = j + offset;
                    
                    s1_matched[i] = 1;
                    s2_matched[match_pos] = 1;
                    matches += 1;
                    found = true;
                }
                
                j += W;
            }
            
            // Scalar fallback for remainder
            if !found {
                for jj in j..end {
                    if s2_matched[jj] == 0 && s2[jj] == needle {
                        s1_matched[i] = 1;
                        s2_matched[jj] = 1;
                        matches += 1;
                        break;
                    }
                }
            }
        }
        
        (matches, s1_matched, s2_matched)
    }
    ```
  - Key insight: Load both characters AND match flags in SIMD, combine with AND
  - Process 32 characters per iteration instead of 1
- **Subtasks:**
  - Subtask 132.1: Implement `jaro_find_matches_simd()` with full vectorization
  - Subtask 132.2: Handle match window boundaries correctly
  - Subtask 132.3: Optimize match flag storage (consider u64 bitmask for ≤64 chars)
  - Subtask 132.4: Add ARM NEON version using `uint8x16_t`
  - Subtask 132.5: Integrate with existing `jaro_similarity_bytes_simd_large()`
  - Subtask 132.6: Benchmark vectorized vs scalar match-finding on 100-500 char strings
- **Test Strategy:** Verify correctness, benchmark on long strings
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 133: Block-Based Jaro for Very Long Strings (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 132
- **Description:** Implement block-based processing for very long strings (>256 chars) to improve cache efficiency
- **Expected Impact:** 30-50% speedup for very long strings
- **Root Cause:** For very long strings, match window can span 100+ chars, causing cache thrashing
- **Implementation:**
  - Process strings in cache-friendly blocks:
    ```rust
    fn jaro_similarity_blocked(s1: &[u8], s2: &[u8]) -> f32 {
        const BLOCK_SIZE: usize = 64;  // Fits in L1 cache
        
        let match_distance = (s1.len().max(s2.len()) / 2).saturating_sub(1);
        
        // If match distance is small, use direct SIMD
        if match_distance <= BLOCK_SIZE {
            return jaro_similarity_simd_direct(s1, s2);
        }
        
        // Block-based processing for large match distances
        let mut total_matches = 0u32;
        let mut s1_matched = vec![false; s1.len()];
        let mut s2_matched = vec![false; s2.len()];
        
        // Process s1 in blocks
        for block_start in (0..s1.len()).step_by(BLOCK_SIZE) {
            let block_end = (block_start + BLOCK_SIZE).min(s1.len());
            
            // Match window for this block
            let window_start = block_start.saturating_sub(match_distance);
            let window_end = (block_end + match_distance).min(s2.len());
            
            // Process matches for this block
            // Only need to load window_start..window_end from s2
            let matches = find_matches_in_block(
                &s1[block_start..block_end],
                &s2[window_start..window_end],
                &mut s1_matched[block_start..block_end],
                &mut s2_matched[window_start..window_end],
            );
            
            total_matches += matches;
        }
        
        // Count transpositions and compute Jaro score
        let transpositions = count_transpositions(&s1_matched, &s2_matched, s1, s2);
        compute_jaro_score(total_matches, transpositions, s1.len(), s2.len())
    }
    ```
  - Benefits:
    - Better cache locality (block + window fit in L1)
    - Reduced memory bandwidth
    - Can parallelize blocks with Rayon
- **Subtasks:**
  - Subtask 133.1: Implement `jaro_similarity_blocked()` function
  - Subtask 133.2: Implement `find_matches_in_block()` with SIMD
  - Subtask 133.3: Handle block boundary edge cases
  - Subtask 133.4: Add parallel block processing with Rayon
  - Subtask 133.5: Tune block size for optimal cache utilization
  - Subtask 133.6: Benchmark blocked vs non-blocked on 500-2000 char strings
- **Test Strategy:** Verify correctness, profile cache misses
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 134: Hybrid Algorithm Selection for Jaro-Winkler (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Tasks 132, 133
- **Description:** Implement intelligent algorithm selection based on string characteristics to always use optimal path
- **Expected Impact:** 10-20% overall improvement by always choosing best algorithm
- **Implementation:**
  - Create unified dispatch that selects optimal algorithm:
    ```rust
    fn jaro_similarity_optimal(s1: &[u8], s2: &[u8]) -> f32 {
        let len1 = s1.len();
        let len2 = s2.len();
        let max_len = len1.max(len2);
        
        // Select optimal algorithm based on string characteristics
        match max_len {
            // Very short: Direct comparison (no SIMD overhead)
            0..=8 => jaro_similarity_tiny(s1, s2),
            
            // Short: Bit-parallel with u64 bitmasks
            9..=64 => jaro_similarity_bitparallel_optimized(s1, s2),
            
            // Medium: Fully vectorized SIMD
            65..=256 => jaro_similarity_simd_vectorized(s1, s2),
            
            // Long: Block-based with SIMD
            257..=1024 => jaro_similarity_blocked(s1, s2),
            
            // Very long: Parallel block-based
            _ => jaro_similarity_parallel_blocked(s1, s2),
        }
    }
    ```
  - Add character diversity analysis for hash-based path:
    ```rust
    fn should_use_hash_based(s1: &[u8], s2: &[u8]) -> bool {
        // Hash-based is better when character diversity is high
        // (many unique characters = less benefit from SIMD char search)
        let unique_s1 = count_unique_chars(s1);
        let unique_s2 = count_unique_chars(s2);
        
        // If >50% unique characters, hash-based may be faster
        unique_s1 > s1.len() / 2 || unique_s2 > s2.len() / 2
    }
    ```
- **Subtasks:**
  - Subtask 134.1: Implement `jaro_similarity_optimal()` dispatch function
  - Subtask 134.2: Add `jaro_similarity_tiny()` for very short strings
  - Subtask 134.3: Implement character diversity analysis
  - Subtask 134.4: Add benchmarks for each algorithm on each length range
  - Subtask 134.5: Tune length thresholds based on benchmarks
- **Test Strategy:** Benchmark on representative string length distribution
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Phase 16 Success Criteria
- Myers' bit-parallel for DL provides 2-3x speedup for strings ≤64 chars
- Diagonal band for DL provides 5-10x speedup for medium-long strings
- **Damerau-Levenshtein should be 1.2-1.5x FASTER than RapidFuzz after all optimizations**
- Fully vectorized Jaro match-finding provides 2-4x speedup for long strings
- Block-based Jaro provides 30-50% additional speedup for very long strings
- **Jaro-Winkler should be 1.5-2x FASTER than RapidFuzz at all scales**
- All optimizations maintain correctness (all tests pass)
- Benchmark results documented and verified against RapidFuzz

### Phase 16 Dependencies
- Phase 16 (Tasks 129-134) can proceed after Phase 15
- Task 129 (Myers DL) is independent
- Task 130 (Banded DL) is independent
- Task 131 (Transposition SIMD) depends on Task 130
- Task 132 (Vectorized Jaro) is independent
- Task 133 (Blocked Jaro) depends on Task 132
- Task 134 (Hybrid Selection) depends on Tasks 132, 133

---

## Phase 17: RapidFuzz Parity Optimizations (Critical Performance Gap Closers)

### Overview
Based on deep analysis of RapidFuzz's C++ implementation (https://github.com/rapidfuzz/rapidfuzz-cpp), this phase implements the key algorithmic optimizations that give RapidFuzz its performance edge. These are not SIMD optimizations but fundamental algorithm improvements that reduce computational complexity.

### Phase 17 Goals
- Close the remaining performance gap with RapidFuzz for Jaro-Winkler
- Achieve 2-3x additional speedup for Levenshtein on high-similarity strings
- Enable competitive performance for fuzzy join batch operations
- Match RapidFuzz's algorithmic sophistication while maintaining Polars code style

### Phase 17 Tasks (135-140)

**Task 135: Common Prefix/Suffix Removal (HIGH PRIORITY - QUICK WIN)**

- **Priority:** Critical
- **Dependencies:** None
- **Description:** Before running any edit distance algorithm, remove common prefixes and suffixes from both strings. This is the #1 missing optimization - RapidFuzz does this for ALL algorithms (Levenshtein, Damerau-Levenshtein, Jaro).
- **Expected Impact:** 10-50% speedup for similar strings (the common case in fuzzy matching!)
- **Rationale:** When comparing "hello world" to "hello universe", we currently process all characters. RapidFuzz first removes "hello " (prefix) leaving us with "world" vs "universe" - a much smaller problem.
- **Implementation:**
  ```rust
  /// Remove common prefix and suffix from two byte slices.
  /// Returns (s1_trimmed, s2_trimmed, prefix_len, suffix_len)
  #[inline(always)]
  fn remove_common_affix<'a>(s1: &'a [u8], s2: &'a [u8]) -> (&'a [u8], &'a [u8], usize, usize) {
      // Find common prefix using SIMD comparison where possible
      let prefix_len = s1.iter()
          .zip(s2.iter())
          .take_while(|(a, b)| a == b)
          .count();
      
      let s1_after_prefix = &s1[prefix_len..];
      let s2_after_prefix = &s2[prefix_len..];
      
      // Find common suffix (excluding already-matched prefix)
      let suffix_len = s1_after_prefix.iter().rev()
          .zip(s2_after_prefix.iter().rev())
          .take_while(|(a, b)| a == b)
          .count();
      
      let s1_trimmed = &s1_after_prefix[..s1_after_prefix.len() - suffix_len];
      let s2_trimmed = &s2_after_prefix[..s2_after_prefix.len() - suffix_len];
      
      (s1_trimmed, s2_trimmed, prefix_len, suffix_len)
  }
  
  /// SIMD-accelerated prefix finding for long strings
  #[cfg(feature = "simd")]
  #[inline(always)]
  fn find_common_prefix_simd(s1: &[u8], s2: &[u8]) -> usize {
      const SIMD_WIDTH: usize = 32;
      type SimdU8 = Simd<u8, SIMD_WIDTH>;
      
      let len = s1.len().min(s2.len());
      let chunks = len / SIMD_WIDTH;
      
      for i in 0..chunks {
          let offset = i * SIMD_WIDTH;
          let v1 = SimdU8::from_slice(&s1[offset..]);
          let v2 = SimdU8::from_slice(&s2[offset..]);
          let mask = v1.simd_ne(v2);
          if mask.any() {
              // Find first mismatch within chunk
              let mask_bits = mask.to_bitmask();
              return offset + mask_bits.trailing_zeros() as usize;
          }
      }
      
      // Scalar fallback for remainder
      let remainder_start = chunks * SIMD_WIDTH;
      for i in remainder_start..len {
          if s1[i] != s2[i] {
              return i;
          }
      }
      len
  }
  ```
- **Integration Points:**
  - `levenshtein_distance_*()` - Call at function entry before any algorithm
  - `damerau_levenshtein_*()` - Call at function entry
  - `jaro_similarity_*()` - For Jaro, prefix removal is already used for Winkler bonus, but suffix removal is new
- **Subtasks:**
  - Subtask 135.1: Implement `remove_common_affix()` function
  - Subtask 135.2: Implement SIMD-accelerated `find_common_prefix_simd()` for strings > 64 bytes
  - Subtask 135.3: Implement SIMD-accelerated `find_common_suffix_simd()` for strings > 64 bytes
  - Subtask 135.4: Integrate into `levenshtein_distance_adaptive_band()` and all Levenshtein variants
  - Subtask 135.5: Integrate into `damerau_levenshtein_distance_banded()` and all DL variants
  - Subtask 135.6: Integrate into `jaro_similarity()` (for suffix removal)
  - Subtask 135.7: Add unit tests for edge cases (all-same, no-common, unicode boundaries)
  - Subtask 135.8: Benchmark on high-similarity strings (>80% similar)
- **Test Strategy:** Verify correctness with edge cases, benchmark against RapidFuzz on similar strings
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 136: mbleven2018 Algorithm for Tiny Edit Distances (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 135 (common affix removal should run first)
- **Description:** Implement the mbleven2018 algorithm for computing Levenshtein distance when max edit distance ≤ 3. This uses a precomputed lookup table of all possible edit sequences and is faster than Myers' bit-parallel for very small distances.
- **Expected Impact:** 2-5x speedup for high-similarity string pairs (the common case in fuzzy matching)
- **Rationale:** In fuzzy matching, most comparisons are between similar strings (threshold > 0.7 means edit distance often ≤ 3). Myers' algorithm has setup overhead (pattern match vector) that dominates for tiny distances.
- **Implementation:**
  ```rust
  /// Precomputed lookup table for mbleven2018 algorithm.
  /// Each u8 encodes an edit sequence: 01=DELETE, 10=INSERT, 11=SUBSTITUTE
  /// Rows are indexed by: (max_dist + max_dist*max_dist) / 2 + len_diff - 1
  const MBLEVEN2018_MATRIX: [[u8; 7]; 9] = [
      // max edit distance 1
      [0x03, 0, 0, 0, 0, 0, 0],         // len_diff 0: substitute
      [0x01, 0, 0, 0, 0, 0, 0],         // len_diff 1: delete
      // max edit distance 2
      [0x0F, 0x09, 0x06, 0, 0, 0, 0],   // len_diff 0
      [0x0D, 0x07, 0, 0, 0, 0, 0],      // len_diff 1
      [0x05, 0, 0, 0, 0, 0, 0],         // len_diff 2
      // max edit distance 3
      [0x3F, 0x27, 0x2D, 0x39, 0x36, 0x1E, 0x1B], // len_diff 0
      [0x3D, 0x37, 0x1F, 0x25, 0x19, 0x16, 0],    // len_diff 1
      [0x35, 0x1D, 0x17, 0, 0, 0, 0],             // len_diff 2
      [0x15, 0, 0, 0, 0, 0, 0],                   // len_diff 3
  ];
  
  /// Compute Levenshtein distance using mbleven2018 for max_dist ≤ 3.
  /// Returns Some(distance) if distance <= max_dist, None otherwise.
  #[inline(always)]
  fn levenshtein_mbleven2018(s1: &[u8], s2: &[u8], max_dist: usize) -> Option<usize> {
      debug_assert!(max_dist <= 3);
      debug_assert!(!s1.is_empty() && !s2.is_empty());
      debug_assert!(s1[0] != s2[0]); // Common prefix already removed
      debug_assert!(s1[s1.len()-1] != s2[s2.len()-1]); // Common suffix already removed
      
      // Ensure s1 is longer
      let (s1, s2) = if s1.len() < s2.len() { (s2, s1) } else { (s1, s2) };
      let len_diff = s1.len() - s2.len();
      
      if len_diff > max_dist {
          return None;
      }
      
      // Special case: max_dist == 1
      if max_dist == 1 {
          return if len_diff == 1 || s1.len() == 1 {
              Some(max_dist + 1)
          } else {
              None
          };
      }
      
      // Look up possible edit sequences
      let ops_index = (max_dist + max_dist * max_dist) / 2 + len_diff - 1;
      let possible_ops = &MBLEVEN2018_MATRIX[ops_index];
      
      let mut best_dist = max_dist + 1;
      
      for &ops in possible_ops.iter() {
          if ops == 0 { break; }
          
          let mut iter_s1 = 0;
          let mut iter_s2 = 0;
          let mut cur_dist = 0;
          let mut ops = ops;
          
          while iter_s1 < s1.len() && iter_s2 < s2.len() {
              if s1[iter_s1] != s2[iter_s2] {
                  cur_dist += 1;
                  if ops == 0 { break; }
                  if ops & 1 != 0 { iter_s1 += 1; } // DELETE
                  if ops & 2 != 0 { iter_s2 += 1; } // INSERT
                  ops >>= 2;
              } else {
                  iter_s1 += 1;
                  iter_s2 += 1;
              }
          }
          
          cur_dist += (s1.len() - iter_s1) + (s2.len() - iter_s2);
          best_dist = best_dist.min(cur_dist);
      }
      
      if best_dist <= max_dist { Some(best_dist) } else { None }
  }
  ```
- **Subtasks:**
  - Subtask 136.1: Implement `MBLEVEN2018_MATRIX` constant lookup table
  - Subtask 136.2: Implement `levenshtein_mbleven2018()` function
  - Subtask 136.3: Integrate into `levenshtein_distance()` dispatcher (call when max_dist ≤ 3 and strings are short)
  - Subtask 136.4: Add comprehensive unit tests including edge cases
  - Subtask 136.5: Benchmark against Myers' bit-parallel for edit distances 1, 2, 3
  - Subtask 136.6: Tune threshold for when to use mbleven vs Myers
- **Test Strategy:** Exhaustive testing for small strings, benchmark against RapidFuzz
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 137: Score Hint Doubling (Iterative Band Widening) (HIGH PRIORITY)**

- **Priority:** High
- **Dependencies:** Task 135
- **Description:** Instead of always using the maximum possible band width, start with a small "score hint" and double it until the actual distance is found. This dramatically reduces work for strings with moderate similarity.
- **Expected Impact:** 2-10x speedup for moderate-distance strings (the algorithm often terminates early)
- **Rationale:** Most fuzzy match operations have a threshold (e.g., > 0.7 similarity). A score_hint of 31 captures most matches, and doubling a few times handles the rest - much cheaper than always computing with max band.
- **Implementation:**
  ```rust
  /// Compute Levenshtein distance with iterative band widening.
  /// Starts with score_hint and doubles until actual distance is found.
  #[inline]
  fn levenshtein_distance_adaptive(s1: &[u8], s2: &[u8], score_cutoff: usize) -> usize {
      // Start with a reasonable initial hint
      let mut score_hint = 31.min(score_cutoff);
      
      loop {
          // Try computing with current hint as max distance
          let result = levenshtein_distance_banded_with_max(s1, s2, score_hint);
          
          // If result is within hint, we found the actual distance
          if result <= score_hint {
              return result;
          }
          
          // If hint >= cutoff, we're done (distance exceeds threshold)
          if score_hint >= score_cutoff {
              return score_cutoff + 1;
          }
          
          // Double the hint and retry
          score_hint = (score_hint * 2).min(score_cutoff);
      }
  }
  
  /// Banded computation that returns early if distance exceeds max.
  #[inline]
  fn levenshtein_distance_banded_with_max(s1: &[u8], s2: &[u8], max: usize) -> usize {
      // Use appropriate algorithm based on string length and band width
      let full_band = s1.len().min(2 * max + 1);
      
      if s1.len() <= 64 {
          // Short strings: use Myers' single-word
          myers_bit_parallel_distance_bounded(s1, s2, max).unwrap_or(max + 1)
      } else if full_band <= 64 {
          // Band fits in single word: use diagonal-shifting
          levenshtein_small_band_diagonal(s1, s2, max)
      } else {
          // Large band: use block-based algorithm
          levenshtein_distance_banded(s1, s2, max)
      }
  }
  ```
- **Subtasks:**
  - Subtask 137.1: Implement `levenshtein_distance_adaptive()` with score hint doubling
  - Subtask 137.2: Implement `levenshtein_distance_banded_with_max()` dispatcher
  - Subtask 137.3: Update all Levenshtein similarity entry points to use adaptive version
  - Subtask 137.4: Implement `damerau_levenshtein_distance_adaptive()` with same pattern
  - Subtask 137.5: Add benchmarks comparing fixed-band vs adaptive approach
  - Subtask 137.6: Tune initial score_hint value (31 vs 63 vs 15)
- **Test Strategy:** Verify correctness, benchmark on various similarity distributions
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 138: Small Band Diagonal Shifting Algorithm (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Task 137
- **Description:** When the band width fits in a single 64-bit word, use a clever diagonal-shifting technique that avoids multi-word BlockPatternMatchVector operations. RapidFuzz's `levenshtein_hyrroe2003_small_band` shifts the band RIGHT instead of left.
- **Expected Impact:** 2-3x speedup for banded computation when band ≤ 64
- **Rationale:** The standard Myers algorithm shifts VP left and tracks the bottom row. The diagonal-shifting variant shifts VP right and tracks the diagonal, which is more efficient for banded computation.
- **Implementation:**
  ```rust
  /// Levenshtein distance using diagonal-shifting for small bands.
  /// Band must fit in 64 bits (i.e., max_distance <= 31 for practical use).
  #[inline(always)]
  fn levenshtein_small_band_diagonal(s1: &[u8], s2: &[u8], max: usize) -> usize {
      debug_assert!(max <= 31); // Band = 2*max+1 must fit in 64 bits
      
      // Initialize VP with high bits set (diagonal starts at top-left)
      let mut vp: u64 = !0u64 << (64 - max - 1);
      let mut vn: u64 = 0;
      
      let mut curr_dist = max;
      let diagonal_mask = 1u64 << 63;
      let mut horizontal_mask = 1u64 << 62;
      
      // Break score: distance can decrease along horizontal but not diagonal
      let break_score = 2 * max + s2.len() - s1.len();
      
      // Build pattern match vector for s1
      let pm = build_pattern_match_vector(s1);
      let mut start_pos: isize = (max + 1) as isize - 64;
      
      // Process each character in s2
      for (j, &ch) in s2.iter().enumerate() {
          // Get PM_j with appropriate shifting based on diagonal position
          let pm_j = if start_pos < 0 {
              pm.get(ch) << (-start_pos) as u32
          } else {
              let word = start_pos as usize / 64;
              let word_pos = start_pos as usize % 64;
              let mut val = pm.get_word(word, ch) >> word_pos;
              if word_pos != 0 && word + 1 < pm.num_words() {
                  val |= pm.get_word(word + 1, ch) << (64 - word_pos);
              }
              val
          };
          
          // Compute D0, HP, HN (same as standard Myers)
          let x = pm_j;
          let d0 = (((x & vp).wrapping_add(vp)) ^ vp) | x | vn;
          let hp = vn | !(d0 | vp);
          let hn = d0 & vp;
          
          // Update distance based on diagonal (not bottom row!)
          if j < s1.len() - max {
              // Phase 1: diagonal stays fixed, band slides
              curr_dist += (!d0 & diagonal_mask != 0) as usize;
          } else {
              // Phase 2: horizontal tracking
              curr_dist += (hp & horizontal_mask != 0) as usize;
              curr_dist -= (hn & horizontal_mask != 0) as usize;
              horizontal_mask >>= 1;
          }
          
          if curr_dist > break_score {
              return max + 1;
          }
          
          // KEY DIFFERENCE: Shift RIGHT instead of left
          vp = hn | !((d0 >> 1) | hp);
          vn = (d0 >> 1) & hp;
          
          start_pos += 1;
      }
      
      if curr_dist <= max { curr_dist } else { max + 1 }
  }
  ```
- **Subtasks:**
  - Subtask 138.1: Implement `levenshtein_small_band_diagonal()` with right-shifting
  - Subtask 138.2: Handle phase transition (diagonal tracking → horizontal tracking)
  - Subtask 138.3: Implement break score early termination
  - Subtask 138.4: Integrate into `levenshtein_distance_banded_with_max()` dispatcher
  - Subtask 138.5: Add unit tests for edge cases (band at boundaries)
  - Subtask 138.6: Benchmark against current banded implementation
- **Test Strategy:** Verify correctness against standard algorithm, benchmark
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 139: Ukkonen Dynamic Band Adjustment (MEDIUM PRIORITY)**

- **Priority:** Medium
- **Dependencies:** Task 138
- **Description:** Implement dynamic adjustment of first_block and last_block during banded Levenshtein computation. This narrows the band when possible and expands it when necessary, following Ukkonen's algorithm.
- **Expected Impact:** 10-30% speedup by avoiding computation in cells that can't affect the result
- **Rationale:** RapidFuzz tracks block scores and adjusts which blocks to process each row. Blocks that can't contribute to a better result are skipped.
- **Implementation:**
  ```rust
  /// Banded Levenshtein with Ukkonen-style dynamic band adjustment.
  fn levenshtein_distance_ukkonen_band(
      s1: &[u8], 
      s2: &[u8], 
      max: usize
  ) -> usize {
      let word_size = 64;
      let words = (s1.len() + word_size - 1) / word_size;
      
      // Track VP, VN, and score for each word
      let mut vecs: Vec<(u64, u64)> = vec![(u64::MAX, 0); words]; // (VP, VN)
      let mut scores: Vec<usize> = (1..=words).map(|i| i.min(words) * word_size).collect();
      scores[words - 1] = s1.len();
      
      let pm = BlockPatternMatchVector::new(s1);
      let last_bit = 1u64 << ((s1.len() - 1) % word_size);
      
      // Initial band bounds
      let mut first_block = 0;
      let mut last_block = ((max + s1.len() - s2.len()) / 2 + 1).min(words) / word_size;
      
      let mut current_max = max;
      
      for (row, &ch) in s2.iter().enumerate() {
          let mut hp_carry = 1u64;
          let mut hn_carry = 0u64;
          
          // Process blocks in current band
          for word in first_block..=last_block {
              let pm_j = pm.get(word, ch);
              let (vp, vn) = vecs[word];
              
              let x = pm_j | hn_carry;
              let d0 = (((x & vp).wrapping_add(vp)) ^ vp) | x | vn;
              let hp = vn | !(d0 | vp);
              let hn = d0 & vp;
              
              let hp_carry_temp = hp_carry;
              let hn_carry_temp = hn_carry;
              
              if word < words - 1 {
                  hp_carry = hp >> 63;
                  hn_carry = hn >> 63;
              } else {
                  hp_carry = (hp & last_bit != 0) as u64;
                  hn_carry = (hn & last_bit != 0) as u64;
              }
              
              let new_vp = (hn << 1 | hn_carry_temp) | !((d0 | (hp << 1 | hp_carry_temp)));
              let new_vn = (hp << 1 | hp_carry_temp) & d0;
              vecs[word] = (new_vp, new_vn);
              
              scores[word] = (scores[word] as isize + hp_carry as isize - hn_carry as isize) as usize;
          }
          
          // Update max based on current scores
          current_max = current_max.min(
              scores[last_block] + (s2.len() - row - 1).max(s1.len().saturating_sub((last_block + 1) * word_size))
          );
          
          // Adjust last_block (expand if needed)
          if last_block + 1 < words {
              let get_row_num = |w| if w + 1 == words { s1.len() - 1 } else { (w + 1) * word_size - 1 };
              let cond = (current_max + 2 * word_size + row + s1.len()) as isize 
                       - (scores[last_block] + 2 + s2.len()) as isize;
              if get_row_num(last_block) < cond as usize {
                  last_block += 1;
                  vecs[last_block] = (u64::MAX, 0);
                  // Process new block...
              }
          }
          
          // Shrink band from end
          while last_block >= first_block {
              let in_band_cond1 = scores[last_block] < current_max + word_size;
              if in_band_cond1 { break; }
              last_block -= 1;
          }
          
          // Shrink band from start
          while first_block <= last_block {
              let in_band_cond1 = scores[first_block] < current_max + word_size;
              if in_band_cond1 { break; }
              first_block += 1;
          }
          
          // Band collapsed - distance exceeds max
          if last_block < first_block {
              return max + 1;
          }
      }
      
      scores[words - 1]
  }
  ```
- **Subtasks:**
  - Subtask 139.1: Implement block-wise score tracking
  - Subtask 139.2: Implement `last_block` expansion logic
  - Subtask 139.3: Implement band shrinking from both ends
  - Subtask 139.4: Handle band collapse (distance exceeds max)
  - Subtask 139.5: Integrate with existing BlockPatternMatchVector
  - Subtask 139.6: Benchmark against static band implementation
- **Test Strategy:** Verify correctness, compare band widths during execution
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

**Task 140: SIMD Batch Processing for Fuzzy Joins (LOWER PRIORITY)**

- **Priority:** Lower (complex, defer if time-constrained)
- **Dependencies:** Tasks 135-139
- **Description:** Implement batch comparison of multiple s1 strings against one s2 using SIMD parallelism. This is the "cdist" pattern from RapidFuzz that enables massive speedup for fuzzy joins.
- **Expected Impact:** 4-8x speedup for fuzzy join operations
- **Rationale:** In fuzzy joins, we compare one string against many candidates. Instead of processing one pair at a time, we can process 4 (AVX2) or 8 (AVX-512) pairs simultaneously.
- **Implementation:**
  ```rust
  /// Batch compute Levenshtein similarity for multiple s1 against single s2.
  /// Uses SIMD to process multiple comparisons in parallel.
  #[cfg(feature = "simd")]
  pub fn levenshtein_similarity_batch_simd(
      s1_strings: &[&str],
      s2: &str,
      scores: &mut [f32],
      score_cutoff: f32,
  ) {
      // Determine optimal vector width based on string lengths
      let max_s1_len = s1_strings.iter().map(|s| s.len()).max().unwrap_or(0);
      
      if max_s1_len <= 8 {
          // Use u8 vectors (32 strings at a time with AVX2)
          levenshtein_batch_simd_u8(s1_strings, s2, scores, score_cutoff);
      } else if max_s1_len <= 16 {
          // Use u16 vectors (16 strings at a time)
          levenshtein_batch_simd_u16(s1_strings, s2, scores, score_cutoff);
      } else if max_s1_len <= 32 {
          // Use u32 vectors (8 strings at a time)
          levenshtein_batch_simd_u32(s1_strings, s2, scores, score_cutoff);
      } else {
          // Use u64 vectors (4 strings at a time)
          levenshtein_batch_simd_u64(s1_strings, s2, scores, score_cutoff);
      }
  }
  
  #[cfg(feature = "simd")]
  fn levenshtein_batch_simd_u64(
      s1_strings: &[&str],
      s2: &str,
      scores: &mut [f32],
      score_cutoff: f32,
  ) {
      const VEC_WIDTH: usize = 4; // AVX2: 4 x u64
      type SimdU64 = Simd<u64, 4>;
      
      let s2_bytes = s2.as_bytes();
      
      // Process strings in batches of VEC_WIDTH
      for (batch_idx, chunk) in s1_strings.chunks(VEC_WIDTH).enumerate() {
          // Prepare batch: build combined pattern match vectors
          let pm = build_batch_pattern_match_vector(chunk);
          
          // Initialize VP, VN, score for each string in batch
          let mut vp = SimdU64::splat(u64::MAX);
          let mut vn = SimdU64::splat(0);
          let mut curr_dist = SimdU64::from_array(
              std::array::from_fn(|i| chunk.get(i).map(|s| s.len() as u64).unwrap_or(0))
          );
          
          // Process s2 character by character
          for &ch in s2_bytes {
              // Get PM for all strings in batch (SIMD gather)
              let pm_j = pm.get_simd(ch);
              
              let x = pm_j;
              let d0 = (((x & vp) + vp) ^ vp) | x | vn;
              let hp = vn | !(d0 | vp);
              let hn = d0 & vp;
              
              // Update scores using SIMD operations
              // (simplified - actual implementation needs proper mask handling)
              let one = SimdU64::splat(1);
              let mask_last_bit = SimdU64::from_array(
                  std::array::from_fn(|i| 1u64 << (chunk.get(i).map(|s| s.len() - 1).unwrap_or(0)))
              );
              
              curr_dist += (hp & mask_last_bit).simd_ne(SimdU64::splat(0)).select(one, SimdU64::splat(0));
              curr_dist -= (hn & mask_last_bit).simd_ne(SimdU64::splat(0)).select(one, SimdU64::splat(0));
              
              vp = (hn << 1) | SimdU64::splat(1);
              vp = hn | !(d0 | vp);
              vn = (hp << 1) & d0;
          }
          
          // Extract results and convert to similarity
          let distances = curr_dist.to_array();
          for (i, &dist) in distances.iter().enumerate() {
              let result_idx = batch_idx * VEC_WIDTH + i;
              if result_idx < scores.len() {
                  let max_len = chunk[i].len().max(s2.len()) as f32;
                  let sim = 1.0 - (dist as f32 / max_len);
                  scores[result_idx] = if sim >= score_cutoff { sim } else { 0.0 };
              }
          }
      }
  }
  ```
- **Subtasks:**
  - Subtask 140.1: Design `BatchPatternMatchVector` that stores PM for multiple strings
  - Subtask 140.2: Implement `levenshtein_batch_simd_u64()` for strings up to 64 chars
  - Subtask 140.3: Implement `levenshtein_batch_simd_u32()` for strings up to 32 chars (8 parallel)
  - Subtask 140.4: Implement `levenshtein_batch_simd_u16()` for strings up to 16 chars (16 parallel)
  - Subtask 140.5: Integrate batch API into `fuzzy_join()` for candidate comparison
  - Subtask 140.6: Add benchmarks comparing batch vs sequential processing
  - Subtask 140.7: Handle length heterogeneity (strings of different lengths in batch)
- **Test Strategy:** Verify correctness against sequential, benchmark on fuzzy join workload
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Phase 17 Success Criteria
- Common affix removal provides 10-30% speedup on high-similarity strings
- mbleven2018 provides 2-3x speedup for edit distances ≤ 3
- Score hint doubling reduces average computation by 50% for moderate similarities
- Small band diagonal shifting matches RapidFuzz performance for band ≤ 64
- **Polars Levenshtein should match or exceed RapidFuzz at all scales**
- **Polars should be within 2x of RapidFuzz for Jaro-Winkler** (remaining gap due to RapidFuzz's deeper SIMD integration)
- All optimizations maintain correctness (all tests pass)

### Phase 17 Dependencies
- Task 135 (Common Affix) is independent - implement first
- Task 136 (mbleven2018) depends on Task 135 (affix should be removed first)
- Task 137 (Score Hint Doubling) depends on Task 135
- Task 138 (Small Band Diagonal) depends on Task 137
- Task 139 (Ukkonen Dynamic Band) depends on Task 138
- Task 140 (SIMD Batch) depends on all above - implement last

### Implementation Order Recommendation
1. **Task 135** - Quick win, easy, high impact
2. **Task 136** - Medium effort, targets the common case
3. **Task 137** - Medium effort, multiplicative benefit
4. **Task 138** - Medium effort, enables efficient small band
5. **Task 139** - Higher effort, incremental improvement
6. **Task 140** - High effort, defer if time-constrained

---

## Phase 18: Jaro-Winkler Performance Optimization

### Phase 18 Objective
Optimize Jaro-Winkler similarity to achieve **parity with or exceed RapidFuzz performance** across all dataset sizes. Current benchmarks show:
- 1K pairs, len=10: **1.33x faster** than RapidFuzz (good)
- 10K pairs, len=20: **0.78x** (22% slower than RapidFuzz)
- 100K pairs, len=30: **0.37x** (2.7x slower than RapidFuzz)

The performance degradation at scale indicates issues with O(n×m) match-finding complexity, cache pressure, and missing preprocessing optimizations that RapidFuzz employs.

### Phase 18 Background

RapidFuzz achieves superior Jaro-Winkler performance through:
1. **Position-based character lookup**: Pre-compute character positions instead of scanning
2. **Vectorized SIMD matching**: Process multiple characters simultaneously
3. **Cache-optimized data structures**: Minimize memory access latency
4. **Early exit optimizations**: Skip computation when result is below threshold

### Task 141: Position-Based Character Matching (Highest Priority)
- **Goal:** Replace O(n×m) scanning with O(1) character position lookup
- **Priority:** HIGH
- **Expected Speedup:** 2-3x for medium strings (20-64 chars)
- **Description:** Instead of scanning through the match window for each character in s1, pre-build a position list mapping each character value (0-255) to its positions in the shorter string. This reduces per-character lookup from O(window_size) to O(avg_char_frequency) - typically O(1-3) for ASCII text.
- **Algorithm:**
  ```rust
  /// RapidFuzz-style position list for O(1) character lookup
  #[inline(always)]
  fn jaro_similarity_position_based(s1: &[u8], s2: &[u8]) -> f32 {
      let len1 = s1.len();
      let len2 = s2.len();
      
      if len1 == 0 && len2 == 0 { return 1.0; }
      if len1 == 0 || len2 == 0 { return 0.0; }
      
      // Always process: iterate over longer string, lookup in shorter
      let (short, long, short_len, long_len) = if len1 <= len2 {
          (s1, s2, len1, len2)
      } else {
          (s2, s1, len2, len1)
      };
      
      // Build character position index for short string
      // char_starts[c] = starting index in positions array for character c
      // char_counts[c] = count of character c in short string
      let mut char_starts: [u8; 256] = [0; 256];
      let mut char_counts: [u8; 256] = [0; 256];
      
      // Count occurrences of each character in short string
      for &c in short {
          char_counts[c as usize] = char_counts[c as usize].saturating_add(1);
      }
      
      // Build cumulative starts (prefix sum)
      let mut total = 0u8;
      for i in 0..256 {
          char_starts[i] = total;
          total = total.saturating_add(char_counts[i]);
      }
      
      // Build position list (positions in short string for each character)
      let mut positions: [u8; 64] = [0; 64]; // Supports strings up to 64 chars
      let mut temp_counts: [u8; 256] = [0; 256];
      for (pos, &c) in short.iter().enumerate() {
          let idx = char_starts[c as usize] + temp_counts[c as usize];
          positions[idx as usize] = pos as u8;
          temp_counts[c as usize] += 1;
      }
      
      let match_distance = (long_len / 2).saturating_sub(1);
      let mut long_matched: u64 = 0;
      let mut short_matched: u64 = 0;
      let mut matches = 0u32;
      
      // For each position in long string, lookup matching positions in short
      for i in 0..long_len {
          let c = long[i];
          let count = char_counts[c as usize];
          if count == 0 { continue; } // Character not in short string
          
          let start_idx = char_starts[c as usize] as usize;
          let end_idx = start_idx + count as usize;
          
          // Search window bounds
          let win_start = i.saturating_sub(match_distance);
          let win_end = (i + match_distance + 1).min(short_len);
          
          // Check each position where this character appears in short
          for idx in start_idx..end_idx {
              let j = positions[idx] as usize;
              if j >= win_start && j < win_end && (short_matched & (1u64 << j)) == 0 {
                  long_matched |= 1u64 << i;
                  short_matched |= 1u64 << j;
                  matches += 1;
                  break;
              }
          }
      }
      
      if matches == 0 { return 0.0; }
      
      // Count transpositions using existing fast implementation
      let transpositions = count_transpositions_fast(long, short, long_matched, short_matched);
      
      let m = matches as f32;
      let t = (transpositions / 2) as f32;
      (m / long_len as f32 + m / short_len as f32 + (m - t) / m) / 3.0
  }
  ```
- **Subtasks:**
  - Subtask 141.1: Implement `CharacterPositionIndex` struct with `build()` and `lookup()` methods
  - Subtask 141.2: Implement `jaro_similarity_position_based()` for strings ≤64 chars
  - Subtask 141.3: Extend to support strings 65-128 chars using u128 bitmasks
  - Subtask 141.4: Add early exit when no characters match in position index
  - Subtask 141.5: Integrate into `jaro_similarity_bytes_optimized()` dispatch
  - Subtask 141.6: Benchmark against current implementation (10K and 100K datasets)
  - Subtask 141.7: Verify correctness with comprehensive test suite
- **Test Strategy:** Compare outputs against current implementation, benchmark at 1K/10K/100K scales
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Task 142: AVX2 SIMD Parallel Match Finding
- **Goal:** Use AVX2 intrinsics to find multiple character matches simultaneously
- **Priority:** HIGH
- **Expected Speedup:** 1.3-1.5x additional on top of Task 141
- **Description:** Process match-finding using AVX2 256-bit vectors to compare 32 characters at once. This accelerates the inner loop of the Jaro algorithm significantly.
- **Algorithm:**
  ```rust
  #[cfg(target_arch = "x86_64")]
  #[target_feature(enable = "avx2")]
  #[inline]
  unsafe fn find_matches_avx2(
      s1: &[u8], 
      s2: &[u8], 
      match_distance: usize
  ) -> (u64, u64, u32) {
      use std::arch::x86_64::*;
      
      let len1 = s1.len();
      let len2 = s2.len();
      
      let mut s1_matches: u64 = 0;
      let mut s2_matches: u64 = 0;
      let mut match_count = 0u32;
      
      // Process each character in s1
      for i in 0..len1 {
          let needle = _mm256_set1_epi8(s1[i] as i8);
          let start = i.saturating_sub(match_distance);
          let end = (i + match_distance + 1).min(len2);
          
          // Use SIMD when search window is large enough
          if end - start >= 32 && start + 32 <= len2 {
              let chunk = _mm256_loadu_si256(s2[start..].as_ptr() as *const __m256i);
              let cmp = _mm256_cmpeq_epi8(needle, chunk);
              let mut mask = _mm256_movemask_epi8(cmp) as u32;
              
              // Clear already-matched positions
              let already_matched = (s2_matches >> start) as u32;
              mask &= !already_matched;
              mask &= (1u32 << (end - start).min(32)) - 1;
              
              if mask != 0 {
                  let j = start + mask.trailing_zeros() as usize;
                  s1_matches |= 1u64 << i;
                  s2_matches |= 1u64 << j;
                  match_count += 1;
                  continue;
              }
          }
          
          // Scalar fallback for small windows
          for j in start..end {
              if (s2_matches & (1u64 << j)) == 0 && s2[j] == s1[i] {
                  s1_matches |= 1u64 << i;
                  s2_matches |= 1u64 << j;
                  match_count += 1;
                  break;
              }
          }
      }
      
      (s1_matches, s2_matches, match_count)
  }
  ```
- **Subtasks:**
  - Subtask 142.1: Implement `find_matches_avx2()` with proper safety guards
  - Subtask 142.2: Add runtime CPU feature detection using `is_x86_feature_detected!`
  - Subtask 142.3: Implement SSE2 fallback for older CPUs
  - Subtask 142.4: Create `jaro_similarity_simd_avx2()` wrapper function
  - Subtask 142.5: Integrate with existing dispatch in `jaro_similarity_bytes_optimized()`
  - Subtask 142.6: Benchmark on AVX2-capable and non-AVX2 systems
- **Test Strategy:** Verify identical results to scalar, benchmark SIMD vs scalar paths
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Task 143: Parallel Batch Processing with Rayon
- **Goal:** Process large datasets (10K+ pairs) in parallel for 1.5-2x speedup
- **Priority:** MEDIUM-HIGH
- **Expected Speedup:** 1.5-2x for 100K+ pairs on multi-core systems
- **Description:** When processing more than a threshold number of string pairs (e.g., 10K), use Rayon to parallelize across multiple CPU cores. Each thread processes a chunk of pairs independently.
- **Algorithm:**
  ```rust
  /// Parallel Jaro-Winkler for large datasets
  pub fn jaro_winkler_similarity_parallel(
      ca: &StringChunked, 
      other: &StringChunked
  ) -> Float32Chunked {
      const PARALLEL_THRESHOLD: usize = 10_000;
      
      let len = ca.len();
      if len < PARALLEL_THRESHOLD {
          return jaro_winkler_similarity(ca, other);
      }
      
      // Collect string references first (avoiding repeated ChunkedArray access)
      let pairs: Vec<_> = ca.iter().zip(other.iter()).collect();
      
      // Process in parallel using Rayon
      use rayon::prelude::*;
      
      let results: Vec<Option<f32>> = pairs
          .par_iter()
          .map(|(a, b)| jaro_winkler_similarity_impl(*a, *b))
          .collect();
      
      Float32Chunked::from_iter(results)
  }
  ```
- **Subtasks:**
  - Subtask 143.1: Add Rayon dependency to polars-ops Cargo.toml (if not present)
  - Subtask 143.2: Implement `jaro_winkler_similarity_parallel()` function
  - Subtask 143.3: Determine optimal `PARALLEL_THRESHOLD` via benchmarking
  - Subtask 143.4: Implement chunk-based parallel processing for better cache locality
  - Subtask 143.5: Update `jaro_winkler_similarity()` to dispatch to parallel version
  - Subtask 143.6: Add benchmarks comparing sequential vs parallel on 10K/100K/1M pairs
  - Subtask 143.7: Ensure thread safety of all shared data structures
- **Test Strategy:** Verify identical results to sequential, benchmark on multi-core systems
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Task 144: Early Exit with Length-Based Upper Bound
- **Goal:** Skip full computation when Jaro similarity will definitely be below threshold
- **Priority:** MEDIUM
- **Expected Speedup:** 10-30% for threshold-based filtering operations
- **Description:** Before computing full Jaro similarity, check if the maximum possible similarity (based on string lengths) can meet the threshold. This optimization is especially valuable for fuzzy matching where most candidates are rejected.
- **Algorithm:**
  ```rust
  /// Check if Jaro similarity can possibly meet threshold based on lengths alone
  #[inline(always)]
  fn jaro_max_possible(len1: usize, len2: usize) -> f32 {
      if len1 == 0 && len2 == 0 { return 1.0; }
      if len1 == 0 || len2 == 0 { return 0.0; }
      
      // Maximum Jaro occurs when:
      // - All characters of shorter string match
      // - No transpositions
      // max_jaro = (shorter/longer + 1 + 1) / 3
      let shorter = len1.min(len2) as f32;
      let longer = len1.max(len2) as f32;
      (shorter / longer + 2.0) / 3.0
  }
  
  /// Jaro-Winkler with early exit based on threshold
  #[inline(always)]
  fn jaro_winkler_with_threshold(s1: &[u8], s2: &[u8], threshold: f32) -> Option<f32> {
      let len1 = s1.len();
      let len2 = s2.len();
      
      // Early exit: check if maximum possible Jaro can meet threshold
      let max_jaro = jaro_max_possible(len1, len2);
      
      // Max Jaro-Winkler = max_jaro + (4 * 0.1 * (1 - max_jaro))
      // = max_jaro + 0.4 - 0.4 * max_jaro = 0.6 * max_jaro + 0.4
      let max_jw = 0.6 * max_jaro + 0.4;
      
      if max_jw < threshold {
          return None; // Will definitely be below threshold
      }
      
      // Compute actual similarity
      let jaro = jaro_similarity_position_based(s1, s2);
      let prefix_len = common_prefix_len(s1, s2, 4);
      let similarity = jaro + (prefix_len as f32 * 0.1 * (1.0 - jaro));
      
      if similarity >= threshold {
          Some(similarity)
      } else {
          None
      }
  }
  ```
- **Subtasks:**
  - Subtask 144.1: Implement `jaro_max_possible()` upper bound function
  - Subtask 144.2: Implement `jaro_winkler_max_possible()` including prefix bonus
  - Subtask 144.3: Add `jaro_winkler_with_threshold()` variant
  - Subtask 144.4: Expose threshold parameter in Python bindings
  - Subtask 144.5: Integrate with fuzzy join operations for candidate filtering
  - Subtask 144.6: Benchmark on typical fuzzy matching workloads (90% rejection rate)
- **Test Strategy:** Verify threshold filtering produces correct results, benchmark rejection scenarios
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Task 145: Cache-Optimized Batch Processing
- **Goal:** Process string pairs in cache-friendly batches to minimize memory latency
- **Priority:** MEDIUM
- **Expected Speedup:** 10-20% for large datasets
- **Description:** Instead of processing pairs one at a time (which causes cache misses when accessing different strings), process in batches that fit in L2 cache. Pre-fetch next batch while processing current.
- **Algorithm:**
  ```rust
  /// Process strings in batches that fit in L2 cache
  const L2_BATCH_SIZE: usize = 256; // Tune based on typical L2 size (256KB-1MB)
  
  fn jaro_winkler_batch_optimized(
      ca: &StringChunked, 
      other: &StringChunked
  ) -> Float32Chunked {
      let len = ca.len();
      let mut results = Vec::with_capacity(len);
      
      // Process in L2-cache-friendly batches
      let pairs: Vec<_> = ca.iter().zip(other.iter()).collect();
      
      for batch in pairs.chunks(L2_BATCH_SIZE) {
          // Prefetch next batch (hint to CPU)
          #[cfg(target_arch = "x86_64")]
          if batch.len() == L2_BATCH_SIZE {
              // prefetch next batch's data
          }
          
          for (a, b) in batch {
              results.push(jaro_winkler_similarity_impl(*a, *b));
          }
      }
      
      Float32Chunked::from_iter(results)
  }
  ```
- **Subtasks:**
  - Subtask 145.1: Profile cache miss patterns in current implementation
  - Subtask 145.2: Implement batch collection from ChunkedArray
  - Subtask 145.3: Determine optimal batch size via benchmarking (L2/L3 trade-off)
  - Subtask 145.4: Add software prefetching hints for next batch
  - Subtask 145.5: Combine with parallel processing (parallel batches)
  - Subtask 145.6: Benchmark cache hit rates before/after optimization
- **Test Strategy:** Use perf counters to measure cache behavior, benchmark throughput
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Task 146: Jaro-Winkler for Long Strings (>64 chars)
- **Goal:** Optimize Jaro-Winkler for strings longer than 64 characters
- **Priority:** LOW-MEDIUM
- **Expected Speedup:** 2-3x for strings 65-256 chars
- **Description:** Current implementation falls back to slower paths for strings >64 chars. Implement a specialized version using u128 bitmasks for 65-128 chars and chunked processing for longer strings.
- **Algorithm:**
  ```rust
  /// Jaro similarity for medium-long strings (65-128 chars)
  fn jaro_similarity_medium_long(s1: &[u8], s2: &[u8]) -> f32 {
      debug_assert!(s1.len() <= 128 && s2.len() <= 128);
      
      // Use u128 bitmasks for match tracking
      let mut s1_matches: u128 = 0;
      let mut s2_matches: u128 = 0;
      
      // Position-based lookup still works, just with larger bitmasks
      // ... (similar to Task 141 but with u128)
  }
  
  /// Jaro similarity for long strings (>128 chars) using block processing
  fn jaro_similarity_long_blocked(s1: &[u8], s2: &[u8]) -> f32 {
      const BLOCK_SIZE: usize = 64;
      
      // Process in blocks, tracking matches across block boundaries
      // Use Vec<u64> for match tracking
      // ... (blocked algorithm)
  }
  ```
- **Subtasks:**
  - Subtask 146.1: Implement `jaro_similarity_u128()` for strings 65-128 chars
  - Subtask 146.2: Implement `jaro_similarity_blocked_v2()` for strings >128 chars
  - Subtask 146.3: Optimize cross-block transposition counting
  - Subtask 146.4: Integrate into dispatch in `jaro_similarity_bytes_optimized()`
  - Subtask 146.5: Benchmark on varying string lengths (50, 100, 200, 500 chars)
- **Test Strategy:** Verify correctness on long strings, benchmark length scaling
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Task 147: Unified Jaro-Winkler Dispatcher Optimization
- **Goal:** Reduce dispatch overhead and ensure optimal path selection
- **Priority:** MEDIUM
- **Expected Speedup:** 5-10% overall
- **Description:** Consolidate and optimize the dispatch logic in `jaro_similarity_bytes_optimized()` to minimize branching overhead and ensure the fastest path is always selected based on string characteristics.
- **Subtasks:**
  - Subtask 147.1: Profile dispatch overhead in current implementation
  - Subtask 147.2: Consolidate redundant dispatch checks
  - Subtask 147.3: Implement length-based jump table for O(1) path selection
  - Subtask 147.4: Remove dead code paths and redundant conditions
  - Subtask 147.5: Add `#[cold]` attributes to unlikely error paths
  - Subtask 147.6: Benchmark dispatch overhead before/after
- **Test Strategy:** Ensure all paths are still reachable and correct, profile overhead
- **Code Location:** `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`

### Phase 18 Success Criteria
- **Task 141 (Position-Based):** 2x+ speedup on 10K pairs, len=20 benchmark
- **Task 142 (AVX2 SIMD):** Additional 1.3x speedup when combined with Task 141
- **Task 143 (Parallel):** Linear scaling on multi-core for 100K+ pairs
- **Task 144 (Early Exit):** 10-30% speedup on threshold-based operations
- **Task 145 (Cache Batch):** Measurable reduction in cache misses
- **Task 146 (Long Strings):** Maintain performance for strings >64 chars
- **Task 147 (Dispatcher):** Reduced dispatch overhead
- **Overall Goal:** Achieve **≥1.0x speedup vs RapidFuzz** on all benchmark sizes (currently 0.37x on 100K pairs)

### Phase 18 Dependencies
- Task 141 (Position-Based) is independent - implement FIRST (highest impact)
- Task 142 (AVX2 SIMD) can be implemented in parallel with Task 141
- Task 143 (Parallel) depends on Tasks 141+142 being optimized first
- Task 144 (Early Exit) is independent - can implement anytime
- Task 145 (Cache Batch) depends on Task 143 for parallel integration
- Task 146 (Long Strings) is independent - implement after core optimizations
- Task 147 (Dispatcher) should be implemented LAST after all paths are finalized

### Implementation Order Recommendation
1. **Task 141** - Highest impact, implement first (expected 2-3x speedup)
2. **Task 142** - High impact SIMD, can parallelize with Task 141
3. **Task 144** - Quick win for threshold operations
4. **Task 143** - Parallel processing for scale
5. **Task 145** - Cache optimization
6. **Task 146** - Long string support
7. **Task 147** - Final cleanup and dispatch optimization
