{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Environment Setup and Polars Codebase Onboarding",
        "description": "Set up the development environment and familiarize with the Polars codebase.",
        "details": "Install necessary tools and dependencies for building Polars. Run existing tests to ensure the environment is correctly set up. Review key modules: polars-core, polars-plan, polars-expr, polars-ops, and py-polars. Study Arrow UTF-8 and list/array layout.",
        "testStrategy": "Verify the setup by running existing Polars tests and ensuring all pass.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Install Development Tools and Dependencies",
            "description": "Set up the necessary development tools and dependencies required for building the Polars codebase.",
            "dependencies": [],
            "details": "Install tools like Rust, Cargo, and any other dependencies specified in the Polars documentation. Ensure that the environment is configured correctly for building the project.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T04:30:12.131Z"
          },
          {
            "id": 2,
            "title": "Run Existing Tests",
            "description": "Execute the existing test suite to verify that the environment is set up correctly.",
            "dependencies": [
              1
            ],
            "details": "Use the command line to run the test suite provided in the Polars codebase. Check for any failing tests and ensure that all dependencies are correctly installed.",
            "status": "done",
            "testStrategy": "Verify that all tests pass without errors.",
            "parentId": "undefined",
            "updatedAt": "2025-12-02T04:35:59.503Z"
          },
          {
            "id": 3,
            "title": "Review Key Polars Modules",
            "description": "Familiarize yourself with the key modules of the Polars codebase: polars-core, polars-plan, polars-expr, polars-ops, and py-polars.",
            "dependencies": [
              1
            ],
            "details": "Read through the documentation and source code of the specified modules to understand their functionality and interdependencies.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T04:37:12.496Z"
          },
          {
            "id": 4,
            "title": "Study Arrow UTF-8 and List/Array Layout",
            "description": "Investigate the Arrow UTF-8 encoding and the layout of lists and arrays used in Polars.",
            "dependencies": [
              3
            ],
            "details": "Review the Arrow documentation and relevant sections in the Polars codebase to understand how data is structured and managed.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T04:37:39.711Z"
          },
          {
            "id": 5,
            "title": "Document Environment Setup Process",
            "description": "Create documentation outlining the steps taken to set up the development environment for future reference.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Compile notes and instructions based on the setup process, including any troubleshooting steps encountered during installation.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T04:38:28.101Z"
          }
        ],
        "updatedAt": "2025-12-02T04:38:28.101Z"
      },
      {
        "id": "2",
        "title": "Implement Hamming Similarity Kernel",
        "description": "Develop the Hamming similarity function for strings of equal length.",
        "details": "Create a new file crates/polars-ops/src/chunked_array/strings/similarity.rs. Implement the function to compare two StringChunked inputs, returning a Float32Chunked. Count differing codepoints, normalize by length, return null if lengths differ, and 1.0 if identical. Handle null bitmaps and multi-chunk columns.",
        "testStrategy": "Create unit tests comparing known string pairs, including edge cases with nulls and differing lengths.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Similarity File",
            "description": "Create a new Rust file for the Hamming similarity function.",
            "dependencies": [],
            "details": "Create the file at crates/polars-ops/src/chunked_array/strings/similarity.rs to house the Hamming similarity function implementation.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:04:40.457Z"
          },
          {
            "id": 2,
            "title": "Implement Hamming Similarity Function",
            "description": "Develop the Hamming similarity function to compare two StringChunked inputs.",
            "dependencies": [
              1
            ],
            "details": "Implement the function that counts differing codepoints between two strings, normalizes by length, and handles nulls and identical strings.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:34.374Z"
          },
          {
            "id": 3,
            "title": "Handle Null Bitmaps",
            "description": "Implement logic to manage null bitmaps in the Hamming similarity function.",
            "dependencies": [
              2
            ],
            "details": "Ensure the function can handle null values in the input StringChunked and return appropriate results without errors.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:34.375Z"
          },
          {
            "id": 4,
            "title": "Create Unit Tests for Hamming Similarity",
            "description": "Develop unit tests for the Hamming similarity function to validate its correctness.",
            "dependencies": [
              2
            ],
            "details": "Write unit tests that cover various cases, including edge cases with nulls, identical strings, and differing lengths.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:34.377Z"
          },
          {
            "id": 5,
            "title": "Validate Hamming Similarity Implementation",
            "description": "Run the unit tests and validate the Hamming similarity function against known values.",
            "dependencies": [
              4
            ],
            "details": "Execute the unit tests created in the previous subtask and ensure all tests pass, confirming the function's correctness.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:34.378Z"
          }
        ],
        "updatedAt": "2025-12-02T05:05:34.378Z"
      },
      {
        "id": "3",
        "title": "Implement Levenshtein Similarity Kernel",
        "description": "Develop the Levenshtein similarity function using the Wagner-Fischer algorithm.",
        "details": "Add a levenshtein_similarity function using a space-optimized Wagner-Fischer algorithm. Normalize the result as 1.0 - (distance / max(len_a, len_b)). Perform codepoint-level operations and validate against RapidFuzz.",
        "testStrategy": "Validate results against RapidFuzz for various string pairs, including edge cases.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Space-Optimized Wagner-Fischer Algorithm",
            "description": "Develop the core function for the Levenshtein similarity using a space-optimized version of the Wagner-Fischer algorithm.",
            "dependencies": [],
            "details": "Create a function that calculates the Levenshtein distance between two strings using a reduced space approach. Ensure that memory usage is minimized while maintaining accuracy.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:35.484Z"
          },
          {
            "id": 2,
            "title": "Normalize Levenshtein Distance",
            "description": "Implement normalization of the Levenshtein distance result to a similarity score between 0 and 1.",
            "dependencies": [
              1
            ],
            "details": "After calculating the distance, normalize the result using the formula: 1.0 - (distance / max(len_a, len_b)). This ensures the output is a similarity score.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:35.486Z"
          },
          {
            "id": 3,
            "title": "Perform Codepoint-Level Operations",
            "description": "Ensure that the Levenshtein function operates at the codepoint level for accurate string comparison.",
            "dependencies": [
              1
            ],
            "details": "Modify the implementation to handle Unicode codepoints correctly, ensuring that multi-byte characters are processed accurately.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:35.487Z"
          },
          {
            "id": 4,
            "title": "Validate Against RapidFuzz",
            "description": "Create a validation suite to compare results of the Levenshtein similarity function against RapidFuzz outputs.",
            "dependencies": [
              2,
              3
            ],
            "details": "Develop unit tests that run various string pairs through both the new function and RapidFuzz, checking for consistency in results, especially with edge cases.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:35.489Z"
          },
          {
            "id": 5,
            "title": "Document the Levenshtein Similarity Function",
            "description": "Write comprehensive documentation for the implemented Levenshtein similarity function, including usage examples.",
            "dependencies": [
              4
            ],
            "details": "Ensure that the documentation covers function parameters, return values, and examples of how to use the function effectively in different scenarios.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:35.490Z"
          }
        ],
        "updatedAt": "2025-12-02T05:05:35.490Z"
      },
      {
        "id": "4",
        "title": "Implement Damerau-Levenshtein Similarity (OSA)",
        "description": "Extend the Levenshtein similarity to support transpositions.",
        "details": "Extend the existing Levenshtein function to implement the OSA variant, allowing for transpositions. Normalize the result and reuse the existing Levenshtein machinery.",
        "testStrategy": "Test against known Damerau-Levenshtein distances and validate with RapidFuzz.",
        "priority": "medium",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend Levenshtein Function",
            "description": "Modify the existing Levenshtein function to support transpositions as per the Damerau-Levenshtein algorithm.",
            "dependencies": [],
            "details": "Implement the logic to handle transpositions in the existing Levenshtein function. This involves checking for adjacent character swaps and adjusting the distance calculation accordingly.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:36.179Z"
          },
          {
            "id": 2,
            "title": "Normalize Damerau-Levenshtein Result",
            "description": "Ensure the result of the Damerau-Levenshtein function is normalized correctly after implementing transpositions.",
            "dependencies": [
              1
            ],
            "details": "After extending the Levenshtein function, normalize the result by dividing the distance by the maximum length of the two input strings. This will ensure the output is between 0 and 1.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:36.181Z"
          },
          {
            "id": 3,
            "title": "Reuse Existing Levenshtein Machinery",
            "description": "Integrate the new Damerau-Levenshtein logic with the existing Levenshtein machinery to maintain code consistency.",
            "dependencies": [
              1
            ],
            "details": "Refactor the code to ensure that the new transposition logic reuses the existing functions and structures from the Levenshtein implementation, minimizing code duplication.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:36.182Z"
          },
          {
            "id": 4,
            "title": "Create Unit Tests for Damerau-Levenshtein",
            "description": "Develop unit tests to validate the functionality of the new Damerau-Levenshtein implementation against known values.",
            "dependencies": [
              2
            ],
            "details": "Write comprehensive unit tests that cover various scenarios, including edge cases for transpositions, to ensure the accuracy of the Damerau-Levenshtein distance calculations.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:36.183Z"
          },
          {
            "id": 5,
            "title": "Validate with RapidFuzz",
            "description": "Test the Damerau-Levenshtein implementation against known results from RapidFuzz to ensure accuracy and performance.",
            "dependencies": [
              4
            ],
            "details": "Use the RapidFuzz library to validate the results of the Damerau-Levenshtein function against its known outputs, ensuring that the implementation is both accurate and efficient.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:36.184Z"
          }
        ],
        "updatedAt": "2025-12-02T05:05:36.184Z"
      },
      {
        "id": "5",
        "title": "Implement Jaro-Winkler Similarity",
        "description": "Develop the Jaro-Winkler similarity function with prefix weighting.",
        "details": "Implement the jaro_winkler_similarity function using the Jaro algorithm with prefix weighting. Use a prefix_weight of 0.1 and prefix_length of 4. Calculate using the formula: jaro + (prefix_len * 0.1 * (1 - jaro)).",
        "testStrategy": "Compare results with known Jaro-Winkler values and validate against RapidFuzz.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Jaro Algorithm",
            "description": "Develop the core Jaro similarity algorithm that calculates the similarity score between two strings based on character matches and transpositions.",
            "dependencies": [],
            "details": "Implement the Jaro algorithm to compute the similarity score between two input strings. This will involve counting matching characters and calculating transpositions. Ensure the function handles edge cases such as empty strings.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:37.168Z"
          },
          {
            "id": 2,
            "title": "Add Prefix Weighting Logic",
            "description": "Integrate prefix weighting into the Jaro similarity calculation to enhance the similarity score for matching prefixes.",
            "dependencies": [
              1
            ],
            "details": "Modify the Jaro similarity function to include prefix weighting. Use a prefix_weight of 0.1 and a prefix_length of 4 to adjust the final similarity score based on the length of the matching prefix.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:37.170Z"
          },
          {
            "id": 3,
            "title": "Calculate Final Jaro-Winkler Score",
            "description": "Combine the Jaro similarity score with the prefix weighting to compute the final Jaro-Winkler similarity score.",
            "dependencies": [
              2
            ],
            "details": "Implement the formula for Jaro-Winkler similarity: jaro + (prefix_len * 0.1 * (1 - jaro)). Ensure that the function returns the final score correctly based on the inputs.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:37.171Z"
          },
          {
            "id": 4,
            "title": "Create Unit Tests for Jaro-Winkler",
            "description": "Develop unit tests to validate the correctness of the Jaro-Winkler similarity function against known values.",
            "dependencies": [
              3
            ],
            "details": "Write a series of unit tests that compare the output of the Jaro-Winkler function with known similarity scores. Include various test cases, such as identical strings, completely different strings, and strings with varying prefixes.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:37.173Z"
          },
          {
            "id": 5,
            "title": "Validate Against RapidFuzz",
            "description": "Ensure the Jaro-Winkler implementation is consistent with results from the RapidFuzz library.",
            "dependencies": [
              4
            ],
            "details": "Run the Jaro-Winkler function against a set of string pairs and compare the results with those produced by the RapidFuzz library. Document any discrepancies and adjust the implementation if necessary.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:05:37.174Z"
          }
        ],
        "updatedAt": "2025-12-02T05:05:37.174Z"
      },
      {
        "id": "6",
        "title": "Implement Cosine Similarity for Vectors",
        "description": "Develop the cosine similarity function for vector data.",
        "details": "Create a new file crates/polars-ops/src/chunked_array/array/similarity.rs. Implement cosine similarity for ArrayChunked or ListChunked inputs. Use the formula: dot(a, b) / (||a|| * ||b||). Handle mismatched lengths, zero-magnitude vectors, and nulls. Ensure numeric stability with epsilon.",
        "testStrategy": "Validate against NumPy for various vector pairs, including edge cases with zero vectors and nulls.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Similarity File",
            "description": "Create a new Rust file for cosine similarity implementation.",
            "dependencies": [],
            "details": "Create the file at crates/polars-ops/src/chunked_array/array/similarity.rs to house the cosine similarity function.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:10:13.922Z"
          },
          {
            "id": 2,
            "title": "Implement Cosine Similarity Function",
            "description": "Develop the cosine similarity function using the specified formula.",
            "dependencies": [
              1
            ],
            "details": "Implement the function that calculates cosine similarity using the formula: dot(a, b) / (||a|| * ||b||).",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:10:13.946Z"
          },
          {
            "id": 3,
            "title": "Handle Edge Cases",
            "description": "Implement checks for edge cases in cosine similarity calculations.",
            "dependencies": [
              2
            ],
            "details": "Ensure the function handles mismatched lengths, zero-magnitude vectors, and null values appropriately.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:10:13.954Z"
          },
          {
            "id": 4,
            "title": "Ensure Numeric Stability",
            "description": "Add mechanisms to ensure numeric stability in calculations.",
            "dependencies": [
              3
            ],
            "details": "Implement an epsilon value to avoid division by zero and ensure stable calculations for cosine similarity.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:10:13.974Z"
          },
          {
            "id": 5,
            "title": "Create Unit Tests for Cosine Similarity",
            "description": "Develop unit tests to validate the cosine similarity function against known values.",
            "dependencies": [
              4
            ],
            "details": "Write unit tests that compare the cosine similarity results with expected outcomes, including edge cases such as zero vectors and nulls.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:10:13.980Z"
          }
        ],
        "updatedAt": "2025-12-02T05:10:13.980Z"
      },
      {
        "id": "7",
        "title": "Add FunctionExpr Variants",
        "description": "Extend the Polars DSL with new function expressions for string and cosine similarity.",
        "details": "In polars-plan/src/dsl/functions.rs, add a StringSimilarity(StringSimilarityType) variant and a CosineSimilarity variant. Create a StringSimilarityType enum with Levenshtein, DamerauLevenshtein, JaroWinkler, and Hamming. Ensure serialization works correctly.",
        "testStrategy": "Test serialization and deserialization of the new function expressions.",
        "priority": "medium",
        "dependencies": [
          "3",
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create StringSimilarityType Enum",
            "description": "Define a new enum called StringSimilarityType with variants for Levenshtein, DamerauLevenshtein, JaroWinkler, and Hamming.",
            "dependencies": [],
            "details": "The enum should be defined in the appropriate module within polars-plan/src/dsl/functions.rs, ensuring it is easily extensible in the future.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:20:12.059Z"
          },
          {
            "id": 2,
            "title": "Implement StringSimilarity Variant",
            "description": "Add a new variant StringSimilarity(StringSimilarityType) to the FunctionExpr in the Polars DSL.",
            "dependencies": [
              1
            ],
            "details": "Modify the FunctionExpr enum in polars-plan/src/dsl/functions.rs to include the new StringSimilarity variant, ensuring it can accept the StringSimilarityType enum as a parameter.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:20:12.063Z"
          },
          {
            "id": 3,
            "title": "Implement CosineSimilarity Variant",
            "description": "Add a new variant CosineSimilarity to the FunctionExpr in the Polars DSL.",
            "dependencies": [],
            "details": "Modify the FunctionExpr enum in polars-plan/src/dsl/functions.rs to include the CosineSimilarity variant, ensuring it is properly structured for future use.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:20:12.069Z"
          },
          {
            "id": 4,
            "title": "Ensure Serialization for New Variants",
            "description": "Implement serialization and deserialization for the new StringSimilarity and CosineSimilarity variants.",
            "dependencies": [
              2,
              3
            ],
            "details": "Utilize existing serialization methods in the Polars codebase to ensure that the new variants can be serialized and deserialized correctly, maintaining compatibility with the existing system.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:20:12.072Z"
          },
          {
            "id": 5,
            "title": "Write Unit Tests for New Variants",
            "description": "Create unit tests to validate the functionality of the new StringSimilarity and CosineSimilarity variants.",
            "dependencies": [
              4
            ],
            "details": "Develop comprehensive unit tests that cover various scenarios for both StringSimilarity and CosineSimilarity, ensuring that they behave as expected and handle edge cases appropriately.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:20:22.870Z"
          }
        ],
        "updatedAt": "2025-12-02T05:20:22.870Z"
      },
      {
        "id": "8",
        "title": "DSL Methods in String Namespace",
        "description": "Add DSL methods for string similarity functions in the Utf8NameSpace.",
        "details": "In polars-plan/src/dsl/strings.rs, implement methods: levenshtein_sim, damerau_levenshtein_sim, jaro_winkler_sim, and hamming_sim. Each method should take another Expr as input and return an Expr.",
        "testStrategy": "Create unit tests for each method, ensuring they produce the correct expressions.",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Levenshtein Similarity Method",
            "description": "Develop the levenshtein_sim method in the Utf8NameSpace to calculate the Levenshtein distance between two strings.",
            "dependencies": [],
            "details": "In polars-plan/src/dsl/strings.rs, implement the levenshtein_sim method. It should take another Expr as input and return an Expr representing the Levenshtein similarity score.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:22:06.904Z"
          },
          {
            "id": 2,
            "title": "Implement Damerau-Levenshtein Similarity Method",
            "description": "Create the damerau_levenshtein_sim method to compute the Damerau-Levenshtein distance for string similarity.",
            "dependencies": [],
            "details": "Add the damerau_levenshtein_sim method in polars-plan/src/dsl/strings.rs. This method will take an Expr as input and return an Expr for the Damerau-Levenshtein similarity score.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:22:06.914Z"
          },
          {
            "id": 3,
            "title": "Implement Jaro-Winkler Similarity Method",
            "description": "Develop the jaro_winkler_sim method to calculate the Jaro-Winkler similarity between two strings.",
            "dependencies": [],
            "details": "In polars-plan/src/dsl/strings.rs, implement the jaro_winkler_sim method. It should accept another Expr and return an Expr that represents the Jaro-Winkler similarity score.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:22:06.941Z"
          },
          {
            "id": 4,
            "title": "Implement Hamming Similarity Method",
            "description": "Create the hamming_sim method to compute the Hamming similarity for strings of equal length.",
            "dependencies": [],
            "details": "Add the hamming_sim method in polars-plan/src/dsl/strings.rs. This method will take an Expr as input and return an Expr for the Hamming similarity score, ensuring both strings are of equal length.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:22:07.015Z"
          },
          {
            "id": 5,
            "title": "Create Unit Tests for String Similarity Methods",
            "description": "Develop unit tests for all string similarity methods implemented in the Utf8NameSpace.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Write comprehensive unit tests for levenshtein_sim, damerau_levenshtein_sim, jaro_winkler_sim, and hamming_sim methods to ensure they produce correct expressions and handle edge cases.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:22:07.972Z"
          }
        ],
        "updatedAt": "2025-12-02T05:22:07.972Z"
      },
      {
        "id": "9",
        "title": "DSL Methods in Array Namespace",
        "description": "Add a DSL method for cosine similarity in the ArrayNameSpace.",
        "details": "In polars-plan/src/dsl/arrays.rs, implement the cosine_similarity method. The method should take another Expr as input and return an Expr.",
        "testStrategy": "Test the method to ensure it produces the correct expression for cosine similarity.",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Cosine Similarity Method Signature",
            "description": "Create the method signature for cosine_similarity in arrays.rs that takes another Expr as input and returns an Expr.",
            "dependencies": [],
            "details": "The method should be defined in polars-plan/src/dsl/arrays.rs with appropriate input and output types. Ensure to include necessary imports for Expr.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:23:02.017Z"
          },
          {
            "id": 2,
            "title": "Implement Cosine Similarity Logic",
            "description": "Develop the core logic for calculating cosine similarity within the cosine_similarity method.",
            "dependencies": [
              1
            ],
            "details": "Utilize the formula: dot(a, b) / (||a|| * ||b||) to compute cosine similarity. Handle edge cases such as zero vectors and null values.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:23:02.043Z"
          },
          {
            "id": 3,
            "title": "Write Unit Tests for Cosine Similarity",
            "description": "Create unit tests to validate the functionality of the cosine_similarity method.",
            "dependencies": [
              2
            ],
            "details": "Develop a suite of unit tests that cover various scenarios, including normal cases, edge cases with zero vectors, and null inputs. Use a testing framework compatible with the Polars codebase.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:23:02.070Z"
          },
          {
            "id": 4,
            "title": "Document Cosine Similarity Method",
            "description": "Add documentation for the cosine_similarity method, including usage examples and parameter descriptions.",
            "dependencies": [
              2
            ],
            "details": "Ensure that the method is well-documented with clear explanations of its parameters, return values, and examples of how to use it in practice.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:23:02.205Z"
          },
          {
            "id": 5,
            "title": "Integrate Cosine Similarity into DSL",
            "description": "Ensure the cosine_similarity method is properly integrated into the DSL for use in expressions.",
            "dependencies": [
              3,
              4
            ],
            "details": "Update the necessary files to expose the cosine_similarity method in the DSL, ensuring it can be called as part of expression evaluations.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:23:02.268Z"
          }
        ],
        "updatedAt": "2025-12-02T05:23:02.268Z"
      },
      {
        "id": "10",
        "title": "Wire Up Physical Expression Builder",
        "description": "Integrate the new similarity functions into the physical expression builder.",
        "details": "In polars-lazy/src/physical_plan/expression.rs, add match arms for StringSimilarity and CosineSimilarity. Handle both column-to-column and column-to-literal cases. Ensure integration tests cover expression execution.",
        "testStrategy": "Run integration tests to verify that expressions are correctly executed in both eager and lazy contexts.",
        "priority": "medium",
        "dependencies": [
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Match Arms for StringSimilarity",
            "description": "Implement match arms for the StringSimilarity function in the expression builder.",
            "dependencies": [],
            "details": "In the file polars-lazy/src/physical_plan/expression.rs, add the necessary match arms to handle StringSimilarity cases for both column-to-column and column-to-literal comparisons.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:31:12.806Z"
          },
          {
            "id": 2,
            "title": "Add Match Arms for CosineSimilarity",
            "description": "Implement match arms for the CosineSimilarity function in the expression builder.",
            "dependencies": [
              1
            ],
            "details": "Extend the match arms in polars-lazy/src/physical_plan/expression.rs to include handling for CosineSimilarity, ensuring both column-to-column and column-to-literal scenarios are covered.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:31:12.811Z"
          },
          {
            "id": 3,
            "title": "Implement Column-to-Column Handling",
            "description": "Ensure that the physical expression builder correctly handles column-to-column similarity cases.",
            "dependencies": [
              1,
              2
            ],
            "details": "Modify the implementation to handle cases where both inputs are columns, ensuring that the similarity functions can operate on two columns of data.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:31:12.824Z"
          },
          {
            "id": 4,
            "title": "Implement Column-to-Literal Handling",
            "description": "Ensure that the physical expression builder correctly handles column-to-literal similarity cases.",
            "dependencies": [
              1,
              2
            ],
            "details": "Add logic to handle cases where one input is a column and the other is a literal value, ensuring correct execution of the similarity functions.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:31:12.830Z"
          },
          {
            "id": 5,
            "title": "Create Integration Tests for Similarity Functions",
            "description": "Develop integration tests to validate the execution of the new similarity functions in the expression builder.",
            "dependencies": [
              3,
              4
            ],
            "details": "Write integration tests that cover various scenarios for both StringSimilarity and CosineSimilarity, ensuring they are executed correctly in both eager and lazy contexts.",
            "status": "done",
            "testStrategy": "Run integration tests to verify that expressions are correctly executed.",
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:31:12.855Z"
          }
        ],
        "updatedAt": "2025-12-02T05:31:12.855Z"
      },
      {
        "id": "11",
        "title": "Python Bindings for String Similarity",
        "description": "Expose string similarity functions to Python via the .str namespace.",
        "details": "In py-polars/src/polars/, add methods for levenshtein_sim, damerau_levenshtein_sim, jaro_winkler_sim, and hamming_sim in the .str namespace. Include type hints and docstrings.",
        "testStrategy": "Create Python unit tests to ensure the bindings work as expected and validate against RapidFuzz.",
        "priority": "medium",
        "dependencies": [
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Levenshtein Similarity Function",
            "description": "Create the Levenshtein similarity function in the .str namespace of py-polars.",
            "dependencies": [],
            "details": "Add a method for levenshtein_sim in py-polars/src/polars/. Ensure it takes two strings as input and returns a float representing the similarity score. Include type hints and a detailed docstring.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:44:10.156Z"
          },
          {
            "id": 2,
            "title": "Implement Damerau-Levenshtein Similarity Function",
            "description": "Create the Damerau-Levenshtein similarity function in the .str namespace of py-polars.",
            "dependencies": [],
            "details": "Add a method for damerau_levenshtein_sim in py-polars/src/polars/. Ensure it takes two strings as input and returns a float representing the similarity score. Include type hints and a detailed docstring.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:44:10.162Z"
          },
          {
            "id": 3,
            "title": "Implement Jaro-Winkler Similarity Function",
            "description": "Create the Jaro-Winkler similarity function in the .str namespace of py-polars.",
            "dependencies": [],
            "details": "Add a method for jaro_winkler_sim in py-polars/src/polars/. Ensure it takes two strings as input and returns a float representing the similarity score. Include type hints and a detailed docstring.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:44:10.168Z"
          },
          {
            "id": 4,
            "title": "Implement Hamming Similarity Function",
            "description": "Create the Hamming similarity function in the .str namespace of py-polars.",
            "dependencies": [],
            "details": "Add a method for hamming_sim in py-polars/src/polars/. Ensure it takes two strings of equal length as input and returns a float representing the similarity score. Include type hints and a detailed docstring.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:44:10.183Z"
          },
          {
            "id": 5,
            "title": "Create Unit Tests for String Similarity Functions",
            "description": "Develop unit tests for the string similarity functions implemented in the .str namespace.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Write unit tests to validate the functionality of levenshtein_sim, damerau_levenshtein_sim, jaro_winkler_sim, and hamming_sim. Ensure tests cover various scenarios, including edge cases and expected outputs.",
            "status": "done",
            "testStrategy": "Use RapidFuzz for validation against known similarity scores.",
            "parentId": "undefined",
            "updatedAt": "2025-12-02T05:44:10.193Z"
          }
        ],
        "updatedAt": "2025-12-02T05:44:10.193Z"
      },
      {
        "id": "12",
        "title": "Python Bindings for Cosine Similarity",
        "description": "Expose the cosine similarity function to Python via the .arr namespace.",
        "details": "In py-polars/src/polars/, add a cosine_similarity method in the .arr namespace. Handle list/array literal conversion and include type hints and docstrings.",
        "testStrategy": "Test the Python binding with NumPy validation for various vector inputs.",
        "priority": "medium",
        "dependencies": [
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Cosine Similarity Function",
            "description": "Develop the core cosine similarity function for vector data, ensuring it handles various input types correctly.",
            "dependencies": [],
            "details": "Create a new function in crates/polars-ops/src/chunked_array/array/similarity.rs that calculates cosine similarity using the formula: dot(a, b) / (||a|| * ||b||).",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:20:28.518Z"
          },
          {
            "id": 2,
            "title": "Handle Input Conversion",
            "description": "Implement logic to convert list/array literals into the appropriate format for the cosine similarity function.",
            "dependencies": [
              1
            ],
            "details": "Ensure that the function can accept both list and array inputs, converting them to the required format for processing within the cosine similarity function.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:20:28.521Z"
          },
          {
            "id": 3,
            "title": "Add Type Hints and Docstrings",
            "description": "Include type hints and comprehensive docstrings for the cosine similarity function to improve code readability and usability.",
            "dependencies": [
              1
            ],
            "details": "Add type hints for function parameters and return types, and write detailed docstrings explaining the function's purpose, parameters, and return values.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:20:28.523Z"
          },
          {
            "id": 4,
            "title": "Implement Edge Case Handling",
            "description": "Ensure the cosine similarity function handles edge cases such as zero-magnitude vectors and null inputs gracefully.",
            "dependencies": [
              1
            ],
            "details": "Implement checks for zero-length vectors and null inputs, returning appropriate values or errors to maintain numeric stability and prevent crashes.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:20:28.524Z"
          },
          {
            "id": 5,
            "title": "Create Unit Tests for Cosine Similarity",
            "description": "Develop unit tests to validate the cosine similarity function against known outputs, including edge cases.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Write unit tests using a testing framework to validate the cosine similarity function, ensuring it works correctly with various vector inputs and edge cases.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:20:28.525Z"
          }
        ],
        "updatedAt": "2025-12-02T06:20:28.525Z"
      },
      {
        "id": "13",
        "title": "Comprehensive Testing Suite",
        "description": "Develop a comprehensive suite of tests for all similarity functions.",
        "details": "Create test files in crates/polars-ops/tests/ and py-polars/tests/unit/expressions/. Include unit tests, edge cases, integration tests, fuzz tests, and performance benchmarks. Validate against RapidFuzz and NumPy.",
        "testStrategy": "Ensure all tests pass, covering all edge cases and performance benchmarks.",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6",
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Unit Tests for Cosine Similarity",
            "description": "Develop unit tests specifically for the cosine similarity function to ensure it behaves as expected with various input vectors.",
            "dependencies": [],
            "details": "Write unit tests in crates/polars-ops/tests/ that cover typical cases, edge cases, and invalid inputs for the cosine similarity function. Ensure tests validate against known outputs from NumPy.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:24:40.385Z"
          },
          {
            "id": 2,
            "title": "Implement Edge Case Tests for Similarity Functions",
            "description": "Identify and implement edge case tests for all similarity functions to ensure robustness.",
            "dependencies": [
              1
            ],
            "details": "Create tests that handle edge cases such as empty vectors, null values, and extreme values in crates/polars-ops/tests/ and py-polars/tests/unit/expressions/.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:24:40.387Z"
          },
          {
            "id": 3,
            "title": "Develop Integration Tests for Similarity Functions",
            "description": "Create integration tests that validate the interaction between different similarity functions and their expected outputs.",
            "dependencies": [
              1,
              2
            ],
            "details": "Write integration tests in py-polars/tests/unit/expressions/ that check the combined functionality of similarity functions, ensuring they work together correctly.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:24:40.388Z"
          },
          {
            "id": 4,
            "title": "Create Fuzz Tests for Similarity Functions",
            "description": "Implement fuzz tests to identify potential vulnerabilities or unexpected behaviors in similarity functions.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Utilize a fuzz testing framework to generate random inputs for the similarity functions in crates/polars-ops/tests/ and validate their outputs.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:24:40.389Z"
          },
          {
            "id": 5,
            "title": "Benchmark Performance of Similarity Functions",
            "description": "Conduct performance benchmarks for all similarity functions to ensure they meet performance criteria.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Set up performance benchmarks in crates/polars-ops/tests/ that measure execution time and memory usage for various input sizes and types, comparing results against RapidFuzz and NumPy.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:24:40.390Z"
          }
        ],
        "updatedAt": "2025-12-02T06:24:40.390Z"
      },
      {
        "id": "14",
        "title": "Documentation and Examples",
        "description": "Document the new similarity functions and provide usage examples.",
        "details": "Add Rust docstrings with algorithm descriptions and Python docstrings with examples. Provide usage examples for fuzzy filtering, deduplication, and ML features. Include performance notes and limitations.",
        "testStrategy": "Review documentation for completeness and clarity. Ensure examples are correct and demonstrate key use cases.",
        "priority": "medium",
        "dependencies": [
          "11",
          "12",
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Rust Docstrings for Similarity Functions",
            "description": "Create detailed Rust docstrings for each of the new similarity functions, explaining the algorithms and their usage.",
            "dependencies": [],
            "details": "Focus on providing clear explanations of the algorithms implemented, including any parameters and return values. Ensure that the documentation adheres to Rust's documentation standards.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:26:26.441Z"
          },
          {
            "id": 2,
            "title": "Add Python Docstrings for Similarity Functions",
            "description": "Implement Python docstrings for the new similarity functions, including usage examples and descriptions.",
            "dependencies": [],
            "details": "Each docstring should include a brief description of the function, its parameters, return values, and at least one usage example. Follow Python's documentation conventions.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:26:26.443Z"
          },
          {
            "id": 3,
            "title": "Create Usage Examples for Fuzzy Filtering",
            "description": "Develop comprehensive usage examples for the fuzzy filtering functionality using the new similarity functions.",
            "dependencies": [],
            "details": "Include various scenarios showcasing how to use the fuzzy filtering feature effectively. Ensure examples are clear and cover edge cases.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:26:26.444Z"
          },
          {
            "id": 4,
            "title": "Document Deduplication Examples",
            "description": "Provide detailed examples demonstrating the deduplication process using the new similarity functions.",
            "dependencies": [],
            "details": "Create examples that illustrate how to apply the similarity functions for deduplication tasks, including sample data and expected outcomes.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:26:26.445Z"
          },
          {
            "id": 5,
            "title": "Include Performance Notes and Limitations",
            "description": "Write performance notes and limitations for each similarity function to inform users about potential constraints.",
            "dependencies": [],
            "details": "Discuss performance metrics, expected behavior under various conditions, and any known limitations of the algorithms. This will help users make informed decisions.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T06:26:26.446Z"
          }
        ],
        "updatedAt": "2025-12-02T06:26:26.446Z"
      },
      {
        "id": "15",
        "title": "ASCII Fast Path Optimization",
        "description": "Detect ASCII-only strings and use byte-level operations instead of Unicode codepoint iteration for all string similarity functions",
        "details": "Implement ASCII detection and byte-level comparison functions. Create `levenshtein_distance_bytes()` for ASCII strings. Apply to all string similarity functions (Levenshtein, Damerau-Levenshtein, Jaro-Winkler, Hamming). Expected impact: 2-5x speedup for common ASCII text.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "14"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement ASCII Detection Function",
            "description": "Create a function to detect if a string is ASCII-only by checking each character's byte value.",
            "dependencies": [],
            "details": "The function will iterate through the string and verify that each character falls within the ASCII range (0-127). If any character exceeds this range, the function will return false; otherwise, it will return true.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:25:34.735Z"
          },
          {
            "id": 2,
            "title": "Develop Byte-Level Comparison Functions",
            "description": "Implement byte-level comparison functions for string similarity operations specifically for ASCII strings.",
            "dependencies": [
              1
            ],
            "details": "These functions will utilize direct byte comparisons instead of Unicode codepoint iterations to enhance performance for ASCII strings. The functions will include optimized versions of Levenshtein, Damerau-Levenshtein, Jaro-Winkler, and Hamming.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:25:34.736Z"
          },
          {
            "id": 3,
            "title": "Create Levenshtein Distance for Bytes",
            "description": "Develop the `levenshtein_distance_bytes()` function to calculate the Levenshtein distance for ASCII strings using byte-level operations.",
            "dependencies": [
              2
            ],
            "details": "This function will leverage the byte-level comparison functions to efficiently compute the Levenshtein distance, focusing on ASCII strings to achieve significant performance improvements.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:25:34.737Z"
          },
          {
            "id": 4,
            "title": "Integrate Byte-Level Functions into Similarity Algorithms",
            "description": "Apply the newly created byte-level functions to all relevant string similarity algorithms.",
            "dependencies": [
              3
            ],
            "details": "Modify the existing string similarity functions to utilize the new byte-level operations for ASCII strings, ensuring that the algorithms fall back to Unicode operations for non-ASCII strings.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:25:34.737Z"
          },
          {
            "id": 5,
            "title": "Benchmark Performance Improvements",
            "description": "Conduct performance benchmarks to measure the speedup achieved by the ASCII fast path optimizations.",
            "dependencies": [
              4
            ],
            "details": "Set up tests to compare the execution time of the original string similarity functions against the optimized versions for ASCII strings. Analyze the results to quantify the expected 2-5x speedup.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:25:34.738Z"
          }
        ],
        "updatedAt": "2025-12-02T20:25:34.738Z"
      },
      {
        "id": "16",
        "title": "Early Exit Optimizations",
        "description": "Add length difference checks, identical string checks, and early termination with threshold for bounded similarity queries",
        "details": "Add length difference check: if max_len - min_len > max_len / 2, return early with similarity 0.0. Add identical string check before running full algorithm. Add early termination with threshold for bounded similarity queries. Expected impact: 1.5-3x speedup for mismatched strings.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "14"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Length Difference Check",
            "description": "Add a function to check the length difference between two strings. If the difference exceeds half of the maximum length, return early with a similarity score of 0.0.",
            "dependencies": [],
            "details": "Create a function that calculates the maximum and minimum lengths of the two strings and checks if the difference exceeds max_len / 2. If true, return 0.0 immediately to optimize performance.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:27:36.645Z"
          },
          {
            "id": 2,
            "title": "Add Identical String Check",
            "description": "Implement a check to determine if two strings are identical before executing the full similarity algorithm. This should be the first step in the similarity calculation process.",
            "dependencies": [
              1
            ],
            "details": "Create a simple equality check that compares the two strings. If they are identical, return a similarity score of 1.0, bypassing further calculations.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:27:36.647Z"
          },
          {
            "id": 3,
            "title": "Implement Early Termination Logic",
            "description": "Add logic for early termination of the similarity calculation based on a predefined threshold for bounded similarity queries.",
            "dependencies": [
              1,
              2
            ],
            "details": "Define a threshold value for similarity scores. If the calculated similarity falls below this threshold during the computation, terminate the process early and return the current score.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:27:36.648Z"
          },
          {
            "id": 4,
            "title": "Unit Tests for Length Check",
            "description": "Create unit tests to validate the functionality of the length difference check implemented in subtask 1.",
            "dependencies": [
              1
            ],
            "details": "Write a series of unit tests that cover various scenarios for the length difference check, ensuring it returns the correct early exit scores for different string lengths.",
            "status": "done",
            "testStrategy": "Create unit tests using a testing framework to validate the length difference logic.",
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:27:36.649Z"
          },
          {
            "id": 5,
            "title": "Unit Tests for Identical String Check",
            "description": "Develop unit tests to ensure the identical string check works correctly as implemented in subtask 2.",
            "dependencies": [
              2
            ],
            "details": "Write unit tests that check various pairs of strings, confirming that identical strings return a similarity score of 1.0 and non-identical strings do not.",
            "status": "done",
            "testStrategy": "Create unit tests using a testing framework to validate the identical string check.",
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:27:36.650Z"
          }
        ],
        "updatedAt": "2025-12-02T20:27:36.650Z"
      },
      {
        "id": "17",
        "title": "Parallel Chunk Processing",
        "description": "Use Rayon to process chunks in parallel for ChunkedArray operations",
        "details": "Implement parallel iterators using Rayon for ChunkedArray operations. Ensure thread safety and proper null handling. Process chunks in parallel across multiple CPU cores. Expected impact: 2-4x speedup on multi-core systems.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "14"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Set Up Rayon for Parallel Processing",
            "description": "Integrate the Rayon library into the project to enable parallel processing capabilities for ChunkedArray operations.",
            "dependencies": [],
            "details": "Add Rayon as a dependency in the Cargo.toml file and ensure it is properly configured for use in the project. Verify that the library compiles without errors.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:30:40.619Z"
          },
          {
            "id": 2,
            "title": "Implement Parallel Iterators for ChunkedArray",
            "description": "Create parallel iterators using Rayon for processing ChunkedArray operations.",
            "dependencies": [
              1
            ],
            "details": "Develop iterators that can process chunks of data in parallel. Ensure that each iterator can handle chunk sizes appropriately and utilize multiple CPU cores effectively.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:30:40.621Z"
          },
          {
            "id": 3,
            "title": "Ensure Thread Safety in Parallel Operations",
            "description": "Implement mechanisms to ensure thread safety during parallel processing of ChunkedArray operations.",
            "dependencies": [
              2
            ],
            "details": "Use synchronization primitives provided by Rust, such as Mutex or RwLock, to protect shared data during parallel processing. Validate that no data races occur during execution.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:30:40.623Z"
          },
          {
            "id": 4,
            "title": "Implement Null Handling in Parallel Processing",
            "description": "Add proper handling for null values during the parallel processing of ChunkedArray operations.",
            "dependencies": [
              3
            ],
            "details": "Ensure that the parallel iterators can gracefully handle null values without causing panics or incorrect results. Implement tests to verify null handling.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:30:40.625Z"
          },
          {
            "id": 5,
            "title": "Benchmark Performance Improvements",
            "description": "Conduct benchmarks to measure the performance improvements of parallel processing on ChunkedArray operations.",
            "dependencies": [
              4
            ],
            "details": "Set up benchmarking tests to compare the performance of the new parallel processing implementation against the existing sequential processing. Aim for a 2-4x speedup on multi-core systems.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:30:40.626Z"
          }
        ],
        "updatedAt": "2025-12-02T20:30:40.626Z"
      },
      {
        "id": "18",
        "title": "Memory Pool and Buffer Reuse",
        "description": "Implement thread-local buffer pool for DP matrix rows and reuse Vec allocations across function calls",
        "details": "Implement thread-local buffer pool for dynamic programming matrix rows. Reuse Vec allocations across function calls to reduce allocation overhead in hot loops. Expected impact: 10-20% speedup, reduced memory pressure.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "14"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-02T20:32:39.540Z"
      },
      {
        "id": "19",
        "title": "Myers' Bit-Parallel Algorithm",
        "description": "Implement Myers' bit-parallel algorithm for small strings (< 64 chars) as an alternative to Wagner-Fischer",
        "details": "Implement Myers' bit-parallel algorithm for small strings (< 64 chars). Use for bounded distance calculations. Fall back to Wagner-Fischer for larger strings. Expected impact: 2-3x speedup for short strings.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "15",
          "16"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-02T21:25:46.732Z"
      },
      {
        "id": "20",
        "title": "Early Termination with Threshold",
        "description": "Add optional threshold parameter to similarity functions for early exit when minimum distance exceeds threshold",
        "details": "Add optional threshold parameter to similarity functions. Exit early if minimum distance exceeds threshold. Useful for filtering scenarios where only high-similarity matches are needed. Expected impact: 1.5-2x speedup for threshold-based queries.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "16"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-02T21:25:46.734Z"
      },
      {
        "id": "21",
        "title": "Branch Prediction Optimization",
        "description": "Add likely/unlikely attributes and optimize inner loop branches for better CPU branch prediction",
        "details": "Add #[likely]/#[unlikely] attributes for common code paths. Optimize inner loop branches (character equality checks). Use #[inline(always)] for hot functions. Expected impact: 5-15% speedup.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "15",
          "16"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-02T21:25:46.737Z"
      },
      {
        "id": "22",
        "title": "SIMD Character Comparison",
        "description": "Use SIMD instructions to compare multiple characters at once using std::simd or portable_simd",
        "details": "Use SIMD instructions to compare multiple characters at once. Implement using std::simd or portable_simd feature. Vectorize character equality checks in inner loops. Expected impact: 2-4x speedup for character comparisons.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "15",
          "16"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Research SIMD Libraries",
            "description": "Investigate the available SIMD libraries such as std::simd and portable_simd to determine the best fit for character comparison implementation.",
            "dependencies": [],
            "details": "Focus on understanding the API and capabilities of std::simd and portable_simd. Look for examples of character comparisons and performance benchmarks to guide implementation.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-03T07:39:28.000Z"
          },
          {
            "id": 2,
            "title": "Implement Character Comparison Function",
            "description": "Develop a function that utilizes SIMD instructions to compare multiple characters simultaneously.",
            "dependencies": [
              1
            ],
            "details": "Create a function that takes two character arrays and uses SIMD to perform equality checks. Ensure the function is optimized for performance and handles edge cases.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-03T07:39:28.006Z"
          },
          {
            "id": 3,
            "title": "Vectorize Inner Loop for Comparisons",
            "description": "Refactor the inner loops of the character comparison to utilize vectorized SIMD instructions for improved performance.",
            "dependencies": [
              2
            ],
            "details": "Identify the inner loops in the character comparison logic and replace them with SIMD vectorized operations. Ensure that the logic remains correct while improving speed.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-03T07:39:28.011Z"
          },
          {
            "id": 4,
            "title": "Benchmark Performance Improvements",
            "description": "Create benchmarks to measure the performance of the SIMD character comparison against the traditional method.",
            "dependencies": [
              3
            ],
            "details": "Develop a set of tests that compare the execution time of the SIMD implementation versus the non-SIMD implementation. Analyze the results to confirm the expected speedup.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-03T07:39:28.014Z"
          },
          {
            "id": 5,
            "title": "Document Implementation and Usage",
            "description": "Write documentation for the SIMD character comparison function, including usage examples and performance expectations.",
            "dependencies": [
              4
            ],
            "details": "Ensure that the documentation clearly explains how to use the SIMD character comparison function, including any limitations and performance benefits observed during benchmarking.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-03T07:39:28.017Z"
          }
        ],
        "updatedAt": "2025-12-03T07:39:28.017Z"
      },
      {
        "id": "23",
        "title": "Inner Loop Optimization",
        "description": "Unroll inner loops, use unsafe indexing, and optimize memory access patterns for cache efficiency",
        "details": "Unroll inner loops for small string lengths. Use unsafe indexing with bounds checks removed in hot paths. Optimize memory access patterns for cache efficiency. Expected impact: 10-30% speedup.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "15",
          "16"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-02T20:34:01.637Z"
      },
      {
        "id": "24",
        "title": "Integer Type Optimization",
        "description": "Use u16 or u8 for distance calculations when strings are bounded to reduce memory footprint",
        "details": "Use u16 or u8 for distance calculations when strings are bounded. Reduce memory footprint and improve cache locality. Dynamic type selection based on string length. Expected impact: 5-15% speedup, reduced memory usage.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "15",
          "16"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-02T21:25:46.741Z"
      },
      {
        "id": "25",
        "title": "SIMD for Cosine Similarity",
        "description": "Vectorize dot product and magnitude calculations using SIMD instructions",
        "details": "Vectorize dot product calculation using SIMD. Vectorize magnitude (sum of squares) computation. Use SIMD for element-wise multiplication. Expected impact: 3-5x speedup for vector operations.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "14"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement SIMD Dot Product",
            "description": "Vectorize the dot product calculation using SIMD instructions to enhance performance.",
            "dependencies": [],
            "details": "Utilize SIMD intrinsics to perform parallel multiplication and addition for the dot product of two vectors. Ensure that the implementation handles different vector sizes appropriately.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:29:12.718Z"
          },
          {
            "id": 2,
            "title": "Vectorize Magnitude Calculation",
            "description": "Implement SIMD for calculating the magnitude (sum of squares) of a vector.",
            "dependencies": [
              1
            ],
            "details": "Use SIMD to compute the sum of squares for the vector elements, followed by a horizontal addition to get the final magnitude. This should also handle edge cases like empty vectors.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:29:12.719Z"
          },
          {
            "id": 3,
            "title": "Element-wise Multiplication with SIMD",
            "description": "Use SIMD instructions for element-wise multiplication of two vectors.",
            "dependencies": [
              1
            ],
            "details": "Implement element-wise multiplication using SIMD to improve performance. Ensure that the implementation is optimized for cache usage and handles different vector lengths.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:29:12.720Z"
          },
          {
            "id": 4,
            "title": "Benchmark SIMD Implementations",
            "description": "Create benchmarks to compare the performance of SIMD implementations against non-SIMD versions.",
            "dependencies": [
              2,
              3
            ],
            "details": "Develop a set of benchmarks that measure the execution time of both SIMD and non-SIMD implementations for dot product and magnitude calculations. Use representative vector sizes for testing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:29:12.722Z"
          },
          {
            "id": 5,
            "title": "Document SIMD Implementation",
            "description": "Write documentation for the SIMD implementation of cosine similarity calculations.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Provide detailed documentation on how the SIMD implementation works, including usage examples and performance expectations. Ensure that it is clear and accessible for future developers.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-02T20:29:12.723Z"
          }
        ],
        "updatedAt": "2025-12-02T20:29:12.723Z"
      },
      {
        "id": "26",
        "title": "Cosine Similarity Memory Optimization",
        "description": "Optimize memory access patterns, cache-friendly iteration order, and reduce temporary allocations",
        "details": "Optimize memory access patterns for vector operations. Cache-friendly iteration order. Reduce temporary allocations. Expected impact: 10-20% speedup.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "25"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-03T07:39:28.725Z"
      },
      {
        "id": "27",
        "title": "Diagonal Band Optimization for Levenshtein (HIGHEST PRIORITY)",
        "description": "Implement diagonal band algorithm to reduce Levenshtein computation from O(mn) to O(mk) where k is max distance. This is the highest ROI optimization addressing the 8x performance gap.",
        "details": "Implement diagonal band algorithm for Levenshtein distance:\n- Only compute cells within diagonal band: [i-j] <= max_distance\n- Use banded matrix storage to reduce memory footprint\n- Estimate max distance from length difference and threshold\n- Apply to both bounded and unbounded Levenshtein calculations\n- Reduce computation from O(mn) to O(mk) where k << n\n- Expected impact: 5-10x speedup for typical cases (makes Levenshtein competitive with RapidFuzz)\n- This addresses the critical 8x performance gap on large datasets (0.161s vs 0.0198s)",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "26"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-03T21:26:14.126Z"
      },
      {
        "id": "28",
        "title": "SIMD for Diagonal Band Computation",
        "description": "Add explicit SIMD vectorization to diagonal band algorithm to achieve 10-40x total speedup vs baseline",
        "details": "Add explicit SIMD vectorization to diagonal band algorithm:\n- Use std::simd (portable_simd) to process multiple cells in band in parallel\n- Vectorize the min operations in the DP recurrence relation\n- Handle data dependencies in SIMD-friendly way\n- Runtime CPU feature detection (AVX-512, AVX2, SSE, NEON)\n- Select optimal SIMD width based on CPU capabilities\n- Expected impact: Additional 2-4x speedup on top of diagonal band (10-40x total vs baseline)\n- Would make Levenshtein significantly faster than RapidFuzz (target: 0.004-0.016s vs RapidFuzz 0.0198s)",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "27"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-03T21:41:21.181Z"
      },
      {
        "id": "29",
        "title": "Explicit SIMD for Character Comparison",
        "description": "Replace auto-vectorized character comparison with explicit std::simd implementation for 2-4x additional speedup",
        "details": "Replace auto-vectorized character comparison with explicit SIMD:\n- Use u8x64 vectors for AVX-512, u8x32 for AVX2, u8x16 for SSE/NEON\n- Implement count_differences_simd() using explicit SIMD intrinsics\n- Add CPU feature detection for optimal lane width selection\n- Process 64 bytes at a time with AVX-512, 32 with AVX2, 16 with SSE\n- Handle remainder with scalar fallback\n- Expected impact: 2-4x additional speedup over current auto-vectorization\n- Applies to Hamming distance and character comparison in other algorithms",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "22"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-03T21:41:21.185Z"
      },
      {
        "id": "30",
        "title": "Explicit SIMD for Cosine Similarity Enhancement",
        "description": "Enhance existing cosine similarity with explicit std::simd implementation for 20-50x total speedup vs NumPy",
        "details": "Enhance existing cosine similarity SIMD with explicit std::simd:\n- Use f64x8 vectors for AVX-512, f64x4 for AVX2\n- Replace loop unrolling with explicit SIMD vector operations\n- Vectorize dot product, norm_a, and norm_b calculations simultaneously\n- Add CPU feature detection and runtime selection\n- Use Simd::from_slice() and reduce_sum() for efficient operations\n- Expected impact: Additional 2-3x speedup (20-50x total vs NumPy)\n- Current: 17x faster, Target: 20-50x faster",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "25",
          "26"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-03T21:41:21.187Z"
      },
      {
        "id": "31",
        "title": "Jaro-Winkler SIMD Optimization (CRITICAL PRIORITY)",
        "description": "Implement comprehensive SIMD optimizations for Jaro-Winkler similarity to address the only function slower than RapidFuzz. Currently 0.88x slower on large datasets - CRITICAL priority.",
        "details": "Task 31: Jaro-Winkler SIMD Optimization (CRITICAL PRIORITY)\n\nStatus: Currently 0.88x slower than RapidFuzz on large datasets - ONLY function slower than reference\nPriority: CRITICAL - Highest ROI optimization\nExpected Impact: 3-5x speedup (would make it 2.6-4.4x faster than RapidFuzz)\n\nSubtask 31.1: SIMD Buffer Clearing\n- Implement clear_buffer_simd() using u8x32 vectors\n- Replace loop-based buffer clearing in jaro_similarity_bytes()\n- Use ptr::write_bytes as fallback when SIMD unavailable\n- Expected: 10-20% speedup\n\nSubtask 31.2: SIMD Character Comparison in Matching Loop\n- Implement SIMD character matching using u8x32 vectors\n- Process 32 bytes at a time in matching window\n- Use simd_eq() and to_bitmask() for efficient comparison\n- Expected: 2-4x speedup for matching phase\n\nSubtask 31.3: Early Exit Optimizations\n- Add length difference check (return 0.0 if >50% difference)\n- Add character set overlap check for long strings (>10 chars)\n- Add #[likely]/#[unlikely] branch hints\n- Expected: 1.5-3x speedup for mismatched strings\n\nSubtask 31.4: SIMD Transposition Counting\n- Vectorize character comparison in transposition counting\n- Use SIMD for parallel character checks where possible\n- Expected: 5-15% speedup\n\nSubtask 31.5: Hash-Based Matching for Long Strings (Optional)\n- Use HashMap for O(1) character lookup when strings >50 chars\n- Fall back to SIMD matching for shorter strings\n- Expected: 3-5x speedup for very long strings\n\nImplementation Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Functions: jaro_similarity_bytes(), jaro_winkler_similarity_impl()",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "30"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement SIMD Buffer Clearing",
            "description": "Develop the clear_buffer_simd() function using u8x32 vectors to optimize buffer clearing in the jaro_similarity_bytes() function.",
            "dependencies": [],
            "details": "Replace the existing loop-based buffer clearing with SIMD instructions for improved performance. Ensure that ptr::write_bytes is used as a fallback when SIMD is not available.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.467Z"
          },
          {
            "id": 2,
            "title": "Optimize Character Comparison in Matching Loop",
            "description": "Implement SIMD character matching in the matching loop to process 32 bytes at a time using u8x32 vectors.",
            "dependencies": [
              1
            ],
            "details": "Utilize simd_eq() and to_bitmask() functions for efficient character comparisons, aiming to significantly speed up the matching phase of the algorithm.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.470Z"
          },
          {
            "id": 3,
            "title": "Add Early Exit Optimizations",
            "description": "Introduce early exit conditions to improve performance by checking length differences and character set overlaps before full processing.",
            "dependencies": [
              2
            ],
            "details": "Implement checks for length differences (return 0.0 if >50% difference) and character set overlaps for long strings. Use #[likely]/#[unlikely] hints to optimize branching.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.476Z"
          },
          {
            "id": 4,
            "title": "Vectorize Transposition Counting",
            "description": "Optimize the transposition counting phase by vectorizing character comparisons using SIMD techniques.",
            "dependencies": [
              3
            ],
            "details": "Implement parallel character checks where possible to enhance performance during the transposition counting process.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.480Z"
          },
          {
            "id": 5,
            "title": "Implement Hash-Based Matching for Long Strings",
            "description": "Develop a hash-based matching strategy for strings longer than 50 characters to improve lookup efficiency.",
            "dependencies": [
              4
            ],
            "details": "Utilize a HashMap for O(1) character lookups while falling back to SIMD matching for shorter strings, aiming for significant speed improvements on long strings.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.500Z"
          }
        ],
        "updatedAt": "2025-12-04T22:50:36.500Z"
      },
      {
        "id": "32",
        "title": "Damerau-Levenshtein SIMD Optimization",
        "description": "Add SIMD optimizations to Damerau-Levenshtein similarity. Currently 1.90x faster than RapidFuzz, but no SIMD. Expected 2-3x additional speedup.",
        "details": "Task 32: Damerau-Levenshtein SIMD Optimization\n\nStatus: Currently 1.90x faster than RapidFuzz, but no SIMD\nPriority: High\nExpected Impact: 2-3x speedup (would make it 3.8-5.7x faster than RapidFuzz)\n\nSubtask 32.1: SIMD Min Operations for DP Matrix\n- Implement simd_min3_u32x8() for parallel min operations\n- Vectorize the DP recurrence: min(prev_row[i] + 1, curr_row[i-1] + 1, prev_row[i-1] + cost)\n- Process 8 cells in parallel using u32x8 vectors\n- Expected: 2-3x speedup\n\nSubtask 32.2: SIMD Character Comparison\n- Use SIMD for cost calculation (character equality)\n- Vectorize the inner loop character comparisons\n- Expected: 10-20% additional speedup\n\nSubtask 32.3: SIMD Transposition Check\n- Vectorize the transposition condition check\n- Use SIMD for parallel character comparisons in transposition detection\n- Expected: 5-10% additional speedup\n\nImplementation Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Function: damerau_levenshtein_distance_bytes()",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "31"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement SIMD Min Operations for DP Matrix",
            "description": "Develop the simd_min3_u32x8() function to perform parallel minimum operations in the DP matrix. This involves vectorizing the DP recurrence relation to compute the minimum of three values simultaneously, processing 8 cells in parallel using u32x8 vectors.",
            "dependencies": [],
            "details": "The function will utilize SIMD instructions to optimize the minimum operation in the dynamic programming matrix, allowing for significant performance improvements. Ensure that the implementation is compatible with existing data structures and integrates smoothly into the current algorithm.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.511Z"
          },
          {
            "id": 2,
            "title": "Vectorize Character Comparison for SIMD",
            "description": "Enhance the Damerau-Levenshtein algorithm by implementing SIMD for character equality checks. This will involve vectorizing the inner loop that compares characters to calculate costs, aiming for a speedup in the similarity computation.",
            "dependencies": [],
            "details": "The implementation should focus on leveraging SIMD instructions to compare multiple characters at once, reducing the number of iterations required for character comparisons. This will be crucial for improving overall performance.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.524Z"
          },
          {
            "id": 3,
            "title": "Implement SIMD Transposition Check",
            "description": "Create a SIMD-optimized function to check for transpositions in the Damerau-Levenshtein algorithm. This will involve vectorizing the condition checks for character transpositions to enhance performance.",
            "dependencies": [],
            "details": "The transposition check will be optimized to use SIMD instructions, allowing for parallel processing of character comparisons. This should be integrated into the existing algorithm without altering its logic.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.539Z"
          },
          {
            "id": 4,
            "title": "Integrate SIMD Optimizations into Main Function",
            "description": "Modify the damerau_levenshtein_distance_bytes() function to incorporate the newly developed SIMD optimizations. Ensure that all SIMD functions are called correctly and that the overall algorithm maintains its integrity.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "This integration will require careful testing to ensure that the SIMD optimizations do not introduce errors into the distance calculations. Validate the output against known results to confirm correctness.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.557Z"
          },
          {
            "id": 5,
            "title": "Benchmark and Validate SIMD Performance",
            "description": "Conduct performance benchmarking to evaluate the speed improvements gained from the SIMD optimizations in the Damerau-Levenshtein algorithm. Compare results against the previous implementation and document findings.",
            "dependencies": [
              4
            ],
            "details": "The benchmarking should include a variety of test cases to assess performance under different conditions. Document the speedup achieved and any discrepancies observed during testing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.570Z"
          }
        ],
        "updatedAt": "2025-12-04T22:50:36.570Z"
      },
      {
        "id": "33",
        "title": "Extend Levenshtein SIMD to Unbounded Queries",
        "description": "Extend SIMD optimizations to unbounded Levenshtein queries. Currently only bounded queries have SIMD (partial coverage). Expected 1.5-2x speedup for unbounded queries.",
        "details": "Task 33: Extend Levenshtein SIMD to Unbounded Queries\n\nStatus: Currently only bounded queries have SIMD (partial coverage)\nPriority: Medium\nExpected Impact: 1.5-2x speedup for unbounded queries\n\nSubtask 33.1: Adaptive Band with SIMD\n- Extend levenshtein_distance_adaptive_band() to use SIMD\n- Apply SIMD min operations to adaptive band computation\n- Use runtime distance estimation for band width\n- Expected: 1.5-2x speedup\n\nSubtask 33.2: SIMD for Standard Wagner-Fischer\n- Add SIMD optimization to unbounded levenshtein_distance_bytes()\n- Vectorize min operations in DP recurrence\n- Process multiple cells in parallel\n- Expected: 1.2-1.5x speedup\n\nImplementation Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Functions: levenshtein_distance_adaptive_band(), levenshtein_distance_bytes_unbanded()",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "32"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement SIMD for Adaptive Band",
            "description": "Extend the levenshtein_distance_adaptive_band() function to utilize SIMD optimizations for improved performance.",
            "dependencies": [],
            "details": "Incorporate SIMD min operations into the adaptive band computation. Use runtime distance estimation to dynamically adjust the band width for unbounded queries.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.589Z"
          },
          {
            "id": 2,
            "title": "Optimize Wagner-Fischer with SIMD",
            "description": "Add SIMD optimizations to the unbounded levenshtein_distance_bytes() function to enhance its efficiency.",
            "dependencies": [],
            "details": "Vectorize the minimum operations in the dynamic programming recurrence relation to allow processing of multiple cells in parallel, targeting a speedup of 1.2-1.5x.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.597Z"
          },
          {
            "id": 3,
            "title": "Benchmark SIMD Performance",
            "description": "Conduct performance benchmarks to evaluate the speed improvements achieved by the SIMD optimizations for both adaptive band and Wagner-Fischer implementations.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create a suite of tests to measure execution time and compare results against the non-SIMD versions to validate the expected speedup of 1.5-2x.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.632Z"
          },
          {
            "id": 4,
            "title": "Document SIMD Implementation",
            "description": "Prepare comprehensive documentation detailing the SIMD optimizations made to the Levenshtein functions.",
            "dependencies": [
              1,
              2
            ],
            "details": "Include explanations of the changes made, performance expectations, and usage examples to assist future developers in understanding the SIMD enhancements.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.644Z"
          },
          {
            "id": 5,
            "title": "Review and Refactor Code",
            "description": "Perform a code review and refactor the SIMD implementations for clarity and maintainability.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Ensure that the code adheres to best practices, is well-commented, and is structured for easy future modifications. Address any identified issues during the benchmarking phase.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.650Z"
          }
        ],
        "updatedAt": "2025-12-04T22:50:36.650Z"
      },
      {
        "id": "34",
        "title": "Enhanced SIMD for Cosine Similarity",
        "description": "Enhance existing cosine similarity SIMD with AVX-512 support and FMA instructions. Already 39x faster, but can be enhanced further. Expected additional 1.5-2x speedup (60-80x total vs NumPy).",
        "details": "Task 34: Enhanced SIMD for Cosine Similarity\n\nStatus: Already has SIMD (39x faster), but can be enhanced\nPriority: Low (already excellent performance)\nExpected Impact: Additional 1.5-2x speedup (60-80x total vs NumPy)\n\nSubtask 34.1: AVX-512 Support\n- Add f64x8 vectors for AVX-512 capable CPUs\n- Runtime CPU feature detection\n- Process 8 doubles at a time (vs current 4)\n- Expected: 1.5-2x speedup on AVX-512 systems\n\nSubtask 34.2: Fused Multiply-Add (FMA)\n- Use FMA instructions for dot product: a * b + acc\n- More accurate and faster than separate multiply/add\n- Expected: 5-10% additional speedup\n\nImplementation Location:\n- polars/crates/polars-ops/src/chunked_array/array/similarity.rs\n- Function: dot_product_and_norms_explicit_simd()",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "33"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement AVX-512 Support",
            "description": "Add support for AVX-512 by implementing f64x8 vectors for capable CPUs. This includes runtime CPU feature detection to ensure compatibility and processing of 8 doubles simultaneously instead of the current 4.",
            "dependencies": [],
            "details": "Modify the existing SIMD implementation to utilize AVX-512 instructions. Ensure that the code checks for CPU capabilities at runtime to avoid crashes on unsupported hardware. This will involve updating the function dot_product_and_norms_explicit_simd() to handle the new vector size.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.660Z"
          },
          {
            "id": 2,
            "title": "Integrate Fused Multiply-Add (FMA) Instructions",
            "description": "Incorporate FMA instructions into the dot product calculation to improve performance and accuracy. This will replace the separate multiply and add operations with a single instruction.",
            "dependencies": [
              1
            ],
            "details": "Update the dot_product_and_norms_explicit_simd() function to utilize FMA for calculating the dot product. This change is expected to yield an additional 5-10% speedup by reducing the number of instructions executed.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.681Z"
          },
          {
            "id": 3,
            "title": "Benchmark Current Performance",
            "description": "Establish a baseline performance measurement for the existing SIMD implementation of cosine similarity before enhancements. This will help quantify the improvements made after implementing AVX-512 and FMA.",
            "dependencies": [],
            "details": "Create a set of benchmark tests that measure the performance of the current SIMD implementation. Use a variety of input sizes and types to ensure comprehensive coverage. Document the results for comparison after enhancements are made.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.692Z"
          },
          {
            "id": 4,
            "title": "Optimize Memory Access Patterns",
            "description": "Review and optimize memory access patterns in the SIMD implementation to ensure cache-friendly iteration and reduce temporary allocations. This will further enhance performance.",
            "dependencies": [
              1,
              2
            ],
            "details": "Analyze the current memory access patterns in the dot_product_and_norms_explicit_simd() function. Implement changes to improve cache locality and reduce the number of temporary allocations during calculations.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.700Z"
          },
          {
            "id": 5,
            "title": "Conduct Performance Testing and Validation",
            "description": "After implementing AVX-512 and FMA, conduct thorough performance testing to validate the improvements against the baseline measurements. Ensure that the enhancements do not introduce any regressions.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Run the benchmark tests established in subtask 3 after implementing the enhancements. Compare the results to the baseline to quantify the performance improvements. Document any discrepancies and ensure that the implementation remains stable and accurate.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:50:36.708Z"
          }
        ],
        "updatedAt": "2025-12-04T22:50:36.708Z"
      },
      {
        "id": "35",
        "title": "Hamming Similarity Small Dataset Optimization",
        "description": "Optimize Hamming similarity for small datasets (1K strings) where RapidFuzz is currently 1.14x faster. Target: make Polars faster than RapidFuzz on all dataset sizes.",
        "details": "Task 35: Hamming Similarity Small Dataset Optimization\n\nStatus: RapidFuzz 1.14x faster on 1K strings (length 10)\nRoot Cause: Per-element overhead dominates for small strings\nPriority: High\nExpected Impact: 1.5-2x speedup on small datasets\n\nKey optimizations:\n1. Batch ASCII detection at column level - check once, not per element\n2. Ultra-fast inline path for strings 16 bytes - no function calls\n3. Branchless XOR-based counting - better CPU pipelining\n4. Specialized column-level processing - bypass generic iterator\n\nImplementation Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Functions: hamming_similarity(), hamming_similarity_impl(), count_differences_optimized()",
        "testStrategy": "Benchmark against RapidFuzz on 1K strings (length 10). Target: Polars 1.2x+ faster than RapidFuzz.",
        "status": "done",
        "dependencies": [
          "34"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Batch ASCII Detection at Column Level",
            "description": "Check if entire column is ASCII once using SIMD scan, skip per-element is_ascii_str() checks when column is pure ASCII.",
            "dependencies": [],
            "details": "Implement a column-level ASCII detection function that scans the entire StringChunked once. Store or return ASCII flag. Modify hamming_similarity() to use this check before processing. Expected: 20-30% speedup by eliminating redundant per-element checks.",
            "status": "done",
            "testStrategy": "Verify ASCII detection is correct for mixed and pure-ASCII columns. Benchmark overhead of column scan vs per-element checks.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:51.952Z"
          },
          {
            "id": 2,
            "title": "Ultra-Fast Inline Path for Very Small Strings",
            "description": "For strings 16 bytes, use completely inline comparison with no function calls and no bounds checks.",
            "dependencies": [],
            "details": "Create an inline path in count_differences_optimized() for strings 16 bytes. Use u64 XOR comparison to process 8 bytes at a time. Manual loop unrolling for 1-16 byte cases. Use #[inline(always)] and unsafe indexing where safe. Expected: 30-50% speedup for small strings.",
            "status": "done",
            "testStrategy": "Verify correctness for all string lengths 1-16. Benchmark small string performance.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:51.956Z"
          },
          {
            "id": 3,
            "title": "Branchless XOR-Based Counting",
            "description": "Replace branching comparison with branchless version using XOR and popcount.",
            "dependencies": [],
            "details": "Replace `if s1[i] != s2[i] { 1 } else { 0 }` with branchless `((s1[i] ^ s2[i]) != 0) as usize`. For SIMD path, use XOR followed by popcount on the mask. Better CPU pipelining, eliminates branch mispredictions. Expected: 10-20% speedup.",
            "status": "done",
            "testStrategy": "Verify identical results to branching version. Benchmark branch misprediction improvement.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:51.960Z"
          },
          {
            "id": 4,
            "title": "Specialized Column-Level Processing",
            "description": "Implement hamming_similarity_batch() that processes entire column directly for homogeneous columns.",
            "dependencies": [
              1
            ],
            "details": "For columns where all strings have same length and are all ASCII, bypass broadcast_binary_elementwise iterator. Process column directly with minimal per-element overhead. Reduces function call overhead significantly. Expected: 15-25% speedup for batch processing.",
            "status": "done",
            "testStrategy": "Verify results match standard implementation. Benchmark on homogeneous vs heterogeneous columns.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:51.963Z"
          },
          {
            "id": 5,
            "title": "Benchmark and Validate Hamming Optimizations",
            "description": "Run comprehensive benchmarks comparing optimized Hamming against RapidFuzz on all dataset sizes.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Run benchmark_similarity.py with focus on Hamming results. Compare small (1K), medium (10K), and large (100K) datasets. Document speedup achieved. Target: Polars faster than RapidFuzz on all sizes.",
            "status": "done",
            "testStrategy": "All existing tests must pass. Benchmark shows Polars >= RapidFuzz on 1K dataset.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:51.966Z"
          }
        ],
        "updatedAt": "2025-12-05T03:18:51.966Z"
      },
      {
        "id": "36",
        "title": "Jaro-Winkler Large Dataset Optimization",
        "description": "Optimize Jaro-Winkler similarity for large datasets (100K strings) where RapidFuzz is currently 1.08x faster. Target: make Polars faster than RapidFuzz on all dataset sizes.",
        "details": "Task 36: Jaro-Winkler Large Dataset Optimization\n\nStatus: RapidFuzz 1.08x faster on 100K strings (length 30)\nRoot Cause: Match window iteration and function call overhead (3M+ calls per benchmark)\nPriority: Critical\nExpected Impact: 1.3-1.8x speedup on large datasets\n\nKey optimizations:\n1. Inline SIMD character search - eliminate function call overhead\n2. Bit-parallel match tracking - use u64 bitmasks instead of Vec<bool>\n3. Pre-indexed character position lookup - O(1) instead of O(n) search\n4. Stack-allocated buffers - avoid thread-local overhead for small strings\n5. Parallel processing with Rayon - multi-threaded for large datasets\n\nImplementation Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Functions: jaro_winkler_similarity(), jaro_similarity_bytes_simd(), simd_find_match_in_range()",
        "testStrategy": "Benchmark against RapidFuzz on 100K strings (length 30). Target: Polars 1.3x+ faster than RapidFuzz.",
        "status": "done",
        "dependencies": [
          "35"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Inline SIMD Character Search",
            "description": "Inline simd_find_match_in_range() into main Jaro loop to eliminate 3M+ function calls per benchmark.",
            "dependencies": [],
            "details": "Currently simd_find_match_in_range() is called for every character in s1 (30 chars  100K pairs = 3M calls). Inline the SIMD search logic directly into jaro_similarity_bytes_simd(). Use #[inline(always)] or manual inlining. Expected: 15-25% speedup from eliminated call overhead.",
            "status": "done",
            "testStrategy": "Verify identical results. Profile to confirm function call overhead eliminated.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:53.676Z"
          },
          {
            "id": 2,
            "title": "Bit-Parallel Match Tracking",
            "description": "Replace Vec<bool> match arrays with u64 bitmasks for faster match tracking.",
            "dependencies": [],
            "details": "For strings up to 64 characters, use u64 bitmasks instead of Vec<bool>. Match tracking: matches |= 1 << j. Check matches: (matches >> j) & 1 == 0. Clear: matches = 0. Much faster operations, better cache locality. Fall back to Vec<bool> for strings >64 chars. Expected: 20-30% speedup.",
            "status": "done",
            "testStrategy": "Verify correctness for strings of all lengths including >64. Benchmark bit operations vs array access.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:53.685Z"
          },
          {
            "id": 3,
            "title": "Pre-Indexed Character Position Lookup",
            "description": "Build character position index for s2 enabling O(1) lookup instead of linear search.",
            "dependencies": [],
            "details": "Build HashMap<u8, SmallVec<[u8; 4]>> mapping characters to their positions in s2. For each char in s1, lookup candidate positions directly instead of searching window. Especially effective for strings with repeated characters. Expected: 20-40% speedup for strings >20 chars.",
            "status": "done",
            "testStrategy": "Verify correctness. Benchmark for various string lengths and character distributions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:53.694Z"
          },
          {
            "id": 4,
            "title": "Stack-Allocated Buffers for Small Strings",
            "description": "Use stack-allocated [bool; 64] array instead of thread-local Vec for strings 64 chars.",
            "dependencies": [],
            "details": "The JARO_BUFFER.with() call has non-trivial overhead. For strings 64 characters (most common case), use stack-allocated arrays: [bool; 64] or [u64; 1] bitmask. Only use thread-local for larger strings. Expected: 10-15% speedup.",
            "status": "done",
            "testStrategy": "Verify correctness for boundary cases (63, 64, 65 char strings). Benchmark thread-local vs stack allocation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:53.703Z"
          },
          {
            "id": 5,
            "title": "Parallel Processing with Rayon",
            "description": "Process multiple string pairs in parallel using Rayon for multi-core speedup.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Split column into chunks using Rayon's par_chunks(). Process chunks in parallel, each with its own buffers. Merge results maintaining order. Ensure thread-local buffers don't cause contention. Expected: 2-4x speedup on multi-core systems.",
            "status": "done",
            "testStrategy": "Verify results identical to single-threaded. Benchmark on multi-core system. Test thread safety.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:53.708Z"
          },
          {
            "id": 6,
            "title": "Benchmark and Validate Jaro-Winkler Optimizations",
            "description": "Run comprehensive benchmarks comparing optimized Jaro-Winkler against RapidFuzz on all dataset sizes.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Run benchmark_similarity.py with focus on Jaro-Winkler results. Compare small (1K), medium (10K), and large (100K) datasets. Document speedup achieved. Target: Polars faster than RapidFuzz on all sizes.",
            "status": "done",
            "testStrategy": "All existing tests must pass. Benchmark shows Polars >= RapidFuzz on 100K dataset.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:53.715Z"
          }
        ],
        "updatedAt": "2025-12-05T03:18:53.715Z"
      },
      {
        "id": "37",
        "title": "General Column-Level Optimizations",
        "description": "Implement column-level optimizations applicable to all similarity functions: metadata pre-scanning, chunked parallel processing, and SIMD column scanning.",
        "details": "Task 37: General Column-Level Optimizations\n\nPriority: Medium\nExpected Impact: 10-20% speedup across all similarity functions\n\nKey optimizations:\n1. Pre-scan column metadata - determine ASCII, max/min length once\n2. Chunked parallel processing - Rayon par_chunks() for all functions\n3. SIMD column scanning - vectorized ASCII detection and length extraction\n\nThese optimizations benefit all similarity functions and reduce per-element overhead.\n\nImplementation Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- New helper functions for column metadata and parallel processing",
        "testStrategy": "Benchmark all similarity functions before and after. Target: 10-20% improvement across the board.",
        "status": "done",
        "dependencies": [
          "36"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Pre-scan Column Metadata",
            "description": "Scan column once to determine: all ASCII? max length? min length? Use metadata to select optimal algorithm path.",
            "dependencies": [],
            "details": "Implement column_metadata() function that scans StringChunked once and returns struct with: is_all_ascii, max_len, min_len, has_nulls. Use this metadata in all similarity functions to skip per-element checks and select optimal code paths.",
            "status": "done",
            "testStrategy": "Verify metadata accuracy on various columns. Benchmark scan overhead vs per-element check savings.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:54.834Z"
          },
          {
            "id": 2,
            "title": "Chunked Parallel Processing",
            "description": "Implement chunked parallel iteration for all similarity functions using Rayon.",
            "dependencies": [],
            "details": "Create parallel processing wrapper that splits columns into chunks, processes in parallel with Rayon par_chunks(), and merges results. Ensure thread-local buffers are properly handled. Apply to all similarity functions.",
            "status": "done",
            "testStrategy": "Verify results match single-threaded. Benchmark parallel speedup on multi-core systems.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:54.874Z"
          },
          {
            "id": 3,
            "title": "SIMD Column Scanning",
            "description": "Use SIMD to scan columns for ASCII detection and batch length extraction.",
            "dependencies": [],
            "details": "Implement SIMD-accelerated ASCII detection for entire column (check all bytes < 128). SIMD-based length extraction for batch processing. Pre-compute column statistics using vectorized operations.",
            "status": "done",
            "testStrategy": "Verify correctness. Benchmark SIMD scan vs scalar loop.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:54.884Z"
          },
          {
            "id": 4,
            "title": "Apply Column Optimizations to All Functions",
            "description": "Integrate column-level optimizations into Hamming, Levenshtein, Damerau-Levenshtein, and Jaro-Winkler.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Update all similarity functions to use column_metadata() for algorithm selection. Add parallel processing paths. Ensure consistent behavior and error handling across all functions.",
            "status": "done",
            "testStrategy": "All existing tests pass. Benchmark shows improvement across all functions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:54.888Z"
          }
        ],
        "updatedAt": "2025-12-05T03:18:54.888Z"
      },
      {
        "id": "38",
        "title": "SIMD-Optimized Prefix Calculation for Jaro-Winkler",
        "description": "Use SIMD to compare up to 4 bytes at once for prefix calculation instead of iterating byte-by-byte. Expected 10-20% speedup.",
        "details": "Task 38: SIMD-Optimized Prefix Calculation\n\nPriority: High\nExpected Impact: 10-20% speedup for prefix calculation\n\nImplementation:\n- Create `calculate_prefix_simd()` function using `u8x4` vectors\n- Use `simd_eq()` and `to_bitmask()` for efficient comparison\n- Compare first 4 bytes using SIMD (MAX_PREFIX_LENGTH = 4)\n- Find first mismatch using bitmask operations\n- Fall back to scalar for strings < 4 bytes\n- Apply to both `jaro_winkler_similarity_impl()` and `jaro_winkler_similarity_impl_ascii()`\n\nCode Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Functions: `jaro_winkler_similarity_impl()` (line 3331), `jaro_winkler_similarity_impl_ascii()` (line 3404)\n\nTesting:\n- Unit tests: Verify prefix calculation matches scalar version\n- Benchmark: Measure speedup on prefix calculation phase\n- Edge cases: Strings < 4 bytes, identical prefixes, no common prefix",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "36"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-04T04:24:27.392Z"
      },
      {
        "id": "39",
        "title": "Early Termination with Threshold for Jaro-Winkler",
        "description": "Stop matching early if similarity will be below threshold. Critical for threshold-based queries. Expected 2-5x speedup.",
        "details": "Task 39: Early Termination with Threshold\n\nPriority: High (CRITICAL for threshold-based queries)\nExpected Impact: 2-5x speedup for threshold-based queries (common use case)\n\nImplementation:\n- Add `min_threshold` parameter to `jaro_similarity_bytes()` variants\n- Calculate minimum matches needed for threshold using Jaro formula:\n  - Jaro formula: (m/len1 + m/len2 + (m-t)/m) / 3 >= threshold\n  - Solve for m: m >= threshold * 3 * len1 * len2 / (len1 + len2 + len1*len2)\n- Early exit check: if remaining potential matches can't reach threshold, return None\n- Integrate with `jaro_winkler_similarity_with_threshold()` function\n- Use conservative estimates to avoid false negatives\n\nCode Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Functions: `jaro_similarity_bytes()`, `jaro_similarity_bytes_simd()`, `jaro_winkler_similarity_with_threshold()`\n\nTesting:\n- Unit tests: Verify early termination doesn't produce false negatives\n- Benchmark: Measure speedup on threshold-based queries\n- Edge cases: Very low thresholds, very high thresholds, edge threshold values",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "36"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-04T04:24:27.395Z"
      },
      {
        "id": "40",
        "title": "Character Frequency Pre-Filtering for Jaro-Winkler",
        "description": "Replace HashSet-based character set overlap check with 256-element array. Expected 15-30% speedup.",
        "details": "Task 40: Character Frequency Pre-Filtering\n\nPriority: High\nExpected Impact: 15-30% speedup for character set overlap check\n\nImplementation:\n- Create `check_character_set_overlap_fast()` function\n- Use stack-allocated `[bool; 256]` array instead of HashSet\n- Count characters in s1: `s1_chars[c as usize] = true`\n- Check overlap with s2 in single pass: if `s1_chars[c as usize]` is true, return true\n- Zero allocation overhead, faster than HashSet for ASCII strings\n- Replace existing `check_character_set_overlap()` in `jaro_similarity_bytes_simd()`\n\nCode Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Function: `jaro_similarity_bytes_simd()` (line 1850)\n\nTesting:\n- Unit tests: Verify overlap detection matches HashSet version\n- Benchmark: Measure speedup on character set overlap check\n- Edge cases: No overlap, full overlap, single character overlap",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "36"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-04T04:24:27.397Z"
      },
      {
        "id": "41",
        "title": "Improved Transposition Counting with SIMD",
        "description": "Use SIMD to find next matched positions in s2_matches instead of sequential scanning. Expected 10-20% speedup.",
        "details": "Task 41: Improved Transposition Counting with SIMD\n\nPriority: Medium\nExpected Impact: 10-20% speedup for transposition counting\n\nImplementation:\n- Create `count_transpositions_simd_optimized()` function\n- Use SIMD to scan `s2_matches` in 32-byte chunks\n- Convert bool slice to u8 for SIMD operations\n- Use `simd_eq()` and `to_bitmask()` to find next matched position\n- Process chunks with `SimdU8::from_slice()` and `to_bitmask()`\n- Use `trailing_zeros()` to find first match in chunk\n- Replace existing transposition counting in `jaro_similarity_bytes_simd_large()`\n\nCode Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Function: `jaro_similarity_bytes_simd_large()` (line 1888)\n\nTesting:\n- Unit tests: Verify transposition count matches scalar version\n- Benchmark: Measure speedup on transposition counting phase\n- Edge cases: Many transpositions, no transpositions, all transpositions",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "36"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-04T04:24:27.399Z"
      },
      {
        "id": "42",
        "title": "Optimized Hash-Based Implementation for Jaro-Winkler",
        "description": "Use SmallVec instead of Vec<usize> in HashMap to avoid heap allocations for small character sets. Expected 10-20% speedup.",
        "details": "Task 42: Optimized Hash-Based Implementation\n\nPriority: Medium\nExpected Impact: 10-20% speedup for hash-based path\n\nImplementation:\n- Add `smallvec` dependency to `polars-ops/Cargo.toml`\n- Replace `HashMap<u8, Vec<usize>>` with `HashMap<u8, SmallVec<[usize; 4]>>`\n- Most strings have <10 unique characters, so SmallVec avoids heap allocations\n- Update `jaro_similarity_bytes_hash_based()` function\n- Use `SmallVec::new()` instead of `Vec::new()` for position vectors\n\nCode Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Function: `jaro_similarity_bytes_hash_based()` (line 2028)\n- Dependency: Add to `polars-ops/Cargo.toml`\n\nTesting:\n- Unit tests: Verify hash-based matching produces identical results\n- Benchmark: Measure speedup on hash-based path (strings >50 chars)\n- Edge cases: Many unique characters, few unique characters, single character strings",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "36"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-04T04:24:27.401Z"
      },
      {
        "id": "43",
        "title": "Adaptive Algorithm Selection for Jaro-Winkler",
        "description": "Choose algorithm variant based on string characteristics (length, character distribution). Expected 10-30% speedup.",
        "details": "Task 43: Adaptive Algorithm Selection\n\nPriority: Medium\nExpected Impact: 10-30% speedup by using optimal algorithm for each case\n\nImplementation:\n- Create `jaro_similarity_bytes_adaptive()` dispatch function\n- Analyze string characteristics:\n  - Unique character count: `count_unique_chars()`\n  - Average character frequency: `(len1 + len2) / (unique_chars1 + unique_chars2)`\n- Select optimal algorithm based on characteristics:\n  - 64 chars: bit-parallel (`jaro_similarity_bitparallel()`)\n  - >50 chars + high frequency (>3.0): hash-based (`jaro_similarity_bytes_hash_based()`)\n  - Low diversity (<20 unique chars): hash-based with small maps\n  - Default: SIMD path (`jaro_similarity_bytes_simd()`)\n- Replace current dispatch logic in `jaro_similarity_bytes()`\n- Create helper functions: `count_unique_chars()`, `analyze_string_characteristics()`\n\nCode Location:\n- polars/crates/polars-ops/src/chunked_array/strings/similarity.rs\n- Function: `jaro_similarity_bytes()` (line 2794)\n\nTesting:\n- Unit tests: Verify all algorithm paths produce identical results\n- Benchmark: Measure speedup across different string characteristics\n- Edge cases: Very long strings, very short strings, high/low character diversity",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "36"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-04T04:24:27.402Z"
      },
      {
        "id": "44",
        "title": "Define Fuzzy Join API and Types",
        "description": "Define the public API for fuzzy joins including FuzzyJoinType enum, FuzzyJoinArgs struct, and FuzzyJoinKeep enum for output control.",
        "details": "Task 44: Define Fuzzy Join API and Types\n\nPriority: High\nPhase: 5 - Basic Fuzzy Join Implementation\n\nImplementation:\n- Add `FuzzyJoinType` enum to `polars-ops/src/frame/join/args.rs`:\n  - Levenshtein\n  - DamerauLevenshtein\n  - JaroWinkler\n  - Hamming\n- Add `FuzzyJoinArgs` struct:\n  - similarity_type: FuzzyJoinType\n  - threshold: f32 (0.0 to 1.0)\n  - left_on: String\n  - right_on: String\n  - suffix: String (default: \"_right\")\n  - keep: FuzzyJoinKeep\n- Add `FuzzyJoinKeep` enum:\n  - BestMatch (highest similarity)\n  - AllMatches (all above threshold)\n  - FirstMatch (first match found)\n- Implement Default trait for FuzzyJoinArgs\n- Ensure serialization/deserialization works (serde derives)\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/args.rs",
        "testStrategy": "Compile tests for type definitions, ensure serialization works with serde round-trip tests",
        "status": "done",
        "dependencies": [
          "43"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Define FuzzyJoinType Enum",
            "description": "Add FuzzyJoinType enum to args.rs with specified types.",
            "dependencies": [],
            "details": "Implement FuzzyJoinType enum in polars-ops/src/frame/join/args.rs with variants: Levenshtein, DamerauLevenshtein, JaroWinkler, Hamming.",
            "status": "done",
            "testStrategy": "Compile and check enum type definitions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:56.605Z"
          },
          {
            "id": 2,
            "title": "Define FuzzyJoinArgs Struct",
            "description": "Create FuzzyJoinArgs struct with necessary fields.",
            "dependencies": [
              1
            ],
            "details": "Add FuzzyJoinArgs struct in polars-ops/src/frame/join/args.rs with fields: similarity_type, threshold, left_on, right_on, suffix, keep.",
            "status": "done",
            "testStrategy": "Compile and check struct field definitions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:56.609Z"
          },
          {
            "id": 3,
            "title": "Define FuzzyJoinKeep Enum",
            "description": "Add FuzzyJoinKeep enum for output control.",
            "dependencies": [
              1
            ],
            "details": "Implement FuzzyJoinKeep enum in polars-ops/src/frame/join/args.rs with variants: BestMatch, AllMatches, FirstMatch.",
            "status": "done",
            "testStrategy": "Compile and check enum type definitions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:56.612Z"
          },
          {
            "id": 4,
            "title": "Implement Default Trait for FuzzyJoinArgs",
            "description": "Implement Default trait for FuzzyJoinArgs struct.",
            "dependencies": [
              2
            ],
            "details": "Provide a default implementation for FuzzyJoinArgs struct in args.rs to set default values for fields.",
            "status": "done",
            "testStrategy": "Compile and check default trait implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:56.616Z"
          },
          {
            "id": 5,
            "title": "Ensure Serialization/Deserialization",
            "description": "Ensure serde derives for serialization/deserialization.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add serde derives to FuzzyJoinArgs and FuzzyJoinKeep in args.rs to support serialization and deserialization.",
            "status": "done",
            "testStrategy": "Perform serde round-trip tests to ensure correct serialization/deserialization.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:56.619Z"
          }
        ],
        "updatedAt": "2025-12-05T03:18:56.619Z"
      },
      {
        "id": "45",
        "title": "Implement Core Fuzzy Join Logic",
        "description": "Implement the core fuzzy join algorithm using nested loop approach with O(n*m) baseline complexity.",
        "details": "Task 45: Implement Core Fuzzy Join Logic\n\nPriority: High\nPhase: 5 - Basic Fuzzy Join Implementation\nDependencies: Task 44\n\nImplementation:\n- Create `polars-ops/src/frame/join/fuzzy.rs`\n- Implement `fuzzy_join_inner()` function:\n  ```rust\n  pub fn fuzzy_join_inner(\n      left: &DataFrame,\n      right: &DataFrame,\n      args: FuzzyJoinArgs,\n  ) -> PolarsResult<DataFrame>\n  ```\n- Algorithm (O(n*m) baseline):\n  1. Extract string columns from both DataFrames\n  2. For each row in left, compute similarity with all rows in right\n  3. Filter pairs above threshold\n  4. Based on `keep` strategy, select matching rows\n  5. Build result DataFrame with matched rows\n- Handle null values (null similarity = null, excluded from matches)\n- Return joined DataFrame with similarity score column named \"_similarity\"\n- Reuse existing similarity kernels from similarity.rs\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy.rs (new file)\n- polars/crates/polars-ops/src/frame/join/mod.rs (add module)",
        "testStrategy": "Unit tests with small DataFrames, validate correctness against manual calculations",
        "status": "done",
        "dependencies": [
          "44"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Fuzzy Join Module File",
            "description": "Create the file for fuzzy join logic implementation.",
            "dependencies": [],
            "details": "Create `polars-ops/src/frame/join/fuzzy.rs` to house the fuzzy join logic.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:57.984Z"
          },
          {
            "id": 2,
            "title": "Implement Fuzzy Join Function",
            "description": "Implement the `fuzzy_join_inner()` function with O(n*m) complexity.",
            "dependencies": [
              1
            ],
            "details": "Implement `fuzzy_join_inner()` in the newly created file, following the specified function signature and algorithm steps.",
            "status": "done",
            "testStrategy": "Unit tests with small DataFrames, validate correctness against manual calculations.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:57.989Z"
          },
          {
            "id": 3,
            "title": "Extract String Columns",
            "description": "Extract string columns from both DataFrames for comparison.",
            "dependencies": [
              2
            ],
            "details": "Within `fuzzy_join_inner()`, extract string columns from the input DataFrames to prepare for similarity computation.",
            "status": "done",
            "testStrategy": "Ensure correct columns are extracted using test DataFrames.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:57.992Z"
          },
          {
            "id": 4,
            "title": "Compute Similarity and Filter Pairs",
            "description": "Compute similarity for each row pair and filter based on threshold.",
            "dependencies": [
              3
            ],
            "details": "Iterate over rows in the left DataFrame, compute similarity with rows in the right DataFrame, and filter pairs above the threshold.",
            "status": "done",
            "testStrategy": "Compare computed similarities with expected values, ensure filtering is correct.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:57.996Z"
          },
          {
            "id": 5,
            "title": "Build Result DataFrame",
            "description": "Construct the result DataFrame with matched rows and similarity scores.",
            "dependencies": [
              4
            ],
            "details": "Build the resulting DataFrame including matched rows and a similarity score column named '_similarity'. Handle null values appropriately.",
            "status": "done",
            "testStrategy": "Validate the structure and content of the result DataFrame against expected outputs.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:57.999Z"
          }
        ],
        "updatedAt": "2025-12-05T03:18:57.999Z"
      },
      {
        "id": "46",
        "title": "Implement Join Type Variants",
        "description": "Implement left, right, outer, and cross fuzzy join variants building on the core inner join logic.",
        "details": "Task 46: Implement Join Type Variants\n\nPriority: High\nPhase: 5 - Basic Fuzzy Join Implementation\nDependencies: Task 45\n\nImplementation:\n- `fuzzy_join_left()`: All left rows, matched right rows (nulls for non-matches)\n- `fuzzy_join_right()`: All right rows, matched left rows (nulls for non-matches)\n- `fuzzy_join_outer()`: All rows from both, matched where possible\n- `fuzzy_join_cross()`: Cartesian product filtered by similarity (all pairs above threshold)\n- Unified `fuzzy_join()` dispatcher function that takes JoinType parameter\n- Handle proper null propagation for each join type\n- Ensure column naming consistency (apply suffix to right columns)\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy.rs",
        "testStrategy": "Test each variant with known expected outputs, compare with equivalent SQL operations",
        "status": "done",
        "dependencies": [
          "45"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement fuzzy_join_left function",
            "description": "Develop the fuzzy_join_left function to include all left rows and matched right rows, with nulls for non-matches.",
            "dependencies": [],
            "details": "Create the fuzzy_join_left function in polars/crates/polars-ops/src/frame/join/fuzzy.rs. Ensure it handles null propagation correctly and matches the left rows with corresponding right rows.",
            "status": "done",
            "testStrategy": "Test with datasets where left rows have varying matches in right.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:59.285Z"
          },
          {
            "id": 2,
            "title": "Implement fuzzy_join_right function",
            "description": "Develop the fuzzy_join_right function to include all right rows and matched left rows, with nulls for non-matches.",
            "dependencies": [
              1
            ],
            "details": "Create the fuzzy_join_right function in polars/crates/polars-ops/src/frame/join/fuzzy.rs. Ensure it handles null propagation correctly and matches the right rows with corresponding left rows.",
            "status": "done",
            "testStrategy": "Test with datasets where right rows have varying matches in left.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:59.289Z"
          },
          {
            "id": 3,
            "title": "Implement fuzzy_join_outer function",
            "description": "Develop the fuzzy_join_outer function to include all rows from both sides, matched where possible.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create the fuzzy_join_outer function in polars/crates/polars-ops/src/frame/join/fuzzy.rs. Ensure it handles null propagation correctly and includes all rows from both DataFrames.",
            "status": "done",
            "testStrategy": "Test with datasets ensuring all rows from both sides are included.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:59.292Z"
          },
          {
            "id": 4,
            "title": "Implement fuzzy_join_cross function",
            "description": "Develop the fuzzy_join_cross function to perform a Cartesian product filtered by similarity threshold.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create the fuzzy_join_cross function in polars/crates/polars-ops/src/frame/join/fuzzy.rs. Implement logic to filter pairs based on a similarity threshold.",
            "status": "done",
            "testStrategy": "Test with datasets ensuring all pairs above the threshold are included.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:59.295Z"
          },
          {
            "id": 5,
            "title": "Develop unified fuzzy_join dispatcher",
            "description": "Create a unified fuzzy_join function that dispatches to the appropriate join variant based on a JoinType parameter.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Implement a dispatcher function in polars/crates/polars-ops/src/frame/join/fuzzy.rs that selects the correct join function based on the JoinType parameter.",
            "status": "done",
            "testStrategy": "Test dispatcher with all join types to ensure correct function calls.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:18:59.298Z"
          }
        ],
        "updatedAt": "2025-12-05T03:18:59.298Z"
      },
      {
        "id": "47",
        "title": "Add FunctionExpr for Fuzzy Join",
        "description": "Integrate fuzzy join into the expression system with proper schema inference and serialization.",
        "details": "Task 47: Add FunctionExpr for Fuzzy Join\n\nPriority: High\nPhase: 5 - Basic Fuzzy Join Implementation\nDependencies: Task 46\n\nImplementation:\n- Add `FuzzyJoin` variant to `FunctionExpr` in `polars-plan/src/dsl/function_expr/mod.rs`\n- Implement schema inference for fuzzy join output:\n  - Left schema columns\n  - Right schema columns (with suffix)\n  - Additional \"_similarity\" Float32 column\n- Handle serialization/deserialization (serde)\n- Wire up in physical expression builder:\n  - polars-expr/src/expressions/function_expr.rs\n- Add match arm in expression evaluation\n\nCode Location:\n- polars/crates/polars-plan/src/dsl/function_expr/mod.rs\n- polars/crates/polars-expr/src/expressions/function_expr.rs",
        "testStrategy": "Expression round-trip tests, lazy evaluation tests, schema validation",
        "status": "done",
        "dependencies": [
          "46"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Add FuzzyJoin Variant to FunctionExpr",
            "description": "Add the FuzzyJoin variant to the FunctionExpr enum in the specified module.",
            "dependencies": [],
            "details": "Modify polars-plan/src/dsl/function_expr/mod.rs to include a new variant for FuzzyJoin.",
            "status": "done",
            "testStrategy": "Ensure the variant is correctly added and accessible.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:04.230Z"
          },
          {
            "id": 2,
            "title": "Implement Schema Inference for Fuzzy Join",
            "description": "Develop schema inference logic for the FuzzyJoin output.",
            "dependencies": [
              1
            ],
            "details": "Implement logic to infer schema including left and right columns with suffixes and a '_similarity' column.",
            "status": "done",
            "testStrategy": "Validate schema inference with unit tests for various input scenarios.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:04.233Z"
          },
          {
            "id": 3,
            "title": "Handle Serialization/Deserialization",
            "description": "Implement serialization and deserialization for the FuzzyJoin variant using serde.",
            "dependencies": [
              1
            ],
            "details": "Ensure that the FuzzyJoin variant can be serialized and deserialized correctly.",
            "status": "done",
            "testStrategy": "Test serialization and deserialization processes to ensure data integrity.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:04.236Z"
          },
          {
            "id": 4,
            "title": "Integrate FuzzyJoin in Physical Expression Builder",
            "description": "Wire up the FuzzyJoin variant in the physical expression builder.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Modify polars-expr/src/expressions/function_expr.rs to handle FuzzyJoin in expression evaluation.",
            "status": "done",
            "testStrategy": "Run integration tests to verify correct expression evaluation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:04.240Z"
          },
          {
            "id": 5,
            "title": "Add Match Arm for Expression Evaluation",
            "description": "Add a match arm for evaluating the FuzzyJoin expression.",
            "dependencies": [
              4
            ],
            "details": "Update expression evaluation logic to include a case for FuzzyJoin.",
            "status": "done",
            "testStrategy": "Ensure expression evaluation is correct through unit and integration tests.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:04.249Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:04.249Z"
      },
      {
        "id": "48",
        "title": "DataFrame Method Interface",
        "description": "Add fuzzy_join method to LazyFrame and DataFrame for convenient API access.",
        "details": "Task 48: DataFrame Method Interface\n\nPriority: High\nPhase: 5 - Basic Fuzzy Join Implementation\nDependencies: Task 47\n\nImplementation:\n- In `polars-lazy/src/frame/mod.rs`, add:\n  ```rust\n  pub fn fuzzy_join(\n      self,\n      other: LazyFrame,\n      left_on: &str,\n      right_on: &str,\n      similarity: FuzzyJoinType,\n      threshold: f32,\n  ) -> LazyFrame\n  ```\n- Add builder pattern for additional options:\n  ```rust\n  pub fn fuzzy_join_builder(\n      self,\n      other: LazyFrame,\n  ) -> FuzzyJoinBuilder\n  ```\n- Support method chaining with other operations\n- Validate column types (must be String/Utf8)\n- Propagate errors for invalid configurations:\n  - Column not found\n  - Wrong column type\n  - Invalid threshold (outside 0.0-1.0)\n- Add eager DataFrame::fuzzy_join() that converts to lazy and collects\n\nCode Location:\n- polars/crates/polars-lazy/src/frame/mod.rs\n- polars/crates/polars-core/src/frame/mod.rs (eager wrapper)",
        "testStrategy": "Integration tests with lazy and eager evaluation, error handling tests",
        "status": "done",
        "dependencies": [
          "47"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement fuzzy_join Method in LazyFrame",
            "description": "Add the fuzzy_join method to LazyFrame in mod.rs.",
            "dependencies": [],
            "details": "In polars-lazy/src/frame/mod.rs, implement the fuzzy_join method with parameters for other frame, columns, similarity, and threshold.",
            "status": "done",
            "testStrategy": "Write unit tests to ensure correct joining behavior.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:05.566Z"
          },
          {
            "id": 2,
            "title": "Create FuzzyJoinBuilder Pattern",
            "description": "Develop a builder pattern for additional fuzzy join options.",
            "dependencies": [
              1
            ],
            "details": "Implement fuzzy_join_builder in polars-lazy/src/frame/mod.rs to allow method chaining and additional configurations.",
            "status": "done",
            "testStrategy": "Test builder pattern for correct option handling.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:05.568Z"
          },
          {
            "id": 3,
            "title": "Add Error Handling for Fuzzy Join",
            "description": "Implement error propagation for invalid configurations in fuzzy_join.",
            "dependencies": [
              1
            ],
            "details": "Ensure errors are raised for missing columns, incorrect types, and invalid thresholds in polars-lazy/src/frame/mod.rs.",
            "status": "done",
            "testStrategy": "Create tests to validate error handling for various invalid inputs.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:05.572Z"
          },
          {
            "id": 4,
            "title": "Implement Eager DataFrame::fuzzy_join Method",
            "description": "Add an eager version of fuzzy_join in DataFrame.",
            "dependencies": [
              1
            ],
            "details": "In polars-core/src/frame/mod.rs, implement DataFrame::fuzzy_join that converts to lazy and collects results.",
            "status": "done",
            "testStrategy": "Integration tests to verify eager and lazy results match.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:05.575Z"
          },
          {
            "id": 5,
            "title": "Validate Column Types for Fuzzy Join",
            "description": "Ensure columns used in fuzzy_join are of type String/Utf8.",
            "dependencies": [
              1
            ],
            "details": "Add validation logic in polars-lazy/src/frame/mod.rs to check column types before performing join.",
            "status": "done",
            "testStrategy": "Test with various column types to ensure only valid types are accepted.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:05.578Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:05.578Z"
      },
      {
        "id": "49",
        "title": "Python Bindings for Fuzzy Join",
        "description": "Expose fuzzy join to Python API with comprehensive type hints and docstrings.",
        "details": "Task 49: Python Bindings for Fuzzy Join\n\nPriority: High\nPhase: 5 - Basic Fuzzy Join Implementation\nDependencies: Task 48\n\nImplementation:\n- In `py-polars/polars/dataframe/frame.py`, add:\n  ```python\n  def fuzzy_join(\n      self,\n      other: DataFrame,\n      left_on: str,\n      right_on: str,\n      similarity: Literal[\"levenshtein\", \"damerau_levenshtein\", \"jaro_winkler\", \"hamming\"] = \"levenshtein\",\n      threshold: float = 0.8,\n      suffix: str = \"_right\",\n      keep: Literal[\"best\", \"all\", \"first\"] = \"best\",\n      how: Literal[\"inner\", \"left\", \"right\", \"outer\", \"cross\"] = \"inner\",\n  ) -> DataFrame\n  ```\n- Add to LazyFrame as well (lazy evaluation)\n- Type hints using Literal types for string options\n- Comprehensive docstrings with:\n  - Parameter descriptions\n  - Return value description\n  - Usage examples\n  - Notes on performance\n- Add to __all__ exports\n\nCode Location:\n- py-polars/polars/dataframe/frame.py\n- py-polars/polars/lazyframe/frame.py\n- py-polars/src/lazyframe/mod.rs (Rust bindings)",
        "testStrategy": "Python unit tests, validate against Rust implementation results",
        "status": "done",
        "dependencies": [
          "48"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Fuzzy Join Function in DataFrame",
            "description": "Add the fuzzy_join function to DataFrame with specified parameters.",
            "dependencies": [],
            "details": "In `py-polars/polars/dataframe/frame.py`, implement the fuzzy_join function with parameters for similarity, threshold, suffix, keep, and how.",
            "status": "done",
            "testStrategy": "Write unit tests to ensure the function behaves as expected with various parameters.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:06.864Z"
          },
          {
            "id": 2,
            "title": "Add Fuzzy Join to LazyFrame",
            "description": "Implement the fuzzy_join function for LazyFrame to support lazy evaluation.",
            "dependencies": [
              1
            ],
            "details": "In `py-polars/polars/lazyframe/frame.py`, add the fuzzy_join function mirroring the DataFrame implementation.",
            "status": "done",
            "testStrategy": "Test lazy evaluation by comparing results with eager execution.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:06.868Z"
          },
          {
            "id": 3,
            "title": "Add Type Hints and Docstrings",
            "description": "Include comprehensive type hints and docstrings for the fuzzy_join function.",
            "dependencies": [
              1,
              2
            ],
            "details": "Use Literal types for string options and provide detailed docstrings with parameter descriptions, return value, and examples.",
            "status": "done",
            "testStrategy": "Review docstrings for clarity and accuracy; ensure type hints are correct.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:06.871Z"
          },
          {
            "id": 4,
            "title": "Export Fuzzy Join in Module",
            "description": "Add fuzzy_join to the module's __all__ exports.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Ensure the fuzzy_join function is included in the module's exports for accessibility.",
            "status": "done",
            "testStrategy": "Verify that the function is accessible when the module is imported.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:06.874Z"
          },
          {
            "id": 5,
            "title": "Validate Python Bindings Against Rust Implementation",
            "description": "Ensure the Python fuzzy_join function matches the Rust implementation results.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Compare results of the Python fuzzy_join with the Rust implementation in `py-polars/src/lazyframe/mod.rs`.",
            "status": "done",
            "testStrategy": "Run integration tests to validate consistency between Python and Rust results.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:06.877Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:06.877Z"
      },
      {
        "id": "50",
        "title": "Fuzzy Join Testing Suite",
        "description": "Comprehensive test suite for fuzzy join functionality covering all similarity metrics, join types, and edge cases.",
        "details": "Task 50: Fuzzy Join Testing Suite\n\nPriority: High\nPhase: 5 - Basic Fuzzy Join Implementation\nDependencies: Tasks 45-49\n\nImplementation:\n- Create `crates/polars-ops/tests/fuzzy_join.rs`\n- Create `py-polars/tests/unit/operations/test_fuzzy_join.py`\n- Test cases:\n  - Basic inner/left/right/outer/cross joins\n  - All 4 similarity metrics (Levenshtein, Damerau-Levenshtein, Jaro-Winkler, Hamming)\n  - Various threshold values (0.0, 0.5, 0.8, 0.9, 1.0)\n  - All keep strategies (best, all, first)\n  - Null handling (nulls in left, right, both)\n  - Empty DataFrames (left empty, right empty, both empty)\n  - Single row DataFrames\n  - Large DataFrames (1K, 10K rows) for performance regression\n  - Unicode strings (non-ASCII characters)\n  - Edge cases:\n    - Identical strings (similarity = 1.0)\n    - Completely different strings (similarity  0.0)\n    - Empty strings\n    - Single character strings\n    - Very long strings\n- Validate against manual calculations\n- Property-based tests (fuzzy similarity is symmetric, etc.)\n\nCode Location:\n- polars/crates/polars-ops/tests/fuzzy_join.rs (new file)\n- py-polars/tests/unit/operations/test_fuzzy_join.py (new file)",
        "testStrategy": "All tests pass, edge cases covered, no regressions on similarity functions",
        "status": "done",
        "dependencies": [
          "45",
          "46",
          "47",
          "48",
          "49"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Rust Test File for Fuzzy Join",
            "description": "Create the Rust test file for fuzzy join functionality.",
            "dependencies": [],
            "details": "Create a new file at crates/polars-ops/tests/fuzzy_join.rs and set up the testing environment.",
            "status": "done",
            "testStrategy": "Ensure the file is created and accessible for testing.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:08.113Z"
          },
          {
            "id": 2,
            "title": "Create Python Test File for Fuzzy Join",
            "description": "Create the Python test file for fuzzy join functionality.",
            "dependencies": [
              1
            ],
            "details": "Create a new file at py-polars/tests/unit/operations/test_fuzzy_join.py and set up the testing environment.",
            "status": "done",
            "testStrategy": "Ensure the file is created and accessible for testing.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:08.118Z"
          },
          {
            "id": 3,
            "title": "Implement Basic Join Tests",
            "description": "Implement test cases for basic join types and similarity metrics.",
            "dependencies": [
              1,
              2
            ],
            "details": "Write test cases for inner, left, right, outer, and cross joins using all four similarity metrics.",
            "status": "done",
            "testStrategy": "Run tests to verify all join types and metrics are covered.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:08.121Z"
          },
          {
            "id": 4,
            "title": "Implement Edge Case Tests",
            "description": "Implement test cases for edge cases and null handling.",
            "dependencies": [
              3
            ],
            "details": "Write test cases for null handling, empty DataFrames, single row DataFrames, and edge cases like identical strings.",
            "status": "done",
            "testStrategy": "Run tests to ensure all edge cases are handled correctly.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:08.124Z"
          },
          {
            "id": 5,
            "title": "Validate and Optimize Test Suite",
            "description": "Validate test results and optimize performance.",
            "dependencies": [
              4
            ],
            "details": "Validate test results against manual calculations and optimize for performance with large DataFrames.",
            "status": "done",
            "testStrategy": "Ensure all tests pass and performance is acceptable.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:08.126Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:08.126Z"
      },
      {
        "id": "51",
        "title": "Fuzzy Join Documentation",
        "description": "Document fuzzy join functionality with usage examples, algorithm descriptions, and performance guidelines.",
        "details": "Task 51: Fuzzy Join Documentation\n\nPriority: Medium\nPhase: 5 - Basic Fuzzy Join Implementation\nDependencies: Task 50\n\nImplementation:\n- Rust docstrings:\n  - Function-level docs with algorithm descriptions\n  - Parameter documentation\n  - Examples in doc tests\n  - Complexity analysis\n- Python docstrings:\n  - Usage examples\n  - Parameter descriptions\n  - Return value documentation\n  - See Also references to similarity functions\n- User guide section (docs/user-guide/):\n  - When to use fuzzy joins vs exact joins\n  - Choosing similarity metrics:\n    - Levenshtein: General purpose, handles insertions/deletions\n    - Damerau-Levenshtein: Good for typos (transpositions)\n    - Jaro-Winkler: Fast, good for names/short strings\n    - Hamming: Fixed-length strings only\n  - Threshold selection guidelines (0.8-0.9 typical)\n  - Performance considerations\n- Example notebooks:\n  - Entity resolution example\n  - Deduplication workflow\n  - Data cleaning pipeline\n\nCode Location:\n- Rust docs in source files\n- py-polars/polars/dataframe/frame.py (docstrings)\n- polars/docs/source/user-guide/operations/fuzzy_join.md (new file)",
        "testStrategy": "Documentation review, examples run successfully, doc tests pass",
        "status": "done",
        "dependencies": [
          "50"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Document Rust Function-Level Docs",
            "description": "Write function-level documentation for Rust fuzzy join functions.",
            "dependencies": [],
            "details": "Add algorithm descriptions, parameter documentation, and complexity analysis in Rust docstrings.",
            "status": "done",
            "testStrategy": "Review docstrings for completeness and accuracy.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:09.425Z"
          },
          {
            "id": 2,
            "title": "Add Python Docstrings for Fuzzy Join",
            "description": "Create Python docstrings with usage examples and parameter descriptions.",
            "dependencies": [
              1
            ],
            "details": "Include usage examples, parameter descriptions, return value documentation, and 'See Also' references in Python docstrings.",
            "status": "done",
            "testStrategy": "Ensure examples run successfully and docstrings are clear.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:09.431Z"
          },
          {
            "id": 3,
            "title": "Write User Guide Section on Fuzzy Joins",
            "description": "Develop a user guide section explaining fuzzy joins and similarity metrics.",
            "dependencies": [
              2
            ],
            "details": "Cover when to use fuzzy joins, choosing similarity metrics, threshold selection, and performance considerations.",
            "status": "done",
            "testStrategy": "Review for clarity and completeness; ensure guidelines are practical.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:09.435Z"
          },
          {
            "id": 4,
            "title": "Create Example Notebooks for Fuzzy Join",
            "description": "Develop example notebooks demonstrating fuzzy join use cases.",
            "dependencies": [
              3
            ],
            "details": "Include examples for entity resolution, deduplication, and data cleaning workflows.",
            "status": "done",
            "testStrategy": "Run notebooks to verify examples work as intended.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:09.439Z"
          },
          {
            "id": 5,
            "title": "Finalize and Review Documentation",
            "description": "Conduct a final review of all documentation and examples.",
            "dependencies": [
              4
            ],
            "details": "Ensure all documentation is complete, accurate, and examples are functional.",
            "status": "done",
            "testStrategy": "Perform a comprehensive review and run all examples to ensure correctness.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:09.442Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:09.442Z"
      },
      {
        "id": "52",
        "title": "Implement Blocking Strategy",
        "description": "Reduce fuzzy join comparisons using blocking (candidate generation) with pluggable strategies.",
        "details": "Task 52: Implement Blocking Strategy\n\nPriority: High\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 50\n\nImplementation:\n- Create `FuzzyJoinBlocker` trait for pluggable blocking strategies:\n  ```rust\n  pub trait FuzzyJoinBlocker {\n      fn generate_candidates(\n          &self,\n          left: &StringChunked,\n          right: &StringChunked,\n      ) -> Vec<(usize, usize)>;\n  }\n  ```\n- Implement `FirstNCharsBlocker`:\n  - Group strings by first N characters\n  - Only compare within same block\n  - Configurable N (default: 3)\n- Implement `NGramBlocker`:\n  - Generate n-grams for each string (default: trigrams)\n  - Use inverted index: n-gram  row IDs\n  - Only compare pairs sharing at least one n-gram\n- Implement `LengthBlocker`:\n  - Group by string length buckets\n  - Only compare strings within length difference threshold\n- Add `blocking` parameter to `FuzzyJoinArgs`:\n  - None (full scan)\n  - FirstChars(n)\n  - NGram(n)\n  - Length(max_diff)\n  - Combined(Vec<BlockingStrategy>)\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs (new file)",
        "testStrategy": "Verify blocking doesn't miss valid matches (recall = 100%), measure reduction in comparisons",
        "status": "done",
        "dependencies": [
          "50"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create FuzzyJoinBlocker Trait",
            "description": "Define the FuzzyJoinBlocker trait for pluggable blocking strategies.",
            "dependencies": [],
            "details": "Create a Rust trait named FuzzyJoinBlocker with a method generate_candidates that takes two StringChunked references and returns a vector of tuple pairs.",
            "status": "done",
            "testStrategy": "Verify trait compiles and method signature is correct.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:36:09.038Z"
          },
          {
            "id": 2,
            "title": "Implement FirstNCharsBlocker",
            "description": "Develop the FirstNCharsBlocker strategy to group strings by the first N characters.",
            "dependencies": [
              1
            ],
            "details": "Implement a struct that groups strings by their first N characters and compares only within the same group. Make N configurable with a default of 3.",
            "status": "done",
            "testStrategy": "Test with different N values and verify only strings within the same group are compared.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:36:09.605Z"
          },
          {
            "id": 3,
            "title": "Implement NGramBlocker",
            "description": "Create the NGramBlocker strategy using n-grams for candidate generation.",
            "dependencies": [
              1
            ],
            "details": "Implement a struct that generates n-grams for each string and uses an inverted index to find candidate pairs sharing at least one n-gram. Default to trigrams.",
            "status": "done",
            "testStrategy": "Ensure pairs sharing n-grams are correctly identified and compared.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:36:10.081Z"
          },
          {
            "id": 4,
            "title": "Implement LengthBlocker",
            "description": "Develop the LengthBlocker strategy to group strings by length buckets.",
            "dependencies": [
              1
            ],
            "details": "Implement a struct that groups strings into buckets based on length and compares strings within a specified length difference threshold.",
            "status": "done",
            "testStrategy": "Test with various length thresholds to ensure only appropriate pairs are compared.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:36:10.550Z"
          },
          {
            "id": 5,
            "title": "Add Blocking Parameter to FuzzyJoinArgs",
            "description": "Integrate blocking strategies into FuzzyJoinArgs with a new parameter.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Modify FuzzyJoinArgs to include a blocking parameter that can be set to None, FirstChars(n), NGram(n), Length(max_diff), or Combined(Vec<BlockingStrategy>).",
            "status": "done",
            "testStrategy": "Verify parameter integration and ensure all blocking strategies are selectable and functional.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:36:11.016Z"
          }
        ],
        "updatedAt": "2025-12-04T07:36:11.016Z"
      },
      {
        "id": "53",
        "title": "Implement Sorted Neighborhood Method",
        "description": "Sort-based blocking using sliding window to compare nearby strings efficiently.",
        "details": "Task 53: Implement Sorted Neighborhood Method\n\nPriority: Medium\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 52\n\nImplementation:\n- Implement `SortedNeighborhoodBlocker`:\n  - Sort both columns by string value\n  - Use sliding window to compare nearby strings\n  - Window size configurable (default: 10)\n- Algorithm:\n  1. Create sorted index of (string_value, original_idx) for both columns\n  2. Merge sorted lists\n  3. For each string in merged list, compare with W neighbors\n  4. Return candidate pairs\n- More efficient for already-sorted or nearly-sorted data\n- Support multi-pass with different sort keys\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs",
        "testStrategy": "Compare results with full scan (no missed matches), measure speedup on sorted vs random data",
        "status": "done",
        "dependencies": [
          "52"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Sorted Index for Columns",
            "description": "Develop a function to create a sorted index of (string_value, original_idx) for both columns.",
            "dependencies": [],
            "details": "Implement a function that takes two columns and returns a sorted list of tuples containing string values and their original indices.",
            "status": "done",
            "testStrategy": "Verify the sorted index matches expected order for sample data.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:10.694Z"
          },
          {
            "id": 2,
            "title": "Merge Sorted Lists",
            "description": "Implement logic to merge the sorted lists from both columns.",
            "dependencies": [
              1
            ],
            "details": "Use a merge algorithm to combine the sorted lists into a single list, maintaining order.",
            "status": "done",
            "testStrategy": "Ensure merged list maintains order and contains all elements from both lists.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:10.699Z"
          },
          {
            "id": 3,
            "title": "Implement Sliding Window Comparison",
            "description": "Develop the sliding window mechanism to compare each string with its W neighbors.",
            "dependencies": [
              2
            ],
            "details": "Create a function that iterates over the merged list and compares each string with its W nearest neighbors.",
            "status": "done",
            "testStrategy": "Test with different window sizes to ensure correct neighbor comparisons.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:10.705Z"
          },
          {
            "id": 4,
            "title": "Return Candidate Pairs",
            "description": "Create a function to return candidate pairs from the sliding window comparisons.",
            "dependencies": [
              3
            ],
            "details": "Extract and return pairs of indices that meet the comparison criteria from the sliding window results.",
            "status": "done",
            "testStrategy": "Validate that candidate pairs are correctly identified and returned.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:10.710Z"
          },
          {
            "id": 5,
            "title": "Support Multi-Pass with Different Sort Keys",
            "description": "Enhance the implementation to support multiple passes with different sort keys.",
            "dependencies": [
              4
            ],
            "details": "Modify the algorithm to allow re-sorting and re-comparing with different keys for improved accuracy.",
            "status": "done",
            "testStrategy": "Test multi-pass functionality with various sort keys to ensure comprehensive candidate generation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:10.715Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:10.715Z"
      },
      {
        "id": "54",
        "title": "Multi-Column Blocking",
        "description": "Support blocking on multiple columns for more precise candidate generation.",
        "details": "Task 54: Multi-Column Blocking\n\nPriority: Medium\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 52\n\nImplementation:\n- Allow multiple blocking columns:\n  - blocking_on: Vec<(String, BlockingStrategy)>\n- Combine blocking keys:\n  - Example: first letter of name + state code\n  - Concatenate keys: \"J_CA\" for \"John\" in California\n- Support union or intersection of candidate pairs:\n  - Union: candidate if matches ANY blocking key (more recall)\n  - Intersection: candidate if matches ALL blocking keys (more precision)\n- Add to FuzzyJoinArgs:\n  ```rust\n  pub struct MultiColumnBlocking {\n      pub columns: Vec<(String, BlockingStrategy)>,\n      pub mode: BlockingMode, // Union or Intersection\n  }\n  ```\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs",
        "testStrategy": "Multi-column blocking produces correct results, compare precision/recall with single-column",
        "status": "done",
        "dependencies": [
          "52"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Multi-Column Blocking Structure",
            "description": "Create a structure to support multiple blocking columns.",
            "dependencies": [
              52
            ],
            "details": "Add a new struct `MultiColumnBlocking` with fields for columns and mode.",
            "status": "done",
            "testStrategy": "Verify struct fields are correctly defined.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:15.907Z"
          },
          {
            "id": 2,
            "title": "Implement Blocking Key Combination",
            "description": "Develop logic to combine blocking keys from multiple columns.",
            "dependencies": [
              1
            ],
            "details": "Concatenate keys from specified columns, e.g., first letter of name + state code.",
            "status": "done",
            "testStrategy": "Check if keys are combined correctly for given inputs.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:15.913Z"
          },
          {
            "id": 3,
            "title": "Develop Union and Intersection Modes",
            "description": "Support union and intersection modes for candidate generation.",
            "dependencies": [
              2
            ],
            "details": "Implement logic to generate candidates based on union or intersection of keys.",
            "status": "done",
            "testStrategy": "Test candidate generation for both modes with sample data.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:15.919Z"
          },
          {
            "id": 4,
            "title": "Integrate Multi-Column Blocking into FuzzyJoinArgs",
            "description": "Add the new blocking structure to FuzzyJoinArgs.",
            "dependencies": [
              3
            ],
            "details": "Modify FuzzyJoinArgs to include `MultiColumnBlocking` as an option.",
            "status": "done",
            "testStrategy": "Ensure FuzzyJoinArgs accepts and processes the new structure.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:15.923Z"
          },
          {
            "id": 5,
            "title": "Update Fuzzy Blocking Module",
            "description": "Modify the fuzzy blocking module to utilize multi-column blocking.",
            "dependencies": [
              4
            ],
            "details": "Update `fuzzy_blocking.rs` to handle multi-column blocking logic.",
            "status": "done",
            "testStrategy": "Run integration tests to verify multi-column blocking functionality.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:15.927Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:15.927Z"
      },
      {
        "id": "55",
        "title": "Parallel Fuzzy Join with Rayon",
        "description": "Parallelize fuzzy join across CPU cores using Rayon for multi-threaded execution.",
        "details": "Task 55: Parallel Fuzzy Join with Rayon\n\nPriority: High\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 52\n\nImplementation:\n- Partition left DataFrame into chunks\n- Process chunks in parallel using Rayon:\n  ```rust\n  use rayon::prelude::*;\n  \n  left_chunks.par_iter()\n      .map(|chunk| compute_matches(chunk, right, &args))\n      .collect::<Vec<_>>()\n  ```\n- Each thread computes matches for its chunk\n- Merge results while maintaining row order\n- Thread-local similarity computation (no contention)\n- Configurable parallelism:\n  - num_threads parameter (default: num_cpus)\n  - Minimum chunk size to avoid overhead\n- Handle thread-local buffers for similarity computation\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy.rs",
        "testStrategy": "Results match single-threaded, measure multi-core speedup (target: near-linear scaling)",
        "status": "done",
        "dependencies": [
          "52"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Partition DataFrame into Chunks",
            "description": "Divide the left DataFrame into manageable chunks for parallel processing.",
            "dependencies": [],
            "details": "Implement a function to partition the left DataFrame into chunks based on a configurable chunk size. Ensure that each chunk is independent and can be processed in parallel.",
            "status": "done",
            "testStrategy": "Verify chunk sizes and independence.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:37:11.514Z"
          },
          {
            "id": 2,
            "title": "Implement Parallel Processing with Rayon",
            "description": "Use Rayon to process DataFrame chunks in parallel.",
            "dependencies": [
              1
            ],
            "details": "Utilize Rayons parallel iterator to process each chunk concurrently. Implement the logic to compute matches for each chunk using the provided compute_matches function.",
            "status": "done",
            "testStrategy": "Compare results with single-threaded execution for correctness.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:37:11.687Z"
          },
          {
            "id": 3,
            "title": "Merge Results Maintaining Row Order",
            "description": "Combine results from all threads while preserving the original row order.",
            "dependencies": [
              2
            ],
            "details": "Develop a mechanism to merge the results from each thread. Ensure that the final output maintains the order of rows as in the original DataFrame.",
            "status": "done",
            "testStrategy": "Check the order of rows in the merged results.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:37:11.881Z"
          },
          {
            "id": 4,
            "title": "Implement Thread-Local Similarity Computation",
            "description": "Ensure similarity computations are thread-local to avoid contention.",
            "dependencies": [
              2
            ],
            "details": "Modify the similarity computation to use thread-local storage, preventing data races and ensuring efficient parallel execution.",
            "status": "done",
            "testStrategy": "Test for thread safety and performance improvements.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:37:12.088Z"
          },
          {
            "id": 5,
            "title": "Configure Parallelism Parameters",
            "description": "Add configuration options for parallelism, including num_threads and minimum chunk size.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement configuration options to control the number of threads and the minimum chunk size. Default to the number of available CPU cores for num_threads.",
            "status": "done",
            "testStrategy": "Test different configurations for performance and correctness.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T07:37:12.325Z"
          }
        ],
        "updatedAt": "2025-12-04T07:37:12.325Z"
      },
      {
        "id": "56",
        "title": "Batch Similarity Computation",
        "description": "Compute similarities in batches for better cache utilization and reduced overhead.",
        "details": "Task 56: Batch Similarity Computation\n\nPriority: High\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 55\n\nImplementation:\n- Process blocks of (left_rows  right_rows) at a time\n- Batch processing strategy:\n  1. Load batch of left strings into cache-friendly structure\n  2. Load batch of right strings\n  3. Compute all pairwise similarities in batch\n  4. Filter and collect results\n- Optimize memory access patterns:\n  - Sequential access within batches\n  - Minimize cache misses\n- Reuse buffers across batch computations:\n  - Pre-allocate similarity result array\n  - Reuse DP matrices for Levenshtein\n- Tune batch size for cache efficiency:\n  - Default: 1024 rows per batch\n  - Auto-tune based on string lengths\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy.rs",
        "testStrategy": "Benchmark batch vs row-by-row, measure cache efficiency (perf stat)",
        "status": "done",
        "dependencies": [
          "55"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Load Left Strings into Cache-Friendly Structure",
            "description": "Load a batch of left strings into a cache-friendly structure to optimize memory access.",
            "dependencies": [],
            "details": "Implement a function to load left strings into a structure that enhances cache utilization. Ensure sequential memory access to minimize cache misses.",
            "status": "done",
            "testStrategy": "Verify memory access patterns and cache utilization.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:17.146Z"
          },
          {
            "id": 2,
            "title": "Load Right Strings for Batch Processing",
            "description": "Load a batch of right strings for processing alongside the left strings.",
            "dependencies": [
              1
            ],
            "details": "Develop a method to load right strings in a manner that complements the left strings' structure. Maintain efficient memory access patterns.",
            "status": "done",
            "testStrategy": "Check for efficient memory access and alignment with left strings.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:17.149Z"
          },
          {
            "id": 3,
            "title": "Compute Pairwise Similarities in Batch",
            "description": "Compute all pairwise similarities between loaded left and right strings in the batch.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement the similarity computation using pre-allocated buffers and reuse DP matrices for Levenshtein calculations. Focus on minimizing computational overhead.",
            "status": "done",
            "testStrategy": "Benchmark similarity computation against non-batch processing.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:17.152Z"
          },
          {
            "id": 4,
            "title": "Filter and Collect Similarity Results",
            "description": "Filter the computed similarities and collect the results for further processing.",
            "dependencies": [
              3
            ],
            "details": "Design a filtering mechanism to select relevant similarity results based on predefined criteria. Ensure efficient data collection.",
            "status": "done",
            "testStrategy": "Validate filtering logic and result accuracy.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:17.155Z"
          },
          {
            "id": 5,
            "title": "Tune Batch Size for Optimal Cache Efficiency",
            "description": "Adjust the batch size dynamically to achieve optimal cache efficiency based on string lengths.",
            "dependencies": [
              4
            ],
            "details": "Implement an auto-tuning mechanism to adjust batch sizes. Use performance metrics to guide adjustments for cache efficiency.",
            "status": "done",
            "testStrategy": "Measure performance improvements with different batch sizes.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:17.158Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:17.158Z"
      },
      {
        "id": "57",
        "title": "Implement Similarity Index",
        "description": "Pre-compute index structure for faster similarity lookups using inverted n-gram index.",
        "details": "Task 57: Implement Similarity Index\n\nPriority: Medium\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 52\n\nImplementation:\n- Create `SimilarityIndex` struct:\n  ```rust\n  pub struct SimilarityIndex {\n      ngram_index: HashMap<String, Vec<usize>>,\n      strings: Vec<String>,\n      ngram_size: usize,\n  }\n  ```\n- Build inverted index: n-gram  row IDs\n- Index building:\n  1. For each string, generate all n-grams\n  2. Add row ID to each n-gram's posting list\n- Query algorithm:\n  1. Generate n-grams for query string\n  2. Look up candidate row IDs from posting lists\n  3. Score only candidates\n- Support incremental index updates:\n  - add_string()\n  - remove_string()\n- Persist index for reuse:\n  - serialize/deserialize to file\n  - Cache index for repeated joins on same column\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy_index.rs (new file)",
        "testStrategy": "Index produces same results as full scan, measure query speedup",
        "status": "done",
        "dependencies": [
          "52"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Define SimilarityIndex Struct",
            "description": "Create the SimilarityIndex struct with necessary fields.",
            "dependencies": [],
            "details": "Define the SimilarityIndex struct in Rust with fields: ngram_index, strings, and ngram_size.",
            "status": "done",
            "testStrategy": "Compile and verify struct definition.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:09.529Z"
          },
          {
            "id": 2,
            "title": "Implement N-gram Inverted Index",
            "description": "Build an inverted index mapping n-grams to row IDs.",
            "dependencies": [
              1
            ],
            "details": "For each string, generate n-grams and map them to row IDs in the ngram_index.",
            "status": "done",
            "testStrategy": "Verify n-gram mappings with test strings.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:09.533Z"
          },
          {
            "id": 3,
            "title": "Develop Query Algorithm",
            "description": "Create an algorithm to query the inverted index for similarity lookups.",
            "dependencies": [
              2
            ],
            "details": "Generate n-grams for a query string, retrieve candidate row IDs, and score candidates.",
            "status": "done",
            "testStrategy": "Test query results against expected outputs.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:09.536Z"
          },
          {
            "id": 4,
            "title": "Add Incremental Update Support",
            "description": "Implement methods to add and remove strings from the index.",
            "dependencies": [
              2
            ],
            "details": "Create add_string() and remove_string() methods to update the index incrementally.",
            "status": "done",
            "testStrategy": "Test adding and removing strings with index consistency checks.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:09.539Z"
          },
          {
            "id": 5,
            "title": "Implement Index Persistence",
            "description": "Enable serialization and deserialization of the index for reuse.",
            "dependencies": [
              4
            ],
            "details": "Implement methods to serialize the index to a file and deserialize it back.",
            "status": "done",
            "testStrategy": "Test persistence by saving and loading the index, ensuring data integrity.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:09.542Z"
          }
        ],
        "updatedAt": "2025-12-04T22:01:09.542Z"
      },
      {
        "id": "58",
        "title": "BK-Tree for Edit Distance",
        "description": "Implement BK-tree data structure for efficient edit distance queries with pruning.",
        "details": "Task 58: BK-Tree for Edit Distance\n\nPriority: Low\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 57\n\nImplementation:\n- Build BK-tree from right DataFrame strings:\n  ```rust\n  pub struct BKTree {\n      root: Option<BKNode>,\n      distance_fn: fn(&str, &str) -> usize,\n  }\n  \n  struct BKNode {\n      value: String,\n      row_id: usize,\n      children: HashMap<usize, BKNode>, // distance -> child\n  }\n  ```\n- Query tree with threshold for candidates:\n  - Use triangle inequality to prune search\n  - If |d(query, node) - d(node, child)| > threshold, skip subtree\n- Only applicable to Levenshtein/Damerau-Levenshtein\n- Trade-off: build time vs query time\n  - Build: O(n log n) average\n  - Query: O(log n) to O(n) depending on threshold\n- Cache BK-tree for repeated queries\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/bk_tree.rs (new file)",
        "testStrategy": "BK-tree produces correct candidates, measure query speedup vs linear scan",
        "status": "done",
        "dependencies": [
          "57"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Define BKTree and BKNode Structures",
            "description": "Create the data structures for BKTree and BKNode in Rust.",
            "dependencies": [],
            "details": "Implement the BKTree and BKNode structs in Rust, including fields for root, distance function, and children.",
            "status": "done",
            "testStrategy": "Verify structure creation with sample data.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.780Z"
          },
          {
            "id": 2,
            "title": "Implement BKTree Build Function",
            "description": "Develop the function to build a BK-tree from a list of strings.",
            "dependencies": [
              1
            ],
            "details": "Write a function to insert strings into the BK-tree, calculating distances and organizing nodes accordingly.",
            "status": "done",
            "testStrategy": "Test tree construction with a small dataset and verify node organization.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.784Z"
          },
          {
            "id": 3,
            "title": "Develop Query Function with Pruning",
            "description": "Create a function to query the BK-tree using a threshold for edit distance.",
            "dependencies": [
              2
            ],
            "details": "Implement the query function using triangle inequality to prune unnecessary branches.",
            "status": "done",
            "testStrategy": "Test query function with various thresholds and compare results to expected outcomes.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.788Z"
          },
          {
            "id": 4,
            "title": "Optimize BKTree for Levenshtein Distance",
            "description": "Ensure the BK-tree is optimized for Levenshtein and Damerau-Levenshtein distances.",
            "dependencies": [
              3
            ],
            "details": "Adjust the distance function to support both Levenshtein and Damerau-Levenshtein calculations.",
            "status": "done",
            "testStrategy": "Validate distance calculations against known values.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.791Z"
          },
          {
            "id": 5,
            "title": "Implement Caching Mechanism for BKTree",
            "description": "Add caching to the BK-tree to improve performance for repeated queries.",
            "dependencies": [
              4
            ],
            "details": "Develop a caching strategy to store and reuse BK-tree structures for repeated queries.",
            "status": "done",
            "testStrategy": "Measure performance improvements with caching enabled.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.794Z"
          }
        ],
        "updatedAt": "2025-12-04T22:01:33.794Z"
      },
      {
        "id": "59",
        "title": "Early Termination in Batch Joins",
        "description": "Stop computing similarity once threshold is determined to save computation.",
        "details": "Task 59: Early Termination in Batch Joins\n\nPriority: High\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 56\n\nImplementation:\n- Use bounded distance algorithms from Phase 2:\n  - levenshtein_distance_bounded()\n  - Pass threshold to similarity functions\n- For `keep=\"best\"`, track current best and prune:\n  - If current_best > 0.9, skip strings with length diff > allowed\n  - Maintain running best match per left row\n  - Skip comparison if upper bound < current best\n- For `keep=\"first\"`, stop after first match:\n  - Return immediately when threshold is met\n  - Process in order of likely matches first\n- Skip remaining comparisons when threshold can't be met:\n  - Use length difference as lower bound on edit distance\n  - Early exit in Levenshtein when distance exceeds bound\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy.rs",
        "testStrategy": "Results match full computation, measure comparisons saved",
        "status": "done",
        "dependencies": [
          "56"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Bounded Distance Algorithms",
            "description": "Implement bounded distance algorithms for early termination in batch joins.",
            "dependencies": [],
            "details": "Use levenshtein_distance_bounded() and pass threshold to similarity functions.",
            "status": "done",
            "testStrategy": "Verify bounded algorithms stop computation early.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.799Z"
          },
          {
            "id": 2,
            "title": "Implement Early Termination for 'keep=\"best\"'",
            "description": "Optimize 'keep=\"best\"' by tracking current best match and pruning unnecessary comparisons.",
            "dependencies": [
              1
            ],
            "details": "Track current best match per left row. Skip comparisons if upper bound < current best.",
            "status": "done",
            "testStrategy": "Ensure comparisons are skipped when current best is above threshold.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.802Z"
          },
          {
            "id": 3,
            "title": "Implement Early Termination for 'keep=\"first\"'",
            "description": "Optimize 'keep=\"first\"' by stopping after the first match that meets the threshold.",
            "dependencies": [
              1
            ],
            "details": "Return immediately when threshold is met. Process in order of likely matches first.",
            "status": "done",
            "testStrategy": "Check that processing stops after the first valid match.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.804Z"
          },
          {
            "id": 4,
            "title": "Optimize Length Difference Checks",
            "description": "Use length difference as a lower bound on edit distance to skip unnecessary comparisons.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement checks to skip comparisons when length difference exceeds allowed bounds.",
            "status": "done",
            "testStrategy": "Verify that comparisons are skipped based on length difference.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.806Z"
          },
          {
            "id": 5,
            "title": "Code Integration and Testing",
            "description": "Integrate all optimizations into the existing codebase and perform comprehensive testing.",
            "dependencies": [
              4
            ],
            "details": "Integrate changes into polars/crates/polars-ops/src/frame/join/fuzzy.rs and test thoroughly.",
            "status": "done",
            "testStrategy": "Results should match full computation; measure comparisons saved.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.808Z"
          }
        ],
        "updatedAt": "2025-12-04T22:01:33.808Z"
      },
      {
        "id": "60",
        "title": "Adaptive Threshold Estimation",
        "description": "Automatically suggest optimal threshold based on data distribution analysis.",
        "details": "Task 60: Adaptive Threshold Estimation\n\nPriority: Low\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 59\n\nImplementation:\n- Sample pairs from both DataFrames:\n  - Random sample of N pairs (default: 1000)\n  - Stratified sampling for better coverage\n- Compute similarity distribution:\n  - Calculate similarity for all sampled pairs\n  - Build histogram of similarity scores\n- Suggest threshold based on distribution:\n  - Mode detection for bimodal distributions\n  - Percentile-based (e.g., 90th percentile of non-matches)\n  - Elbow detection in cumulative distribution\n- Provide confidence metrics:\n  - Expected precision/recall at suggested threshold\n  - Distribution statistics\n- Add `estimate_threshold()` utility function:\n  ```python\n  threshold, stats = pl.estimate_fuzzy_threshold(\n      left_df, right_df,\n      left_on=\"name\", right_on=\"company\",\n      similarity=\"jaro_winkler\",\n      sample_size=1000,\n  )\n  ```\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/threshold_estimation.rs (new file)\n- py-polars/polars/functions/fuzzy.py (Python API)",
        "testStrategy": "Threshold suggestions produce reasonable match rates, validate on known datasets",
        "status": "done",
        "dependencies": [
          "59"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Sampling Function",
            "description": "Create a function to sample pairs from DataFrames.",
            "dependencies": [],
            "details": "Develop a function to randomly sample 1000 pairs from the DataFrames, ensuring stratified sampling for better coverage.",
            "status": "done",
            "testStrategy": "Verify sample size and distribution coverage.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.813Z"
          },
          {
            "id": 2,
            "title": "Compute Similarity Distribution",
            "description": "Calculate similarity scores for sampled pairs.",
            "dependencies": [
              1
            ],
            "details": "Use the Jaro-Winkler similarity metric to compute scores for all sampled pairs and build a histogram of these scores.",
            "status": "done",
            "testStrategy": "Check histogram accuracy against known distributions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.814Z"
          },
          {
            "id": 3,
            "title": "Develop Threshold Suggestion Logic",
            "description": "Create logic to suggest thresholds based on distribution.",
            "dependencies": [
              2
            ],
            "details": "Implement mode detection for bimodal distributions, percentile-based thresholds, and elbow detection in cumulative distribution.",
            "status": "done",
            "testStrategy": "Validate threshold suggestions with test datasets.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.817Z"
          },
          {
            "id": 4,
            "title": "Calculate Confidence Metrics",
            "description": "Provide precision/recall metrics at suggested threshold.",
            "dependencies": [
              3
            ],
            "details": "Calculate expected precision and recall at the suggested threshold and provide distribution statistics.",
            "status": "done",
            "testStrategy": "Compare metrics against benchmark datasets.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.819Z"
          },
          {
            "id": 5,
            "title": "Add `estimate_threshold()` Utility Function",
            "description": "Implement the `estimate_threshold()` function in the codebase.",
            "dependencies": [
              4
            ],
            "details": "Add the function to the specified Rust and Python files, ensuring it integrates with existing APIs.",
            "status": "done",
            "testStrategy": "Test function integration and output correctness with sample data.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.821Z"
          }
        ],
        "updatedAt": "2025-12-04T22:01:33.821Z"
      },
      {
        "id": "61",
        "title": "Python Fuzzy Join Optimizations",
        "description": "Expose all optimization parameters to Python API including blocking, parallelism, and indexing.",
        "details": "Task 61: Python Fuzzy Join Optimizations\n\nPriority: Medium\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Tasks 52-60\n\nImplementation:\n- Add blocking parameters to Python API:\n  ```python\n  df.fuzzy_join(\n      other,\n      left_on=\"name\",\n      right_on=\"company\",\n      similarity=\"jaro_winkler\",\n      threshold=0.85,\n      # Blocking options\n      blocking=\"first_chars\",  # or \"ngram\", \"length\", \"sorted_neighborhood\"\n      blocking_chars=3,\n      blocking_ngram_size=3,\n      # Parallel options\n      parallel=True,\n      n_threads=4,\n      # Batch options\n      batch_size=1024,\n  )\n  ```\n- Add index building/loading methods:\n  ```python\n  index = df[\"name\"].str.build_similarity_index(ngram_size=3)\n  index.save(\"name_index.bin\")\n  index = pl.load_similarity_index(\"name_index.bin\")\n  result = df.fuzzy_join(other, index=index, ...)\n  ```\n- Performance hints in docstrings\n- Type hints for all new parameters\n\nCode Location:\n- py-polars/polars/dataframe/frame.py\n- py-polars/polars/lazyframe/frame.py\n- py-polars/polars/series/string.py (index methods)",
        "testStrategy": "Python tests for all optimization options, verify parameters are passed correctly",
        "status": "done",
        "dependencies": [
          "52",
          "53",
          "54",
          "55",
          "56",
          "57",
          "58",
          "59",
          "60"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Blocking Parameters in Python API",
            "description": "Add blocking parameters to the Python API for fuzzy joins.",
            "dependencies": [],
            "details": "Modify the `fuzzy_join` method to include blocking options such as `blocking`, `blocking_chars`, and `blocking_ngram_size`. Update the code in `py-polars/polars/dataframe/frame.py`.",
            "status": "done",
            "testStrategy": "Write unit tests to verify blocking parameters are correctly passed and applied.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.825Z"
          },
          {
            "id": 2,
            "title": "Add Parallelism Options to Python API",
            "description": "Incorporate parallelism options into the Python API for fuzzy joins.",
            "dependencies": [
              1
            ],
            "details": "Enhance the `fuzzy_join` method to support parallel execution with options like `parallel` and `n_threads`. Implement changes in `py-polars/polars/dataframe/frame.py`.",
            "status": "done",
            "testStrategy": "Create tests to ensure parallel execution parameters are functional and improve performance.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.827Z"
          },
          {
            "id": 3,
            "title": "Develop Index Building and Loading Methods",
            "description": "Implement methods for building and loading similarity indexes in Python.",
            "dependencies": [
              1
            ],
            "details": "Add methods to `py-polars/polars/series/string.py` for building and saving similarity indexes. Implement loading functionality as well.",
            "status": "done",
            "testStrategy": "Test index creation and loading with various configurations to ensure accuracy and performance.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.829Z"
          },
          {
            "id": 4,
            "title": "Update Docstrings with Performance Hints",
            "description": "Enhance docstrings with performance hints for new parameters.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Revise docstrings in `py-polars/polars/dataframe/frame.py` to include performance tips related to blocking, parallelism, and indexing.",
            "status": "done",
            "testStrategy": "Review docstrings for clarity and completeness, ensuring all new parameters are documented.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.831Z"
          },
          {
            "id": 5,
            "title": "Add Type Hints for New Parameters",
            "description": "Incorporate type hints for all new parameters in the Python API.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Update function signatures in `py-polars/polars/dataframe/frame.py` to include type hints for blocking, parallelism, and indexing parameters.",
            "status": "done",
            "testStrategy": "Run static type checks to verify correctness of type hints.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.833Z"
          }
        ],
        "updatedAt": "2025-12-04T22:01:33.833Z"
      },
      {
        "id": "62",
        "title": "Fuzzy Join Performance Benchmarks",
        "description": "Comprehensive benchmarking suite comparing fuzzy join performance across configurations and against alternatives.",
        "details": "Task 62: Fuzzy Join Performance Benchmarks\n\nPriority: Medium\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 61\n\nImplementation:\n- Create benchmark datasets:\n  - Synthetic: 1K, 10K, 100K, 1M rows with controlled similarity distribution\n  - Real-world: Company names, addresses, product names\n  - Various string lengths: short (5-10), medium (20-50), long (100+)\n- Benchmark all similarity metrics:\n  - Levenshtein, Damerau-Levenshtein, Jaro-Winkler, Hamming\n  - Time per 1K joins\n- Benchmark blocking strategies:\n  - No blocking (baseline)\n  - First N chars\n  - N-gram\n  - Length\n  - Sorted neighborhood\n  - Measure: comparisons reduced, time saved, recall maintained\n- Benchmark parallel scaling:\n  - 1, 2, 4, 8, 16 threads\n  - Measure speedup and efficiency\n- Compare with alternatives:\n  - Python recordlinkage library\n  - dedupe library\n  - fuzzywuzzy/rapidfuzz standalone\n- Document performance characteristics:\n  - Best configurations per use case\n  - Memory usage\n  - Scaling behavior\n\nCode Location:\n- benchmark_fuzzy_join.py (new file in project root)\n- polars/crates/polars-ops/benches/fuzzy_join.rs (Rust benchmarks)",
        "testStrategy": "Benchmarks run successfully, results documented and reproducible",
        "status": "done",
        "dependencies": [
          "61"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Benchmark Datasets",
            "description": "Develop synthetic and real-world datasets for benchmarking.",
            "dependencies": [],
            "details": "Generate synthetic datasets with 1K, 10K, 100K, and 1M rows. Include real-world datasets like company names and addresses. Ensure various string lengths are represented.",
            "status": "done",
            "testStrategy": "Verify dataset sizes and distributions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.837Z"
          },
          {
            "id": 2,
            "title": "Benchmark Similarity Metrics",
            "description": "Evaluate performance of different similarity metrics.",
            "dependencies": [
              1
            ],
            "details": "Implement benchmarks for Levenshtein, Damerau-Levenshtein, Jaro-Winkler, and Hamming metrics. Measure time per 1K joins.",
            "status": "done",
            "testStrategy": "Compare results with expected performance baselines.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.839Z"
          },
          {
            "id": 3,
            "title": "Benchmark Blocking Strategies",
            "description": "Assess different blocking strategies for performance.",
            "dependencies": [
              1
            ],
            "details": "Test no blocking, first N chars, N-gram, length, and sorted neighborhood strategies. Measure comparisons reduced, time saved, and recall maintained.",
            "status": "done",
            "testStrategy": "Ensure recall and time savings are documented.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.841Z"
          },
          {
            "id": 4,
            "title": "Benchmark Parallel Scaling",
            "description": "Evaluate performance scaling with multiple threads.",
            "dependencies": [
              2,
              3
            ],
            "details": "Test with 1, 2, 4, 8, and 16 threads. Measure speedup and efficiency.",
            "status": "done",
            "testStrategy": "Verify linear scaling and efficiency metrics.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.843Z"
          },
          {
            "id": 5,
            "title": "Compare with Alternative Libraries",
            "description": "Benchmark against existing libraries for fuzzy joins.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Compare performance with Python recordlinkage, dedupe, and fuzzywuzzy/rapidfuzz libraries. Document performance characteristics.",
            "status": "done",
            "testStrategy": "Ensure results are reproducible and documented.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.846Z"
          }
        ],
        "updatedAt": "2025-12-04T22:01:33.846Z"
      },
      {
        "id": "63",
        "title": "Fuzzy Join Advanced Documentation",
        "description": "Performance tuning guide and advanced usage documentation for fuzzy joins.",
        "details": "Task 63: Fuzzy Join Advanced Documentation\n\nPriority: Medium\nPhase: 6 - Optimized Fuzzy Join Implementation\nDependencies: Task 62\n\nImplementation:\n- Performance tuning guide:\n  - When to use blocking:\n    - Dataset size > 10K rows\n    - N*M comparisons > 1M\n  - Choosing blocking strategy:\n    - First chars: Fast, good for sorted data\n    - N-gram: Best recall, moderate speed\n    - Length: Simple, effective for variable-length strings\n    - Sorted neighborhood: Best for nearly-sorted data\n  - Parallel execution guidelines:\n    - Enable for datasets > 10K rows\n    - Diminishing returns beyond 8 threads\n  - Memory considerations:\n    - Index memory usage\n    - Batch size tuning\n- Advanced examples:\n  - Large-scale entity resolution (1M+ records)\n  - Incremental matching (new records vs existing)\n  - Multi-pass deduplication\n  - Combining with exact joins\n- API reference for all parameters:\n  - Complete parameter documentation\n  - Default values and their rationale\n  - Performance implications\n- Troubleshooting guide:\n  - Common issues and solutions\n  - Debugging slow joins\n\nCode Location:\n- polars/docs/source/user-guide/operations/fuzzy_join_advanced.md (new file)\n- polars/docs/source/user-guide/operations/fuzzy_join_performance.md (new file)",
        "testStrategy": "Documentation review, examples verified to run correctly",
        "status": "done",
        "dependencies": [
          "62"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Performance Tuning Guide",
            "description": "Develop a comprehensive guide for performance tuning of fuzzy joins.",
            "dependencies": [],
            "details": "Include sections on blocking strategies, parallel execution, and memory considerations.",
            "status": "done",
            "testStrategy": "Review for completeness and accuracy.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.850Z"
          },
          {
            "id": 2,
            "title": "Document Advanced Usage Examples",
            "description": "Write documentation for advanced usage scenarios of fuzzy joins.",
            "dependencies": [
              1
            ],
            "details": "Cover large-scale entity resolution, incremental matching, and multi-pass deduplication.",
            "status": "done",
            "testStrategy": "Ensure examples are correct and executable.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.852Z"
          },
          {
            "id": 3,
            "title": "Develop API Reference Documentation",
            "description": "Create detailed API reference for all fuzzy join parameters.",
            "dependencies": [
              1
            ],
            "details": "Include parameter documentation, default values, and performance implications.",
            "status": "done",
            "testStrategy": "Verify completeness and clarity of parameter descriptions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.853Z"
          },
          {
            "id": 4,
            "title": "Write Troubleshooting Guide",
            "description": "Develop a guide for troubleshooting common issues in fuzzy joins.",
            "dependencies": [
              1
            ],
            "details": "Include common problems, solutions, and debugging tips for slow joins.",
            "status": "done",
            "testStrategy": "Review for clarity and usefulness of solutions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.855Z"
          },
          {
            "id": 5,
            "title": "Integrate Documentation into User Guide",
            "description": "Add the new documentation sections to the user guide.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Integrate the performance guide, advanced examples, API reference, and troubleshooting guide into the user guide.",
            "status": "done",
            "testStrategy": "Ensure all sections are correctly linked and formatted.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:01:33.858Z"
          }
        ],
        "updatedAt": "2025-12-04T22:01:33.858Z"
      },
      {
        "id": "64",
        "title": "LSH (Locality Sensitive Hashing) Blocking Strategy",
        "description": "Implement Locality Sensitive Hashing for approximate nearest neighbor blocking to achieve sub-linear candidate generation.",
        "details": "Create `LSHBlocker` struct implementing `FuzzyJoinBlocker` trait. Implement MinHash LSH for Jaccard similarity estimation by generating shingles (character n-grams) from strings, applying multiple hash functions to create a signature, banding the signature into b bands of r rows, and hashing each band to a bucket. Implement SimHash LSH for cosine/angular similarity by generating character-level feature vectors and applying random hyperplane hashing. Configurable parameters include `num_hashes`, `num_bands`, and `shingle_size`. Add `BlockingStrategy::LSH { num_hashes, num_bands, shingle_size }` variant to `args.rs`. Support both MinHash and SimHash variants. Tune parameters for target similarity threshold using the formula: Probability = 1 - (1 - s^r)^b. Code location: `polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs` and `args.rs`.",
        "testStrategy": "Verify LSH produces candidates with high recall (>95%), measure precision/recall tradeoff at different parameter settings, and benchmark against other blocking strategies.",
        "status": "done",
        "dependencies": [
          "52"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement MinHash Signature Generation",
            "description": "Develop MinHash LSH for Jaccard similarity estimation by generating shingles and applying hash functions.",
            "dependencies": [],
            "details": "Create shingles from input strings, apply multiple hash functions to generate MinHash signatures.",
            "status": "done",
            "testStrategy": "Verify signature generation with known Jaccard similarity values.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:08:05.252Z"
          },
          {
            "id": 2,
            "title": "Implement SimHash with Hyperplane Hashing",
            "description": "Develop SimHash LSH for cosine similarity using character-level feature vectors and hyperplane hashing.",
            "dependencies": [
              1
            ],
            "details": "Generate feature vectors from input strings and apply random hyperplane hashing to create SimHash signatures.",
            "status": "done",
            "testStrategy": "Compare SimHash results with expected cosine similarity outcomes.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:08:05.256Z"
          },
          {
            "id": 3,
            "title": "Develop Banding and Bucketing for Candidate Generation",
            "description": "Implement banding of signatures into bands and hash each band to a bucket for candidate generation.",
            "dependencies": [
              1,
              2
            ],
            "details": "Divide signatures into b bands of r rows, hash each band to a bucket to generate candidates.",
            "status": "done",
            "testStrategy": "Ensure candidate generation aligns with theoretical probability calculations.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:08:05.259Z"
          },
          {
            "id": 4,
            "title": "Tune Parameters and Calculate Probability",
            "description": "Adjust parameters for target similarity threshold and calculate probability of candidate generation.",
            "dependencies": [
              3
            ],
            "details": "Use the formula Probability = 1 - (1 - s^r)^b to tune parameters like num_hashes, num_bands, and shingle_size.",
            "status": "done",
            "testStrategy": "Validate parameter tuning with precision/recall metrics.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:08:05.262Z"
          },
          {
            "id": 5,
            "title": "Integrate with BlockingStrategy Enum and FuzzyJoinBlocker Trait",
            "description": "Add LSH as a variant to BlockingStrategy and implement the FuzzyJoinBlocker trait.",
            "dependencies": [
              4
            ],
            "details": "Modify args.rs to include LSH variant and implement necessary methods in FuzzyJoinBlocker trait.",
            "status": "done",
            "testStrategy": "Check integration by ensuring LSH can be selected and used as a blocking strategy.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:08:05.265Z"
          },
          {
            "id": 6,
            "title": "Testing and Benchmarking",
            "description": "Conduct testing and benchmarking to ensure high recall and performance of LSH blocking strategy.",
            "dependencies": [
              5
            ],
            "details": "Perform tests to verify recall >95% and benchmark against other strategies for performance.",
            "status": "done",
            "testStrategy": "Measure precision/recall tradeoff and compare performance with existing strategies.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:09:37.171Z"
          }
        ],
        "updatedAt": "2025-12-04T22:09:37.171Z"
      },
      {
        "id": "65",
        "title": "Memory-Efficient Batch Processing for Large Datasets",
        "description": "Implement streaming batch processing to handle datasets larger than memory with predictable memory usage.",
        "details": "Create the `BatchedFuzzyJoin` struct for memory-efficient processing with parameters like `batch_size` (default: 10000), `memory_limit_mb` (default: 1024), and `streaming_mode`. Implement a chunked processing pipeline: split the left DataFrame into batches, build a temporary index for the right DataFrame for each batch, compute matches, yield results, and release memory. Merge results while maintaining order. Implement memory-aware batch sizing by estimating memory per row based on string lengths and dynamically adjusting batch size to stay within limits. Enable streaming output mode by returning an iterator instead of collecting all results, allowing processing of unbounded data streams. Add a Python API with parameters for `batch_size`, `memory_limit_mb`, and `streaming`. Code will be located in `polars/crates/polars-ops/src/frame/join/fuzzy.rs` and a new file `fuzzy_batch.rs`.",
        "testStrategy": "Process a 10M row dataset with a 2GB memory limit. Verify that results match non-batched processing and measure memory usage to ensure it stays within limits.",
        "status": "done",
        "dependencies": [
          "56",
          "45"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement BatchedFuzzyJoin Struct and Chunked Processing Pipeline",
            "description": "Create the BatchedFuzzyJoin struct and implement the chunked processing pipeline.",
            "dependencies": [],
            "details": "Define the BatchedFuzzyJoin struct with parameters like batch_size, memory_limit_mb, and streaming_mode. Implement the pipeline to split the left DataFrame into batches, build a temporary index for the right DataFrame, compute matches, yield results, and release memory.",
            "status": "done",
            "testStrategy": "Test with small batches to ensure correct splitting and merging.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:13:43.781Z"
          },
          {
            "id": 2,
            "title": "Develop Memory-Aware Batch Sizing",
            "description": "Implement dynamic adjustment of batch size based on memory usage.",
            "dependencies": [
              1
            ],
            "details": "Estimate memory usage per row by analyzing string lengths. Adjust batch size dynamically to ensure memory usage stays within the specified limit.",
            "status": "done",
            "testStrategy": "Simulate varying memory conditions and verify batch size adjustments.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:13:43.788Z"
          },
          {
            "id": 3,
            "title": "Implement Streaming Output Mode with Iterator",
            "description": "Enable streaming output mode by returning an iterator.",
            "dependencies": [
              1
            ],
            "details": "Modify the processing pipeline to return an iterator instead of collecting all results. Ensure compatibility with unbounded data streams.",
            "status": "done",
            "testStrategy": "Test with continuous data streams to verify iterator functionality.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:13:43.795Z"
          },
          {
            "id": 4,
            "title": "Integrate Python API with Batch Parameters",
            "description": "Add a Python API to expose batch processing parameters.",
            "dependencies": [
              1
            ],
            "details": "Create a Python interface allowing users to set batch_size, memory_limit_mb, and streaming_mode. Ensure seamless integration with existing Python bindings.",
            "status": "done",
            "testStrategy": "Test API with various parameter settings to ensure correct behavior.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:15:09.940Z"
          },
          {
            "id": 5,
            "title": "Test with Large Datasets and Monitor Memory Usage",
            "description": "Conduct testing with large datasets to ensure memory efficiency.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Use a dataset with 10 million rows and a 2GB memory limit. Verify that results match non-batched processing and monitor memory usage to ensure it stays within limits.",
            "status": "done",
            "testStrategy": "Compare results with expected outputs and measure memory usage during processing.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:15:09.944Z"
          }
        ],
        "updatedAt": "2025-12-04T22:15:09.944Z"
      },
      {
        "id": "66",
        "title": "Progressive Batch Processing with Early Results",
        "description": "Implement a system to return partial results as batches complete for improved user experience.",
        "details": "Implement the `FuzzyJoinIterator` to stream results as each batch completes. Allow early termination after N results and support `take(n)` to limit results. Use a heap to track best matches across batches for `keep=\"best\"`, optionally sorting candidates by blocking score first. Add a callback API for progress reporting with `on_progress(batch_num, total_batches, matches_found)` to enable progress bars and status updates. Support early termination to stop processing when sufficient matches are found, useful for `keep=\"first\"` strategy. Code location: `polars/crates/polars-ops/src/frame/join/fuzzy_batch.rs`.",
        "testStrategy": "Verify that streaming results match batch results. Measure time-to-first-result improvement and test early termination correctness. Ensure the system handles large datasets efficiently with expected memory usage.",
        "status": "done",
        "dependencies": [
          "65",
          "59",
          "56"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement FuzzyJoinIterator for Streaming Results",
            "description": "Develop the FuzzyJoinIterator to stream results as each batch completes.",
            "dependencies": [],
            "details": "Create the FuzzyJoinIterator in `polars-ops/src/frame/join/fuzzy_batch.rs` to handle streaming of results. Ensure it supports early termination after N results and implements `take(n)` to limit results.",
            "status": "done",
            "testStrategy": "Test streaming correctness and ensure results are returned as expected.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:29:51.160Z"
          },
          {
            "id": 2,
            "title": "Implement Priority Ordering with Heap for Best Match Tracking",
            "description": "Use a heap to track best matches across batches for `keep=\"best\"`.",
            "dependencies": [
              1
            ],
            "details": "Implement a heap structure to maintain the best matches across batches, optionally sorting candidates by blocking score first.",
            "status": "done",
            "testStrategy": "Verify that the heap correctly tracks the best matches and maintains order.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:29:51.195Z"
          },
          {
            "id": 3,
            "title": "Develop Progress Callback API and Early Termination Support",
            "description": "Add a callback API for progress reporting and support early termination.",
            "dependencies": [
              1
            ],
            "details": "Implement `on_progress(batch_num, total_batches, matches_found)` to enable progress bars and status updates. Ensure early termination is supported when sufficient matches are found.",
            "status": "done",
            "testStrategy": "Test the callback API for correct progress reporting and validate early termination functionality.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:29:51.200Z"
          },
          {
            "id": 4,
            "title": "Test Streaming Correctness and Time-to-First-Result",
            "description": "Ensure the streaming implementation is correct and efficient.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Conduct tests to verify that streaming results match batch results. Measure time-to-first-result improvement and test early termination correctness.",
            "status": "done",
            "testStrategy": "Measure performance improvements and validate correctness against expected results.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:29:51.263Z"
          }
        ],
        "updatedAt": "2025-12-04T22:29:51.263Z"
      },
      {
        "id": "67",
        "title": "Batch-Aware Blocking Integration",
        "description": "Optimize blocking strategies to efficiently handle batch processing by reusing indices across batches.",
        "details": "- Implement a persistent blocking index that is built once for the right DataFrame and reused across all left batches. Memory-map large indices to disk for efficiency.\n- Develop incremental blocking for streaming by updating the blocking index as new data arrives, supporting append-only index updates.\n- Optimize batch-aware candidate generation to generate candidates per batch without redundant index lookups.\n- Integrate LSH with batching by building the LSH index for the right DataFrame once and querying it for each left batch, supporting disk-backed LSH index for large datasets.\n- Add index persistence API with methods like build_blocking_index() on StringChunked and save()/load() for index serialization. Include blocking_index parameter in fuzzy_join().",
        "testStrategy": "Verify the correctness of index persistence by saving and loading indices. Benchmark the performance of batched blocking against rebuilding per batch. Test the performance of memory-mapped indices to ensure efficient disk usage.",
        "status": "done",
        "dependencies": [
          "64",
          "65"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Persistent Blocking Index with Memory-Mapping",
            "description": "Develop a persistent blocking index for the right DataFrame, utilizing memory-mapping for large indices.",
            "dependencies": [],
            "details": "Create a blocking index that is built once for the right DataFrame and can be reused across all left batches. Implement memory-mapping to efficiently handle large indices by storing them on disk.",
            "status": "done",
            "testStrategy": "Verify index persistence by saving and loading indices. Test memory-mapped indices for efficient disk usage.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:19:16.226Z"
          },
          {
            "id": 2,
            "title": "Develop LSH Index Persistence and Disk-Backed Storage",
            "description": "Integrate LSH with batching by building a persistent LSH index for the right DataFrame.",
            "dependencies": [
              1
            ],
            "details": "Build the LSH index once for the right DataFrame and enable querying for each left batch. Implement disk-backed storage for the LSH index to handle large datasets efficiently.",
            "status": "done",
            "testStrategy": "Test LSH index persistence by saving and loading. Benchmark performance with disk-backed storage.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:23:06.889Z"
          },
          {
            "id": 3,
            "title": "Optimize Batch-Aware Candidate Generation",
            "description": "Enhance candidate generation to avoid redundant index lookups for each batch.",
            "dependencies": [
              1,
              2
            ],
            "details": "Optimize the process of generating candidates per batch by reusing the blocking index, reducing redundant lookups and improving efficiency.",
            "status": "done",
            "testStrategy": "Benchmark candidate generation performance against redundant lookup methods.",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:23:06.893Z"
          },
          {
            "id": 4,
            "title": "Create Index Persistence API for Python",
            "description": "Develop an API for index persistence, including methods for building and serializing indices.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Add methods like build_blocking_index() on StringChunked and save()/load() for index serialization. Include a blocking_index parameter in fuzzy_join() to support index persistence.",
            "status": "done",
            "testStrategy": "Test API methods for building, saving, and loading indices. Validate integration with fuzzy_join().",
            "parentId": "undefined",
            "updatedAt": "2025-12-04T22:25:43.380Z"
          }
        ],
        "updatedAt": "2025-12-04T22:25:43.380Z"
      },
      {
        "id": "68",
        "title": "Implement Adaptive Blocking with Fuzzy Matching",
        "description": "Implement adaptive blocking strategy that uses fuzzy matching for blocking keys instead of exact matching, improving recall while maintaining performance.",
        "details": "Task 68: Implement Adaptive Blocking with Fuzzy Matching\n\nPriority: High\nPhase: 7 - Advanced Blocking & Automatic Optimization\nDependencies: Task 52 (blocking infrastructure)\n\nImplementation:\n- Create AdaptiveBlocker struct that wraps existing blocking strategies\n- Add max_key_distance parameter (default: 1 edit distance for blocking keys)\n- For FirstNChars: Allow approximate prefix matches (e.g., \"Joh\" matches \"Jon\" with distance 1)\n- For NGram: Allow approximate n-gram matches (e.g., \"abc\" matches \"abd\" with distance 1)\n- Expand blocking keys: Generate all keys within edit distance threshold\n- Example: FirstNChars(3) with max_key_distance=1:\n  - \"John\"  keys: [\"Joh\", \"Jon\", \"Jhn\", \"ohn\"] (original + 1-edit variants)\n  - \"Jon\"  keys: [\"Jon\", \"Joh\", \"Jhn\", \"oon\"] (original + 1-edit variants)\n  - Both strings now share \"Joh\" and \"Jon\" keys  will be compared\n- Add BlockingStrategy::Adaptive { base_strategy, max_key_distance } variant\n- Integrate with existing blocking infrastructure\n\nExpected Impact:\n- 5-15% improvement in recall (fewer missed matches)\n- 10-30% increase in candidate pairs (more comparisons, but still much less than full scan)\n- Maintains 80-95% comparison reduction vs full scan\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs",
        "testStrategy": "Verify recall improvement on datasets with typos/errors in prefixes, measure precision/recall tradeoff, ensure all tests still pass",
        "status": "done",
        "dependencies": [
          "52"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create AdaptiveBlocker Struct",
            "description": "Develop the AdaptiveBlocker struct that will encapsulate the existing blocking strategies and allow for fuzzy matching.",
            "dependencies": [],
            "details": "The AdaptiveBlocker struct should wrap around the existing blocking strategies and include methods for fuzzy matching. It will serve as the main interface for the adaptive blocking functionality.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:18.488Z"
          },
          {
            "id": 2,
            "title": "Implement max_key_distance Parameter",
            "description": "Add a max_key_distance parameter to the AdaptiveBlocker struct to define the allowable edit distance for blocking keys.",
            "dependencies": [
              1
            ],
            "details": "The max_key_distance parameter should default to 1 and be used to control the edit distance for fuzzy matching. This will allow for more flexible matching of blocking keys.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:18.498Z"
          },
          {
            "id": 3,
            "title": "Enhance FirstNChars for Approximate Matches",
            "description": "Modify the FirstNChars function to support approximate prefix matches based on the max_key_distance parameter.",
            "dependencies": [
              2
            ],
            "details": "The FirstNChars function should be updated to allow for approximate matches, enabling it to match strings like 'Joh' with 'Jon' when the edit distance is 1.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:18.505Z"
          },
          {
            "id": 4,
            "title": "Update NGram for Fuzzy Matching",
            "description": "Revise the NGram function to allow for approximate n-gram matches using the max_key_distance parameter.",
            "dependencies": [
              2
            ],
            "details": "The NGram function should be enhanced to support fuzzy matching, allowing it to match strings like 'abc' with 'abd' when the edit distance is 1.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:18.512Z"
          },
          {
            "id": 5,
            "title": "Integrate Adaptive Blocking with Existing Infrastructure",
            "description": "Integrate the new AdaptiveBlocker struct and its functionalities into the existing blocking infrastructure.",
            "dependencies": [
              1,
              3,
              4
            ],
            "details": "Ensure that the AdaptiveBlocker works seamlessly with the existing blocking infrastructure, allowing for both exact and fuzzy matching strategies to be utilized effectively.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:18.519Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:18.519Z"
      },
      {
        "id": "69",
        "title": "Implement Automatic Blocking Strategy Selection",
        "description": "Automatically select optimal blocking strategy based on data characteristics (dataset size, string length distribution, character diversity, data distribution).",
        "details": "Task 69: Implement Automatic Blocking Strategy Selection\n\nPriority: High\nPhase: 7 - Advanced Blocking & Automatic Optimization\nDependencies: Task 68\n\nImplementation:\n- Create BlockingStrategySelector that analyzes:\n  - Dataset size (n, m)\n  - String length distribution (mean, std, min, max)\n  - Character diversity (unique characters, entropy)\n  - Data distribution (sorted, random, clustered)\n  - Expected match rate (from threshold)\n- Selection logic:\n  - Small datasets (< 1K rows): No blocking (full scan faster)\n  - Medium datasets (1K-10K rows): FirstChars(3) or NGram(3) based on prefix stability\n  - Large datasets (10K-100K rows): NGram(3) or SortedNeighborhood(10)\n  - Very large datasets (100K+ rows): LSH with auto-tuned parameters\n  - Sorted data: SortedNeighborhood (detect via prefix distribution)\n  - High character diversity: NGram (more robust to variations)\n  - Low character diversity: FirstChars (fewer collisions)\n- Add BlockingStrategy::Auto variant that triggers automatic selection\n- Cache selection results for repeated joins on same columns\n- Provide recommend_blocking_strategy() utility function\n\nExpected Impact:\n- Users don't need to manually tune blocking parameters\n- Optimal strategy selected automatically for each dataset\n- 20-50% better performance vs manual strategy selection\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs\n- New file: polars/crates/polars-ops/src/frame/join/fuzzy_blocking_auto.rs",
        "testStrategy": "Test on various dataset characteristics, verify optimal strategy selected, benchmark performance vs manual selection",
        "status": "done",
        "dependencies": [
          "68"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create BlockingStrategySelector Class",
            "description": "Develop the BlockingStrategySelector class that will analyze various data characteristics to determine the optimal blocking strategy.",
            "dependencies": [],
            "details": "The class should include methods to analyze dataset size, string length distribution, character diversity, and data distribution. It will serve as the core component for automatic blocking strategy selection.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:19.878Z"
          },
          {
            "id": 2,
            "title": "Implement Selection Logic for Blocking Strategies",
            "description": "Define the selection logic based on dataset characteristics to choose the appropriate blocking strategy.",
            "dependencies": [
              1
            ],
            "details": "Implement logic to handle different dataset sizes and characteristics, including conditions for small, medium, large, and very large datasets. Ensure that the logic is flexible to accommodate various data distributions.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:19.956Z"
          },
          {
            "id": 3,
            "title": "Add Auto Variant to BlockingStrategy",
            "description": "Introduce an Auto variant in the BlockingStrategy that triggers automatic selection based on the analysis results.",
            "dependencies": [
              2
            ],
            "details": "This variant should call the BlockingStrategySelector and return the optimal blocking strategy based on the analyzed data characteristics.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:20.051Z"
          },
          {
            "id": 4,
            "title": "Implement Caching for Selection Results",
            "description": "Create a caching mechanism to store selection results for repeated joins on the same columns to improve performance.",
            "dependencies": [
              3
            ],
            "details": "The caching should be efficient and allow quick retrieval of previously computed blocking strategies to avoid redundant calculations during repeated joins.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:20.057Z"
          },
          {
            "id": 5,
            "title": "Develop recommend_blocking_strategy Utility Function",
            "description": "Create a utility function recommend_blocking_strategy() that provides recommendations for blocking strategies based on input data characteristics.",
            "dependencies": [
              4
            ],
            "details": "This function should take parameters related to dataset characteristics and return the recommended blocking strategy, enhancing user experience by simplifying the selection process.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:20.062Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:20.062Z"
      },
      {
        "id": "70",
        "title": "Implement Approximate Nearest Neighbor Pre-filtering for Large Datasets",
        "description": "Use approximate nearest neighbor search (LSH, HNSW, or FAISS-style) for very large datasets (1M+ rows) to pre-filter candidates before exact similarity computation.",
        "details": "Task 70: Implement Approximate Nearest Neighbor Pre-filtering for Large Datasets\n\nPriority: Medium\nPhase: 7 - Advanced Blocking & Automatic Optimization\nDependencies: Task 64 (LSH), Task 69\n\nImplementation:\n- Create ANNPreFilter struct that wraps LSH or other ANN algorithms\n- Two-stage filtering:\n  1. ANN Stage: Use LSH/ANN to find approximate nearest neighbors (fast, approximate)\n  2. Exact Stage: Compute exact similarity only for ANN candidates (slower, exact)\n- For datasets > 1M rows:\n  - Build ANN index (LSH, HNSW, or FAISS-style)\n  - Query ANN index for each left string  get top-K approximate neighbors\n  - Only compute exact similarity for ANN candidates\n  - Reduces comparisons from O(n*m) to O(n*log(m) + n*K) where K << m\n- Integrate with existing blocking:\n  - Use ANN as pre-filter, then apply blocking on ANN candidates\n  - Or use blocking first, then ANN on blocked candidates\n- Configurable K (number of approximate neighbors to retrieve)\n- Add BlockingStrategy::ANN { k, lsh_config } variant\n- Support multiple ANN backends:\n  - LSH (already implemented)\n  - HNSW (Hierarchical Navigable Small World) - optional\n  - FAISS integration - optional\n\nExpected Impact:\n- Enable fuzzy joins on billion-scale datasets (1M+ rows)\n- 100-1000x reduction in comparisons for very large datasets\n- Sub-second query time for 1M+ row datasets\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/fuzzy_blocking.rs\n- New file: polars/crates/polars-ops/src/frame/join/fuzzy_ann.rs",
        "testStrategy": "Test on large synthetic datasets (1M+ rows), verify recall > 95%, benchmark query time, compare with full scan",
        "status": "done",
        "dependencies": [
          "64",
          "69"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ANNPreFilter Struct",
            "description": "Develop the ANNPreFilter struct that will encapsulate the logic for using LSH or other ANN algorithms for pre-filtering.",
            "dependencies": [],
            "details": "The struct should include methods for initializing the ANN algorithm, configuring parameters, and storing the index. Ensure it can handle multiple backends like LSH, HNSW, and FAISS.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:21.118Z"
          },
          {
            "id": 2,
            "title": "Implement ANN Stage Filtering",
            "description": "Implement the first stage of the two-stage filtering process using LSH or another ANN algorithm to find approximate nearest neighbors.",
            "dependencies": [
              1
            ],
            "details": "This stage should efficiently query the ANN index to retrieve top-K approximate neighbors for each input string, optimizing for speed and accuracy.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:21.122Z"
          },
          {
            "id": 3,
            "title": "Implement Exact Similarity Computation",
            "description": "Develop the logic to compute exact similarity for the candidates retrieved from the ANN stage.",
            "dependencies": [
              2
            ],
            "details": "This should involve calculating the exact similarity scores only for the candidates identified in the ANN stage, ensuring that the process is efficient and scalable.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:21.126Z"
          },
          {
            "id": 4,
            "title": "Integrate with Existing Blocking Mechanism",
            "description": "Integrate the ANN pre-filtering with the existing blocking strategies to enhance performance.",
            "dependencies": [
              1,
              2
            ],
            "details": "Determine whether to apply ANN as a pre-filter before blocking or to use blocking first. Update the blocking strategy to support the new ANN approach.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:21.130Z"
          },
          {
            "id": 5,
            "title": "Add Configurable Parameters and Test",
            "description": "Implement configurable parameters for the number of approximate neighbors (K) and test the overall system performance.",
            "dependencies": [
              3,
              4
            ],
            "details": "Ensure that the system allows for dynamic configuration of K and validate the performance on large datasets, aiming for a recall rate above 95%.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:21.134Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:21.134Z"
      },
      {
        "id": "71",
        "title": "Use Existing Blocking Strategies More Aggressively by Default",
        "description": "Enable blocking by default with optimal parameters, make it easier to use without manual configuration.",
        "details": "Task 71: Use Existing Blocking Strategies More Aggressively by Default\n\nPriority: Medium\nPhase: 7 - Advanced Blocking & Automatic Optimization\nDependencies: Task 69\n\nImplementation:\n- Change default BlockingStrategy from None to Auto in FuzzyJoinArgs\n- Auto-enable blocking when:\n  - Dataset size > 100 rows (left or right)\n  - Expected comparisons > 10,000 (n * m > 10,000)\n- Provide smart defaults for blocking parameters:\n  - FirstChars: n = 3 (good balance of precision/recall)\n  - NGram: n = 3 (trigrams are standard)\n  - Length: max_diff = 2 (reasonable for most use cases)\n  - SortedNeighborhood: window = 10 (good for large datasets)\n  - LSH: Auto-tune based on dataset size and threshold\n- Add auto_blocking parameter (default: true) to allow disabling\n- Update Python API to enable blocking by default\n- Add warnings when blocking is disabled for large datasets\n\nExpected Impact:\n- Users get optimal performance without manual configuration\n- 90%+ of users benefit from automatic blocking\n- Reduced support burden (fewer \"why is my join slow?\" questions)\n\nCode Location:\n- polars/crates/polars-ops/src/frame/join/args.rs\n- polars/crates/polars-ops/src/frame/join/fuzzy.rs\n- polars/crates/polars-python/src/functions/fuzzy_join.rs",
        "testStrategy": "Test default behavior on various dataset sizes, verify blocking enabled appropriately, ensure backward compatibility",
        "status": "done",
        "dependencies": [
          "69"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Change Default BlockingStrategy to Auto",
            "description": "Modify the default BlockingStrategy from None to Auto in FuzzyJoinArgs.",
            "dependencies": [],
            "details": "Update the FuzzyJoinArgs struct to set the default BlockingStrategy to Auto. Ensure that this change is reflected in all relevant parts of the codebase.",
            "status": "done",
            "testStrategy": "Verify that the default strategy is set to Auto in new instances of FuzzyJoinArgs.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:22.306Z"
          },
          {
            "id": 2,
            "title": "Implement Auto-Enable Blocking Logic",
            "description": "Enable blocking automatically based on dataset size and expected comparisons.",
            "dependencies": [
              1
            ],
            "details": "Add logic to auto-enable blocking when the dataset size exceeds 100 rows or expected comparisons exceed 10,000. Implement checks in the relevant functions to enforce these conditions.",
            "status": "done",
            "testStrategy": "Test with datasets of varying sizes to ensure blocking is enabled appropriately.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:22.316Z"
          },
          {
            "id": 3,
            "title": "Set Smart Defaults for Blocking Parameters",
            "description": "Define smart default values for various blocking parameters.",
            "dependencies": [
              1
            ],
            "details": "Set default values for FirstChars, NGram, Length, SortedNeighborhood, and LSH parameters. Ensure these defaults provide a good balance of precision and recall.",
            "status": "done",
            "testStrategy": "Evaluate the performance of default parameters on standard datasets.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:22.324Z"
          },
          {
            "id": 4,
            "title": "Add Auto-Blocking Parameter to API",
            "description": "Introduce an auto_blocking parameter to allow users to disable automatic blocking.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Update the Python API to include an auto_blocking parameter, defaulting to true. Ensure this parameter can be toggled by users to disable automatic blocking if desired.",
            "status": "done",
            "testStrategy": "Test the API to confirm that the auto_blocking parameter functions as expected.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:22.328Z"
          },
          {
            "id": 5,
            "title": "Implement Warnings for Disabled Blocking",
            "description": "Add warnings when blocking is disabled for large datasets.",
            "dependencies": [
              2,
              4
            ],
            "details": "Implement a warning system that alerts users when blocking is disabled for datasets that exceed the size threshold. Ensure warnings are clear and informative.",
            "status": "done",
            "testStrategy": "Verify that warnings are displayed correctly when blocking is disabled for large datasets.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:22.333Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:22.333Z"
      },
      {
        "id": "72",
        "title": "Additional Performance Optimizations for Phase 7",
        "description": "Additional optimizations identified during Phase 7 implementation based on profiling results.",
        "details": "Task 72: Additional Performance Optimizations\n\nPriority: Low\nPhase: 7 - Advanced Blocking & Automatic Optimization\nDependencies: Tasks 68-71\n\nPotential Optimizations:\n- Blocking key caching: Cache generated blocking keys across multiple joins\n- Parallel blocking key generation: Use Rayon for generating blocking keys\n- Blocking index persistence: Save/load blocking indices for repeated joins\n- Multi-threaded candidate generation: Parallelize candidate pair generation\n- Blocking strategy combination: Better support for combining multiple strategies\n\nImplementation: To be determined based on profiling results during Phase 7 implementation\n\nExpected Impact: Additional 10-30% performance improvement based on identified bottlenecks\n\nCode Location: To be determined based on profiling results",
        "testStrategy": "Profile Phase 7 implementation, identify bottlenecks, implement targeted optimizations, verify performance improvements",
        "status": "done",
        "dependencies": [
          "68",
          "69",
          "70",
          "71"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Blocking Key Caching",
            "description": "Cache generated blocking keys across multiple joins to improve performance.",
            "dependencies": [],
            "details": "Modify the existing join logic to store blocking keys in a cache. Ensure keys are reused in subsequent joins.",
            "status": "done",
            "testStrategy": "Verify cache hits and measure performance improvement.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:27.653Z"
          },
          {
            "id": 2,
            "title": "Enable Parallel Blocking Key Generation",
            "description": "Use Rayon to parallelize the generation of blocking keys for efficiency.",
            "dependencies": [
              1
            ],
            "details": "Integrate Rayon into the blocking key generation process to allow concurrent execution.",
            "status": "done",
            "testStrategy": "Measure execution time before and after parallelization.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:27.657Z"
          },
          {
            "id": 3,
            "title": "Implement Blocking Index Persistence",
            "description": "Save and load blocking indices to avoid recomputation in repeated joins.",
            "dependencies": [
              1
            ],
            "details": "Develop a mechanism to serialize and deserialize blocking indices to disk.",
            "status": "done",
            "testStrategy": "Test persistence by saving indices and reloading them in a new session.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:27.661Z"
          },
          {
            "id": 4,
            "title": "Parallelize Candidate Pair Generation",
            "description": "Use multi-threading to parallelize the generation of candidate pairs.",
            "dependencies": [
              2
            ],
            "details": "Refactor candidate generation logic to utilize multiple threads for processing.",
            "status": "done",
            "testStrategy": "Compare performance with single-threaded execution.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:27.665Z"
          },
          {
            "id": 5,
            "title": "Enhance Blocking Strategy Combination",
            "description": "Improve support for combining multiple blocking strategies effectively.",
            "dependencies": [
              3,
              4
            ],
            "details": "Design a flexible framework to allow dynamic combination of blocking strategies based on data characteristics.",
            "status": "done",
            "testStrategy": "Evaluate performance with different strategy combinations and validate results.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T03:19:27.668Z"
          }
        ],
        "updatedAt": "2025-12-05T03:19:27.668Z"
      },
      {
        "id": "73",
        "title": "Implement TF-IDF N-gram Sparse Vector Blocker",
        "description": "Implement a blocking strategy using TF-IDF weighted n-gram sparse vectors with cosine similarity thresholding. This is the core implementation for Phase 8 that replaces LSH with a more deterministic approach used by pl-fuzzy-frame-match.",
        "details": "Create SparseVectorBlocker struct implementing FuzzyJoinBlocker trait with: ngram_size (default: 3), min_cosine_similarity (default: 0.3), and precomputed IDF HashMap. Implement build_idf() to compute IDF from both columns using ln(N/df). Implement to_sparse_vector() for TF-IDF weighted sparse vectors with L2 normalization. Implement efficient generate_candidates() using inverted index with dot product accumulation and threshold filtering. Add BlockingStrategy::SparseVector { ngram_size, min_cosine_similarity } variant.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "52",
          "64"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Define SparseVectorBlocker Struct",
            "description": "Create the SparseVectorBlocker struct implementing the FuzzyJoinBlocker trait.",
            "dependencies": [],
            "details": "Define the struct with ngram_size (default: 3), min_cosine_similarity (default: 0.3), and precomputed IDF HashMap.",
            "status": "done",
            "testStrategy": "Ensure struct fields are correctly initialized with default values.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T07:00:12.243Z"
          },
          {
            "id": 2,
            "title": "Implement build_idf() Function",
            "description": "Develop the build_idf() function to compute IDF values.",
            "dependencies": [
              1
            ],
            "details": "Compute IDF from both columns using ln(N/df) and store in a HashMap.",
            "status": "done",
            "testStrategy": "Verify IDF values against known datasets.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T07:00:12.247Z"
          },
          {
            "id": 3,
            "title": "Develop to_sparse_vector() Method",
            "description": "Implement the to_sparse_vector() method for TF-IDF weighted vectors.",
            "dependencies": [
              2
            ],
            "details": "Create TF-IDF weighted sparse vectors with L2 normalization.",
            "status": "done",
            "testStrategy": "Check vector normalization and TF-IDF weighting accuracy.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T07:00:12.251Z"
          },
          {
            "id": 4,
            "title": "Create generate_candidates() Method",
            "description": "Implement generate_candidates() using inverted index and dot product.",
            "dependencies": [
              3
            ],
            "details": "Use dot product accumulation and threshold filtering for candidate generation.",
            "status": "done",
            "testStrategy": "Test candidate generation efficiency and correctness.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T07:00:12.255Z"
          },
          {
            "id": 5,
            "title": "Add BlockingStrategy::SparseVector Variant",
            "description": "Introduce the SparseVector variant to BlockingStrategy enum.",
            "dependencies": [
              1
            ],
            "details": "Add BlockingStrategy::SparseVector { ngram_size, min_cosine_similarity } variant.",
            "status": "done",
            "testStrategy": "Ensure enum variant is correctly integrated and accessible.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T07:00:12.258Z"
          },
          {
            "id": 6,
            "title": "Integrate and Test Sparse Vector Blocker",
            "description": "Integrate SparseVectorBlocker into the system and perform end-to-end testing.",
            "dependencies": [
              4,
              5
            ],
            "details": "Ensure full integration with existing system and validate with comprehensive tests.",
            "status": "done",
            "testStrategy": "Conduct end-to-end tests with various datasets to validate functionality.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T07:00:12.261Z"
          }
        ],
        "updatedAt": "2025-12-05T07:00:12.261Z"
      },
      {
        "id": "74",
        "title": "Optimize Sparse Vector Operations",
        "description": "Optimize sparse vector operations for performance including efficient IDF computation, SIMD-accelerated dot product accumulation, memory-efficient inverted index, parallel candidate generation, and early termination optimizations.",
        "details": "Subtasks: 1) Efficient IDF Computation with Rayon parallel iteration, 2) SIMD-Accelerated Dot Product Accumulation for score aggregation, 3) Memory-Efficient Inverted Index using SmallVec and arena allocation, 4) Parallel Candidate Generation with thread-local accumulators, 5) Early Termination tracking maximum possible remaining score. Expected 2-3x speedup over naive implementation.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "73"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Efficient IDF Computation",
            "description": "Use Rayon for parallel iteration to optimize IDF computation.",
            "dependencies": [],
            "details": "Implement parallel iteration with Rayon to efficiently compute IDF values across large datasets.\n<info added on 2025-12-05T17:10:01.760Z>\nParallel IDF computation is implemented using Rayon in `build_idf_parallel()` method (lines 2205-2248 in fuzzy_blocking.rs). The implementation uses `into_par_iter()` for parallel document frequency computation across both left and right columns.\n</info added on 2025-12-05T17:10:01.760Z>",
            "status": "done",
            "testStrategy": "Benchmark against single-threaded implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:09:53.332Z"
          },
          {
            "id": 2,
            "title": "Develop SIMD-Accelerated Dot Product",
            "description": "Use SIMD instructions to accelerate dot product calculations.",
            "dependencies": [
              1
            ],
            "details": "Leverage SIMD instructions to optimize dot product operations for faster score aggregation.\n<info added on 2025-12-05T17:10:18.631Z>\nThe SIMD-accelerated dot product implementation is missing. The `SparseVector::dot()` method currently uses a standard sequential merge algorithm without SIMD optimizations. No SIMD intrinsics or vectorized operations are present in the codebase.\n</info added on 2025-12-05T17:10:18.631Z>",
            "status": "done",
            "testStrategy": "Compare performance with non-SIMD implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:39:56.792Z"
          },
          {
            "id": 3,
            "title": "Create Memory-Efficient Inverted Index",
            "description": "Utilize SmallVec and arena allocation for memory-efficient indexing.",
            "dependencies": [
              1
            ],
            "details": "Implement inverted index using SmallVec for small allocations and arena allocation for efficient memory use.\n<info added on 2025-12-05T17:10:21.259Z>\nThe current implementation of the inverted index does not utilize SmallVec or arena allocation as intended. Instead, it uses a standard `HashMap<u64, Vec<(usize, f32)>>`, which does not align with the memory efficiency goals. This discrepancy needs to be addressed to meet the subtask's requirements.\n</info added on 2025-12-05T17:10:21.259Z>",
            "status": "deferred",
            "testStrategy": "Measure memory usage against traditional Vec-based implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:40:07.096Z"
          },
          {
            "id": 4,
            "title": "Implement Parallel Candidate Generation",
            "description": "Use thread-local accumulators for parallel candidate generation.",
            "dependencies": [
              2,
              3
            ],
            "details": "Optimize candidate generation by implementing thread-local accumulators to enhance parallel processing.\n<info added on 2025-12-05T17:10:04.732Z>\n VERIFIED: Parallel candidate generation is implemented using Rayon in `generate_candidates_parallel()` method (lines 2120-2203 in fuzzy_blocking.rs). The implementation uses thread-local processing with `into_par_iter()` for building sparse vectors and generating candidates in parallel. Uses Arc for shared inverted index and IDF HashMap.\n</info added on 2025-12-05T17:10:04.732Z>",
            "status": "done",
            "testStrategy": "Test concurrency and correctness with multi-threaded scenarios.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:09:54.292Z"
          },
          {
            "id": 5,
            "title": "Optimize Early Termination Strategy",
            "description": "Track maximum possible remaining score for early termination.",
            "dependencies": [
              4
            ],
            "details": "Implement a strategy to track and utilize the maximum possible remaining score to enable early termination in computations.\n<info added on 2025-12-05T17:10:23.802Z>\nThe early termination strategy for tracking the maximum possible remaining score has not been implemented. The candidate generation methods (`generate_candidates_sequential` and `generate_candidates_parallel`) currently process all candidates without utilizing early termination optimizations. Implement a mechanism to track and utilize the maximum possible remaining score to enable early termination in computations.\n</info added on 2025-12-05T17:10:23.802Z>",
            "status": "done",
            "testStrategy": "Validate correctness and performance improvement with test cases that trigger early termination.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:39:56.797Z"
          }
        ],
        "updatedAt": "2025-12-05T17:40:18.100Z"
      },
      {
        "id": "75",
        "title": "Integrate BK-Tree with Sparse Vector Blocking",
        "description": "Create a hybrid blocking strategy combining BK-Tree for small edit distances with sparse vectors for general matching. BK-Tree provides 100% recall for high-threshold edit distance queries while sparse vectors handle general cases.",
        "details": "Create HybridBlocker struct combining bk_tree (for Levenshtein with high thresholds) and sparse_blocker (for general matching). Selection logic: Levenshtein/Damerau-Levenshtein + threshold >= 0.8 uses BK-Tree, Jaro-Winkler/Hamming or lower thresholds use Sparse Vector. Add BlockingStrategy::Hybrid variant. Auto-select in BlockingStrategySelector based on similarity metric and threshold.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "58",
          "73"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Design HybridBlocker Struct",
            "description": "Design the HybridBlocker struct to combine BK-Tree and sparse vector blocking.",
            "dependencies": [],
            "details": "Create a struct that integrates bk_tree for Levenshtein with high thresholds and sparse_blocker for general matching. Define fields and methods for each component.\n<info added on 2025-12-05T17:10:07.591Z>\n VERIFIED: HybridBlocker struct is fully implemented (lines 2264-2312 in fuzzy_blocking.rs). The struct combines `sparse_blocker: SparseVectorBlocker` and uses `HybridBlockingConfig` for configuration. It has methods `new()`, `for_metric_and_threshold()`, and implements `FuzzyJoinBlocker` trait.\n</info added on 2025-12-05T17:10:07.591Z>",
            "status": "done",
            "testStrategy": "Review design with team and ensure alignment with requirements.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:09:55.203Z"
          },
          {
            "id": 2,
            "title": "Implement BK-Tree Integration",
            "description": "Implement the BK-Tree component for high-threshold edit distance queries.",
            "dependencies": [
              1
            ],
            "details": "Develop the BK-Tree logic within the HybridBlocker struct. Ensure it handles Levenshtein/Damerau-Levenshtein with threshold >= 0.8.\n<info added on 2025-12-05T17:10:10.246Z>\nBK-Tree integration is implemented in the `generate_candidates_bktree()` method (lines 2284-2295 in fuzzy_blocking.rs). The method builds a BK-Tree from the right column using `BKTree::from_chunked()` and generates candidates using the `generate_candidates_with_bktree()` function with a configurable `max_edit_distance`.\n</info added on 2025-12-05T17:10:10.246Z>",
            "status": "done",
            "testStrategy": "Test with high-threshold edit distance queries to ensure 100% recall.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:09:56.185Z"
          },
          {
            "id": 3,
            "title": "Implement Sparse Vector Integration",
            "description": "Implement the sparse vector component for general matching cases.",
            "dependencies": [
              1
            ],
            "details": "Develop the sparse vector logic within the HybridBlocker struct. Handle Jaro-Winkler, Hamming, and lower threshold cases.\n<info added on 2025-12-05T17:10:12.998Z>\nSparse Vector integration is verified in HybridBlocker. The struct includes `sparse_blocker: SparseVectorBlocker` and utilizes it in the `generate_candidates()` method when `use_bktree` is false. The HybridBlocker can switch between BK-Tree and Sparse Vector based on configuration.\n</info added on 2025-12-05T17:10:12.998Z>",
            "status": "done",
            "testStrategy": "Validate against known sparse vector matching cases.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:09:57.006Z"
          },
          {
            "id": 4,
            "title": "Develop BlockingStrategySelector Logic",
            "description": "Implement logic to auto-select blocking strategy based on metric and threshold.",
            "dependencies": [
              2,
              3
            ],
            "details": "Modify BlockingStrategySelector to choose between BK-Tree and sparse vector based on similarity metric and threshold. Add BlockingStrategy::Hybrid variant.\n<info added on 2025-12-05T17:10:15.820Z>\nThe auto-selector logic needs to be updated to consider the similarity metric type and threshold when selecting the blocking strategy. Currently, it only considers dataset size, and users must manually specify `blocking=\"hybrid\"` to use the HybridBlocker. The `select_strategy()` method should be modified to receive the similarity metric as a parameter to enable metric-based decision-making.\n</info added on 2025-12-05T17:10:15.820Z>",
            "status": "done",
            "testStrategy": "Test auto-selection with various metrics and thresholds to ensure correct strategy is chosen.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:39:57.592Z"
          }
        ],
        "updatedAt": "2025-12-05T17:39:57.592Z"
      },
      {
        "id": "76",
        "title": "Replace LSH with Sparse Vector in Auto-Selector",
        "description": "Update the automatic blocking strategy selector to prefer sparse vector over LSH for medium-large datasets. This closes the 28% performance gap with pl-fuzzy-frame-match observed at 25M comparisons.",
        "details": "Update BlockingStrategySelector::select_strategy() to use: 10K-100K comparisons  SparseVector(min_cosine=0.3), 100K-1M comparisons  SparseVector(min_cosine=0.5), 1M+ comparisons  ANN with SparseVector backend. Update ANNPreFilter to support SparseVector backend via ANNBackend enum. Keep LSH as fallback for users who explicitly request it.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "73",
          "74",
          "69"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update BlockingStrategySelector for Sparse Vector",
            "description": "Modify the select_strategy function to prefer Sparse Vector over LSH for specific comparison ranges.",
            "dependencies": [],
            "details": "Update BlockingStrategySelector::select_strategy() to use SparseVector(min_cosine=0.3) for 10K-100K comparisons, SparseVector(min_cosine=0.5) for 100K-1M comparisons, and ANN with SparseVector backend for 1M+ comparisons.",
            "status": "done",
            "testStrategy": "Verify strategy selection with unit tests for each comparison range.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T16:48:09.032Z"
          },
          {
            "id": 2,
            "title": "Implement Sparse Vector Backend in ANNPreFilter",
            "description": "Add support for Sparse Vector backend in ANNPreFilter using the ANNBackend enum.",
            "dependencies": [
              1
            ],
            "details": "Modify ANNPreFilter to recognize and utilize Sparse Vector as a backend option through the ANNBackend enum.\n<info added on 2025-12-05T16:48:48.518Z>\nThe ANNPreFilter does not support the SparseVector backend as initially planned. Instead, the strategy selector has been updated to use SparseVector directly for datasets with over 1 million comparisons, bypassing the need for ANNPreFilter modifications. The implementation now utilizes StreamingSparseVectorBlocker for these large datasets, which is more suitable than using ANN with LSH.\n</info added on 2025-12-05T16:48:48.518Z>",
            "status": "done",
            "testStrategy": "Create unit tests to ensure ANNPreFilter correctly initializes and uses Sparse Vector backend.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T16:48:54.564Z"
          },
          {
            "id": 3,
            "title": "Maintain LSH as Fallback Option",
            "description": "Ensure LSH remains available as a fallback for users who explicitly request it.",
            "dependencies": [
              1
            ],
            "details": "Update the configuration to allow users to select LSH explicitly, overriding the default Sparse Vector preference.",
            "status": "done",
            "testStrategy": "Test user configuration options to confirm LSH can be selected explicitly.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:39:58.566Z"
          },
          {
            "id": 4,
            "title": "Performance Testing and Validation",
            "description": "Conduct performance tests to validate the 28% improvement claim with the new Sparse Vector strategy.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Run performance benchmarks comparing the updated strategy against the previous LSH approach, focusing on datasets with 25M comparisons.",
            "status": "done",
            "testStrategy": "Benchmark tests with datasets of varying sizes to confirm performance improvements.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T17:39:58.570Z"
          }
        ],
        "updatedAt": "2025-12-05T17:39:58.570Z"
      },
      {
        "id": "77",
        "title": "Add Sparse Vector Blocking Parameters to Python API",
        "description": "Expose sparse vector blocking parameters to Python API including blocking strategy selection and cosine similarity threshold configuration.",
        "details": "Update Python fuzzy_join() signature with: blocking parameter (auto, sparse_vector, lsh, ngram, first_chars, none), blocking_ngram_size (default: 3), blocking_min_cosine (default: 0.3). Add documentation explaining blocking strategies. Add get_blocking_strategies() function. Update type hints and docstrings for backwards compatibility.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "76"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-05T07:03:07.138Z"
      },
      {
        "id": "78",
        "title": "Benchmark Sparse Vector vs LSH vs pl-fuzzy-frame-match",
        "description": "Comprehensive benchmarking to validate sparse vector approach matches or exceeds pl-fuzzy-frame-match performance and closes the 28% gap observed at 25M comparisons for Levenshtein.",
        "details": "Create benchmark script comparing Sparse Vector, LSH, NGram blocking, and pl-fuzzy-frame-match. Test dataset sizes: 1K, 10K, 100K, 1M rows. Test metrics: Levenshtein, Jaro-Winkler, Damerau-Levenshtein. Test thresholds: 0.5, 0.7, 0.8, 0.9. Measure: candidate generation time, total join time, recall, precision, memory usage, comparisons/second. Document findings in SPARSE_VECTOR_BENCHMARK.md.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "76"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-05T07:00:14.967Z"
      },
      {
        "id": "79",
        "title": "Adaptive Cosine Threshold Based on String Length",
        "description": "Automatically adjust cosine similarity threshold based on string length characteristics. Shorter strings need lower thresholds (fewer n-grams), longer strings can use higher thresholds.",
        "details": "Implement adaptive_threshold() function: threshold = base_threshold * length_factor where length_factor = (avg_length / 10.0).clamp(0.5, 1.5). Integrate with SparseVectorBlocker to compute average string length and adjust threshold automatically. Add adaptive_threshold: bool parameter (default: true) with user override option.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "73"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-05T07:03:07.793Z"
      },
      {
        "id": "80",
        "title": "Streaming Sparse Vector Index for Large Datasets",
        "description": "Support streaming/batched sparse vector blocking for datasets larger than memory using disk-backed inverted index and memory-mapped files.",
        "details": "Implement StreamingSparseVectorBlocker: build IDF from sample, process left DataFrame in batches, query inverted index per batch, yield candidates as batches complete. Memory-efficient inverted index with disk-backed storage and memory-mapped files. Integrate with existing batch processing (Task 65). Add streaming: bool parameter to SparseVectorBlocker.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "73",
          "65"
        ],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2025-12-05T07:03:08.426Z"
      },
      {
        "id": "81",
        "title": "Batch-Level SIMD for Fuzzy Join",
        "description": "Implement SIMD processing for multiple string pairs to enhance fuzzy join performance.",
        "details": "Develop the `compute_batch_similarities_simd8()` function to process 8 string pairs simultaneously using SIMD. Implement `jaro_winkler_batch8()`, `levenshtein_batch8()`, and `damerau_levenshtein_batch8()` functions. Optimize for 2-4x speedup in fuzzy join operations by leveraging SIMD instructions. Ensure compatibility with existing fuzzy join infrastructure and consider edge cases such as varying string lengths and null values.",
        "testStrategy": "Benchmark the SIMD implementation against the current single-pair processing. Validate speedup by comparing execution times for large datasets. Ensure accuracy by cross-verifying results with non-SIMD implementations. Test edge cases including null values and varying string lengths to ensure robustness.",
        "status": "done",
        "dependencies": [
          "17",
          "65",
          "8"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-05T21:10:18.683Z"
      },
      {
        "id": "82",
        "title": "Stack Allocation for Medium Strings",
        "description": "Optimize string similarity functions by using stack-allocated buffers for strings 128 chars to reduce overhead.",
        "details": "Implement stack-allocated buffers for medium-length strings in the string similarity functions to eliminate the overhead of thread-local storage. Specifically, create `levenshtein_distance_stack_optimized()` using `[usize; 129]` stack arrays, `jaro_similarity_stack_optimized()` using `[bool; 65]` stack arrays, and `damerau_levenshtein_stack_optimized()` using `[usize; 129]` stack arrays. This change is expected to reduce the overhead by 10-20%. Ensure that the stack allocation is efficient and does not introduce stack overflow risks. Refactor existing functions to utilize these optimized versions where applicable.",
        "testStrategy": "Develop unit tests to compare the performance and correctness of the stack-optimized functions against the existing implementations. Measure the overhead reduction using benchmarks that focus on function call overhead. Validate the correctness by comparing results with known outputs for various string pairs, including edge cases with maximum length strings (128 chars).",
        "status": "done",
        "dependencies": [
          "8",
          "18"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Levenshtein Distance Stack Optimized Function",
            "description": "Create a stack-optimized version of the Levenshtein distance function using `[usize; 129]` arrays.",
            "dependencies": [],
            "details": "Develop `levenshtein_distance_stack_optimized()` using stack-allocated buffers to reduce overhead. Ensure it handles strings up to 128 characters efficiently.",
            "status": "done",
            "testStrategy": "Develop unit tests to compare performance and correctness with existing implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:03.382Z"
          },
          {
            "id": 2,
            "title": "Implement Jaro Similarity Stack Optimized Function",
            "description": "Create a stack-optimized version of the Jaro similarity function using `[bool; 65]` arrays.",
            "dependencies": [
              1
            ],
            "details": "Develop `jaro_similarity_stack_optimized()` using stack-allocated buffers. Ensure it efficiently handles strings up to 128 characters.",
            "status": "done",
            "testStrategy": "Develop unit tests to validate performance and correctness against existing implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:03.389Z"
          },
          {
            "id": 3,
            "title": "Implement Damerau-Levenshtein Stack Optimized Function",
            "description": "Create a stack-optimized version of the Damerau-Levenshtein distance function using `[usize; 129]` arrays.",
            "dependencies": [
              1
            ],
            "details": "Develop `damerau_levenshtein_stack_optimized()` using stack-allocated buffers. Ensure efficient handling of strings up to 128 characters.",
            "status": "done",
            "testStrategy": "Develop unit tests to ensure performance and correctness compared to existing implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:03.395Z"
          },
          {
            "id": 4,
            "title": "Refactor Existing Functions to Use Stack Optimized Versions",
            "description": "Update existing string similarity functions to utilize the new stack-optimized versions where applicable.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Refactor code to replace existing function calls with stack-optimized versions, ensuring compatibility and performance improvements.",
            "status": "done",
            "testStrategy": "Run integration tests to ensure refactored functions maintain expected behavior and performance.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:03.399Z"
          },
          {
            "id": 5,
            "title": "Benchmark and Validate Overhead Reduction",
            "description": "Measure the overhead reduction achieved by the stack-optimized functions.",
            "dependencies": [
              4
            ],
            "details": "Conduct benchmarks focusing on function call overhead reduction. Validate the expected 10-20% overhead reduction.",
            "status": "done",
            "testStrategy": "Use performance benchmarks to compare with previous implementations and validate overhead reduction.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:03.404Z"
          }
        ],
        "updatedAt": "2025-12-06T02:40:03.404Z"
      },
      {
        "id": "83",
        "title": "Medium String Specialization for Jaro-Winkler (15-30 chars)",
        "description": "Create a specialized function for Jaro-Winkler similarity targeting medium-length strings (15-30 characters) to improve performance.",
        "details": "Implement `jaro_similarity_medium_strings()` optimized for strings between 15 and 30 characters. Use `[bool; 32]` stack-allocated match arrays to fit within an L1 cache line. Integrate inline SIMD character search using `u8x16` vectors and unroll the match-finding loop to enhance performance. This optimization aims to achieve a 15-30% speedup for Jaro-Winkler calculations on typical name and company data.",
        "testStrategy": "Develop unit tests to validate the correctness of `jaro_similarity_medium_strings()` against known Jaro-Winkler values. Benchmark the performance improvement compared to the existing implementation, focusing on datasets with string lengths between 15 and 30 characters. Ensure the function maintains accuracy and achieves the expected speedup.",
        "status": "done",
        "dependencies": [
          "5",
          "31",
          "36"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Jaro-Winkler Function for Medium Strings",
            "description": "Design the specialized function for Jaro-Winkler similarity targeting medium-length strings.",
            "dependencies": [],
            "details": "Outline the function signature and logic for handling strings between 15 and 30 characters. Plan the use of `[bool; 32]` arrays and SIMD vectors.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:16.094Z"
          },
          {
            "id": 2,
            "title": "Implement Stack-Allocated Match Arrays",
            "description": "Implement stack-allocated match arrays using `[bool; 32]`.",
            "dependencies": [
              1
            ],
            "details": "Use stack allocation to create match arrays that fit within an L1 cache line, optimizing for performance.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:16.102Z"
          },
          {
            "id": 3,
            "title": "Integrate SIMD Character Search",
            "description": "Integrate SIMD character search using `u8x16` vectors into the function.",
            "dependencies": [
              2
            ],
            "details": "Use SIMD vectors to perform character searches, enhancing the speed of match finding.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:16.109Z"
          },
          {
            "id": 4,
            "title": "Unroll Match-Finding Loop",
            "description": "Unroll the loop responsible for finding matches to improve performance.",
            "dependencies": [
              3
            ],
            "details": "Optimize the loop by unrolling it, reducing the overhead of loop control and increasing execution speed.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:16.115Z"
          },
          {
            "id": 5,
            "title": "Develop and Run Unit Tests",
            "description": "Develop unit tests to validate the function's correctness and performance.",
            "dependencies": [
              4
            ],
            "details": "Create tests to compare the new function against known Jaro-Winkler values and benchmark performance improvements.",
            "status": "done",
            "testStrategy": "Develop unit tests to validate correctness and benchmark performance improvements.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:16.119Z"
          }
        ],
        "updatedAt": "2025-12-06T02:40:16.119Z"
      },
      {
        "id": "84",
        "title": "AVX-512 16-Wide Vectors for Levenshtein and Damerau-Levenshtein",
        "description": "Implement AVX-512 SIMD optimizations for Levenshtein and Damerau-Levenshtein distance calculations to achieve a 2x speedup on compatible CPUs.",
        "details": "Develop `levenshtein_distance_banded_avx512()` and `damerau_levenshtein_avx512()` functions using `Simd<u32, 16>` vectors to leverage AVX-512 capabilities. Implement runtime CPU feature detection using `is_x86_feature_detected!(\"avx512f\")` to ensure compatibility. Optimize the dynamic programming matrix operations to process 16 elements in parallel, focusing on minimizing data dependencies and maximizing throughput. Ensure backward compatibility by falling back to existing SIMD implementations on non-AVX-512 systems.",
        "testStrategy": "Create unit tests to validate the correctness of the AVX-512 implementations against known Levenshtein and Damerau-Levenshtein distances. Benchmark the performance on AVX-512 capable systems (e.g., Intel Xeon, AMD Zen4+) to confirm the expected 2x speedup. Include tests for edge cases and ensure the fallback mechanism works correctly on non-AVX-512 systems.",
        "status": "done",
        "dependencies": [
          "3",
          "4",
          "32"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement AVX-512 Feature Detection",
            "description": "Develop runtime CPU feature detection for AVX-512.",
            "dependencies": [],
            "details": "Use `is_x86_feature_detected!(\"avx512f\")` to check for AVX-512 support at runtime.",
            "status": "done",
            "testStrategy": "Test on various CPUs to ensure correct detection.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.425Z"
          },
          {
            "id": 2,
            "title": "Develop AVX-512 Optimized Levenshtein Function",
            "description": "Create `levenshtein_distance_banded_avx512()` using AVX-512.",
            "dependencies": [
              1
            ],
            "details": "Implement the function using `Simd<u32, 16>` to process 16 elements in parallel, optimizing the dynamic programming matrix.",
            "status": "done",
            "testStrategy": "Validate against known Levenshtein distances and benchmark performance.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.429Z"
          },
          {
            "id": 3,
            "title": "Develop AVX-512 Optimized Damerau-Levenshtein Function",
            "description": "Create `damerau_levenshtein_avx512()` using AVX-512.",
            "dependencies": [
              1
            ],
            "details": "Implement the function using `Simd<u32, 16>` to handle transpositions and other operations efficiently.",
            "status": "done",
            "testStrategy": "Validate against known Damerau-Levenshtein distances and benchmark performance.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.432Z"
          },
          {
            "id": 4,
            "title": "Optimize Data Dependencies and Throughput",
            "description": "Optimize matrix operations to minimize data dependencies.",
            "dependencies": [
              2,
              3
            ],
            "details": "Focus on reducing data dependencies and maximizing throughput in the AVX-512 implementations.",
            "status": "done",
            "testStrategy": "Benchmark to ensure throughput improvements and check for data dependency issues.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.436Z"
          },
          {
            "id": 5,
            "title": "Ensure Backward Compatibility",
            "description": "Implement fallback to existing SIMD implementations.",
            "dependencies": [
              2,
              3
            ],
            "details": "Ensure that systems without AVX-512 support use existing SIMD implementations for compatibility.",
            "status": "done",
            "testStrategy": "Test on non-AVX-512 systems to ensure correct fallback behavior.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.439Z"
          }
        ],
        "updatedAt": "2025-12-06T02:40:44.439Z"
      },
      {
        "id": "85",
        "title": "Implement SIMD-Based Fuzzy Join Benchmarking",
        "description": "Develop a benchmarking suite to evaluate the performance of SIMD-based fuzzy join implementations.",
        "details": "Create a benchmarking suite to measure the performance of the new SIMD-based fuzzy join functions. Implement benchmarks for `compute_batch_similarities_simd8()`, `jaro_winkler_batch8()`, `levenshtein_batch8()`, and `damerau_levenshtein_batch8()`. Use datasets of varying sizes and string lengths to assess speedup and accuracy compared to non-SIMD implementations. Integrate with existing benchmark infrastructure and document results in a comprehensive report.",
        "testStrategy": "Run benchmarks on datasets ranging from 1K to 10M rows. Compare execution times and accuracy against non-SIMD implementations. Validate that the SIMD implementation achieves the expected 2-4x speedup. Ensure results are consistent with known outputs for accuracy verification. Document findings in a detailed report.",
        "status": "done",
        "dependencies": [
          "81",
          "65",
          "8"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-05T21:05:05.565Z"
      },
      {
        "id": "86",
        "title": "Optimize String Similarity Functions for Large Strings",
        "description": "Implement optimizations for string similarity functions to handle strings larger than 128 characters efficiently.",
        "details": "Develop new functions to handle strings larger than 128 characters using thread-local storage. Create `levenshtein_distance_large()` and `damerau_levenshtein_large()` with dynamic buffer allocation. Ensure these functions are integrated into the main dispatch logic with appropriate length checks. Optimize memory usage and reduce overhead by reusing buffers where possible. Consider using a buffer pool to manage memory efficiently.",
        "testStrategy": "Develop unit tests to ensure the correctness of the new large string functions by comparing results with known outputs. Benchmark the performance against existing implementations to measure overhead reduction. Validate memory usage improvements by monitoring buffer allocation and reuse during tests.",
        "status": "done",
        "dependencies": [
          "82",
          "18"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-05T21:05:06.508Z"
      },
      {
        "id": "87",
        "title": "Integrate Medium String Specialization into Jaro-Winkler Dispatch",
        "description": "Integrate the specialized Jaro-Winkler implementation for medium strings (15-30 chars) into the main dispatch logic.",
        "details": "Update the `jaro_similarity_bytes()` function to incorporate the `jaro_similarity_medium_strings()` for handling strings between 15 and 30 characters. Ensure that the dispatch logic correctly routes strings of this length to the optimized path. Modify the existing match-finding loop to include the SIMD character search and manual unrolling optimizations. Ensure that the integration maintains the expected 15-30% performance improvement for medium-length strings.",
        "testStrategy": "Develop unit tests to verify that strings between 15 and 30 characters are correctly dispatched to the optimized path. Benchmark the integrated function against the previous implementation to confirm the performance gains. Validate the correctness of the results by comparing them with known Jaro-Winkler values for medium-length strings.",
        "status": "done",
        "dependencies": [
          "83",
          "5",
          "31"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-05T21:05:06.512Z"
      },
      {
        "id": "88",
        "title": "Optimize AVX-512 SIMD for Jaro-Winkler Similarity",
        "description": "Implement AVX-512 SIMD optimizations for the Jaro-Winkler similarity function to enhance performance on compatible CPUs.",
        "details": "Develop the `jaro_winkler_similarity_avx512()` function using `Simd<u32, 16>` vectors to leverage AVX-512 capabilities. Implement runtime CPU feature detection using `is_x86_feature_detected!(\"avx512f\")` to ensure compatibility. Optimize the similarity calculation by processing multiple elements in parallel, focusing on minimizing data dependencies and maximizing throughput. Ensure backward compatibility by falling back to existing SIMD implementations on non-AVX-512 systems.",
        "testStrategy": "Create unit tests to validate the correctness of the AVX-512 implementation against known Jaro-Winkler similarity values. Benchmark the performance on AVX-512 capable systems (e.g., Intel Xeon, AMD Zen4+) to confirm the expected speedup. Include tests for edge cases and ensure compatibility with existing results.",
        "status": "done",
        "dependencies": [
          "5",
          "84"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-05T21:05:06.515Z"
      },
      {
        "id": "89",
        "title": "Hybrid Early Termination with Batch SIMD",
        "description": "Enable batch SIMD in the sequential path when early termination is enabled to improve performance significantly.",
        "details": "Modify the `compute_batch_similarities_sequential()` function to incorporate batch SIMD processing. The implementation should process pairs in batches of 8 using SIMD functions, ensuring that early termination conditions are checked after each batch completes. Maintain correctness by ensuring that early termination still functions as intended, leveraging the insight that batch SIMD and early termination can coexist effectively. This requires careful handling of the SIMD operations and ensuring that the logic for early termination does not interfere with the SIMD processing flow.",
        "testStrategy": "Verify the implementation by benchmarking the performance against the previous sequential processing method. Ensure that the results match those of the full computation for correctness. Test edge cases, including varying string lengths and null values, to confirm that the early termination logic works correctly with SIMD. Measure the speedup achieved in early termination scenarios to validate the expected 2-4x performance improvement.",
        "status": "done",
        "dependencies": [
          "59",
          "81"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create compute_batch_similarities_with_early_term_simd() function",
            "description": "Develop a new function that integrates batch SIMD processing with early termination capabilities. This function will serve as the main entry point for computing similarities in batches while checking for early termination conditions.",
            "dependencies": [],
            "details": "The function should be structured to accept input pairs and utilize SIMD instructions to process them in batches of 8. It must also include logic to handle early termination based on the results of the batch processing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:34.197Z"
          },
          {
            "id": 2,
            "title": "Group pairs into batches of 8 for SIMD processing",
            "description": "Implement logic to group similarity pairs into batches of 8 to optimize processing using SIMD functions. This will enhance performance by reducing the number of function calls and leveraging parallel processing capabilities.",
            "dependencies": [],
            "details": "The batching logic should ensure that pairs are correctly grouped and that any remaining pairs (if not a multiple of 8) are handled appropriately. This will require careful indexing and memory management.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:34.204Z"
          },
          {
            "id": 3,
            "title": "Integrate early termination checks after batch computation",
            "description": "Add checks for early termination after each batch of similarities is computed. This will ensure that the function can exit early if the termination conditions are met, thus saving unnecessary computations.",
            "dependencies": [
              1
            ],
            "details": "The integration should be seamless, allowing the function to evaluate the results of each batch and determine if further processing is needed based on the early termination criteria defined in the task.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:34.209Z"
          },
          {
            "id": 4,
            "title": "Handle BestMatch strategy: track best similarity per left index",
            "description": "Implement logic to track the best similarity score for each left index during batch processing. This will be crucial for optimizing the similarity computation and ensuring that the best matches are retained.",
            "dependencies": [
              2,
              3
            ],
            "details": "The implementation should maintain a data structure to store the best similarity scores and update them as new batches are processed. This will involve comparisons and potential updates to the stored values.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:34.214Z"
          },
          {
            "id": 5,
            "title": "Benchmark early termination with/without batch SIMD",
            "description": "Conduct performance benchmarking to compare the new batch SIMD implementation with early termination against the previous sequential processing method. This will help quantify the performance improvements achieved.",
            "dependencies": [
              1,
              3
            ],
            "details": "The benchmarking should include various test cases, including edge cases, to ensure that the new implementation not only performs better but also maintains correctness in results. Metrics to track should include execution time and resource utilization.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:34.224Z"
          },
          {
            "id": 6,
            "title": "Handle FirstMatch strategy: stop after first match per left index",
            "description": "Implement logic to stop processing after finding the first match that meets the threshold for each left index in the batch.",
            "details": "For the FirstMatch strategy, the function should track which left indices have found a match and stop processing additional right strings for those indices once a match is found. This requires maintaining a boolean array or set to track satisfied left indices and checking this after each batch completes.",
            "status": "done",
            "dependencies": [
              4
            ],
            "parentTaskId": 89,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:34.234Z"
          },
          {
            "id": 7,
            "title": "Handle AllMatches with limit: stop after reaching max matches",
            "description": "Implement logic to stop processing when the maximum number of matches (if specified) has been reached across all left indices.",
            "details": "For AllMatches with a limit, track the total number of matches found and stop processing additional batches once the limit is reached. This requires maintaining a counter and checking it after each batch completes.",
            "status": "done",
            "dependencies": [
              4
            ],
            "parentTaskId": 89,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:34.241Z"
          },
          {
            "id": 8,
            "title": "Update compute_batch_similarities_with_termination() to use new function",
            "description": "Refactor the existing compute_batch_similarities_with_termination() function to call the new compute_batch_similarities_with_early_term_simd() function instead of the sequential path.",
            "details": "Replace the sequential processing logic in compute_batch_similarities_with_termination() with a call to the new batch SIMD function. Ensure all termination strategies (BestMatch, FirstMatch, AllMatches) are properly handled through the new function.",
            "status": "done",
            "dependencies": [
              1,
              3,
              4,
              6,
              7
            ],
            "parentTaskId": 89,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:34.247Z"
          }
        ],
        "updatedAt": "2025-12-06T02:40:34.247Z"
      },
      {
        "id": "90",
        "title": "Integrate compute_hamming_batch8() for Hamming Similarity",
        "description": "Replace individual Hamming processing with the existing compute_hamming_batch8() function to enhance performance in batch SIMD operations.",
        "details": "Update the process_simd8_batch() function to utilize compute_hamming_batch8() for Hamming similarity calculations. The implementation should include the following steps:\n- Filter input pairs to ensure they are of equal length, as Hamming similarity requires this.\n- Implement batch SIMD processing when all 8 pairs have equal lengths, leveraging the compute_hamming_batch8() function.\n- For mixed-length batches, ensure a fallback to individual processing is in place.\n- Remove any redundant individual Hamming computations that currently exist in the batch processing path to streamline performance.\n- Ensure that the changes are well-documented and maintainable, following coding standards and best practices for SIMD implementations.",
        "testStrategy": "To verify the implementation, conduct the following tests:\n- Benchmark the performance of the new implementation against the previous individual processing method to quantify speed improvements.\n- Validate the correctness of the Hamming similarity results by comparing outputs with known correct values for various input pairs.\n- Test edge cases, including pairs with differing lengths and null values, to ensure robust handling of all scenarios.\n- Perform regression testing to confirm that existing functionalities remain unaffected by the changes.",
        "status": "done",
        "dependencies": [
          "81",
          "35"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Check for Equal-Length Pairs in process_simd8_batch()",
            "description": "Update the process_simd8_batch() function to ensure that all input pairs are of equal length before processing them for Hamming similarity. This is crucial for the correct application of the compute_hamming_batch8() function.",
            "dependencies": [],
            "details": "Implement a check within the process_simd8_batch() function to filter out pairs that do not have equal lengths. This will involve iterating through the input pairs and validating their lengths before proceeding with any further processing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:42.969Z"
          },
          {
            "id": 2,
            "title": "Implement Batch SIMD Processing with compute_hamming_batch8()",
            "description": "Modify the process_simd8_batch() function to call compute_hamming_batch8() when all input pairs are confirmed to be of equal length. This will enhance performance by utilizing batch processing capabilities.",
            "dependencies": [
              1
            ],
            "details": "Integrate the compute_hamming_batch8() function into the process_simd8_batch() function, ensuring it is only called when the length check has passed. This will involve setting up the necessary parameters for the batch processing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:42.974Z"
          },
          {
            "id": 3,
            "title": "Efficiently Filter Equal-Length Pairs for Batch Processing",
            "description": "Develop an efficient filtering mechanism within the process_simd8_batch() function to handle equal-length pairs for batch processing. This will ensure that only valid pairs are processed together.",
            "dependencies": [
              1
            ],
            "details": "Create a filtering algorithm that efficiently groups input pairs of equal lengths. This may involve using data structures that optimize the grouping process to minimize overhead during batch processing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.002Z"
          },
          {
            "id": 4,
            "title": "Update process_remainder_batch() for Batch Hamming",
            "description": "Revise the process_remainder_batch() function to utilize batch Hamming processing when possible, ensuring that it falls back to individual processing only when necessary.",
            "dependencies": [
              2
            ],
            "details": "Modify the logic within process_remainder_batch() to check if batch processing can be applied. If so, call compute_hamming_batch8() instead of individual processing methods, ensuring a smooth transition between batch and individual processing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.025Z"
          },
          {
            "id": 5,
            "title": "Remove Redundant Individual Hamming Calls",
            "description": "Identify and eliminate any redundant calls to hamming_similarity_direct_bytes() within the batch processing path to streamline performance and reduce unnecessary computations.",
            "dependencies": [
              2
            ],
            "details": "Conduct a thorough review of the batch processing code to locate and remove any instances where hamming_similarity_direct_bytes() is called unnecessarily. This will help in optimizing the overall performance of the batch processing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.049Z"
          },
          {
            "id": 6,
            "title": "Benchmark Hamming with/without batch SIMD",
            "description": "Run performance benchmarks comparing Hamming similarity computation with batch SIMD versus individual processing.",
            "details": "Create benchmark tests that measure execution time and throughput for Hamming similarity calculations. Compare results with and without batch SIMD to quantify the expected 2-3x speedup.",
            "status": "done",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "parentTaskId": 90,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.054Z"
          }
        ],
        "updatedAt": "2025-12-06T02:40:43.054Z"
      },
      {
        "id": "91",
        "title": "Batch SIMD for Candidate Verification Integration",
        "description": "Integrate batch SIMD processing into candidate verification for improved performance in fuzzy joins.",
        "details": "Modify the `compute_fuzzy_matches_from_candidates()` function to implement batch SIMD for candidate verification. Group candidates into batches of 8 and utilize SIMD functions to process these batches efficiently. Ensure that the implementation maintains the order of candidate pairs for accurate result assembly. The integration must support all existing blocking strategies, including FirstNChars, NGram, Length, LSH, and SparseVector. Consider edge cases such as handling remainders when the total number of candidates is not a multiple of 8.",
        "testStrategy": "Verify the correctness of candidate verification by comparing results with non-SIMD implementations. Benchmark performance improvements by measuring execution times for large datasets before and after SIMD integration. Ensure that all blocking strategies yield consistent results and that the order of candidate pairs is preserved. Test edge cases, including varying string lengths and null values, to confirm robustness.",
        "status": "done",
        "dependencies": [
          "54",
          "67",
          "81"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create verify_candidates_batch_simd() function",
            "description": "Develop the verify_candidates_batch_simd() function to handle batch processing of candidate verification using SIMD techniques.",
            "dependencies": [],
            "details": "This function will be responsible for processing batches of candidates using SIMD instructions. It will take an array of candidate pairs and apply the appropriate SIMD operations to verify their similarity efficiently.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.397Z"
          },
          {
            "id": 2,
            "title": "Group candidate pairs into batches of 8",
            "description": "Implement logic to group candidate pairs into batches of 8 for SIMD processing.",
            "dependencies": [],
            "details": "This subtask involves creating a method to iterate through the list of candidates and group them into arrays of 8. This will facilitate efficient processing using SIMD functions.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.402Z"
          },
          {
            "id": 3,
            "title": "Extract string pairs for batch processing",
            "description": "Write a function to extract string pairs from the candidate data for batch processing.",
            "dependencies": [
              2
            ],
            "details": "This function will take the grouped candidate pairs and extract the relevant string data needed for similarity calculations. It will ensure that the data is formatted correctly for SIMD operations.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.406Z"
          },
          {
            "id": 4,
            "title": "Call appropriate batch SIMD function based on similarity type",
            "description": "Implement logic to call the correct SIMD function based on the specified similarity type for each batch.",
            "dependencies": [
              3
            ],
            "details": "This subtask will involve determining the similarity type (e.g., FirstNChars, NGram) and invoking the corresponding SIMD function for each batch of candidates to perform the verification.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.410Z"
          },
          {
            "id": 5,
            "title": "Handle remainder candidates (< 8) efficiently",
            "description": "Develop a strategy to process any remaining candidates that do not fit into a full batch of 8.",
            "dependencies": [
              2
            ],
            "details": "This subtask will ensure that any candidates left over after batching (less than 8) are processed efficiently, possibly by using a fallback non-SIMD method or a smaller SIMD batch.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.413Z"
          },
          {
            "id": 6,
            "title": "Filter candidates above threshold using SIMD threshold filtering",
            "description": "Use SIMD operations to efficiently filter candidate pairs that meet the similarity threshold in parallel.",
            "details": "After computing similarities for a batch, use SIMD comparison operations to filter candidates that are above the threshold. This can be done using SIMD mask operations to identify which pairs meet the threshold condition.",
            "status": "done",
            "dependencies": [
              4
            ],
            "parentTaskId": 91,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.417Z"
          },
          {
            "id": 7,
            "title": "Update compute_fuzzy_matches_from_candidates() to use batch verification",
            "description": "Modify the main candidate verification function to use the new batch SIMD verification instead of individual processing.",
            "details": "Replace the individual candidate processing loop in compute_fuzzy_matches_from_candidates() with calls to verify_candidates_batch_simd(). Ensure the function maintains correct ordering and handles all blocking strategies properly.",
            "status": "done",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "parentTaskId": 91,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.421Z"
          },
          {
            "id": 8,
            "title": "Benchmark blocked joins with/without batch SIMD candidate verification",
            "description": "Run performance benchmarks comparing blocked fuzzy joins using batch SIMD candidate verification versus individual processing.",
            "details": "Create benchmark tests that measure execution time, throughput, and memory usage for blocked joins with various blocking strategies. Compare results with and without batch SIMD to quantify the expected 2-3x speedup.",
            "status": "done",
            "dependencies": [
              8
            ],
            "parentTaskId": 91,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.424Z"
          }
        ],
        "updatedAt": "2025-12-06T02:40:43.424Z"
      },
      {
        "id": "92",
        "title": "Implement 16-Wide Batch SIMD Support for All Similarity Metrics",
        "description": "Extend batch SIMD to utilize 16-wide AVX-512 vectors for all similarity metrics, enhancing performance on compatible CPUs.",
        "details": "1. Create functions: `compute_*_batch16_with_threshold()` for each similarity metric: Jaro-Winkler, Levenshtein, Damerau-Levenshtein, and Hamming.\n2. Implement runtime CPU feature detection using `is_x86_feature_detected!(\"avx512f\")` to determine AVX-512 availability.\n3. Auto-select 16-wide processing when AVX-512 is available; otherwise, fallback to existing 8-wide implementations.\n4. Update batch processing logic to handle 16-pair batches efficiently, ensuring that remainder cases (8-15 pairs) are processed using 8-wide SIMD.\n5. Optimize memory access patterns to maximize throughput and minimize cache misses.\n6. Ensure backward compatibility with existing implementations by maintaining fallback mechanisms.",
        "testStrategy": "1. Develop unit tests for each new function to validate correctness against known similarity values.\n2. Benchmark performance on AVX-512 capable systems (e.g., Intel Xeon, AMD Zen4+) to confirm expected speedup of 1.5-2x.\n3. Include tests for edge cases, such as varying string lengths and null values, to ensure robustness.\n4. Compare results with existing 8-wide implementations to verify accuracy and performance improvements.",
        "status": "done",
        "dependencies": [
          "84",
          "88"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement compute_jaro_winkler_batch16_with_threshold()",
            "description": "Create the function to compute Jaro-Winkler similarity using 16-wide AVX-512 vectors with a threshold for batch processing.",
            "dependencies": [],
            "details": "This function will utilize Simd<f32, 16> to process multiple pairs of strings simultaneously, enhancing performance. Ensure to handle edge cases where input strings may vary in length.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.774Z"
          },
          {
            "id": 2,
            "title": "Implement compute_levenshtein_batch16_with_threshold()",
            "description": "Develop the function to compute Levenshtein distance using 16-wide AVX-512 vectors with a threshold for batch processing.",
            "dependencies": [],
            "details": "This function will leverage Simd<f32, 16> for efficient computation of Levenshtein distance across multiple string pairs. Include checks for varying string lengths to ensure robustness.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.778Z"
          },
          {
            "id": 3,
            "title": "Implement compute_damerau_levenshtein_batch16_with_threshold()",
            "description": "Create the function to compute Damerau-Levenshtein distance using 16-wide AVX-512 vectors with a threshold for batch processing.",
            "dependencies": [],
            "details": "Utilize Simd<f32, 16> to enhance performance for Damerau-Levenshtein calculations. Ensure that the function can handle edge cases, such as empty strings.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.782Z"
          },
          {
            "id": 4,
            "title": "Implement compute_hamming_batch16()",
            "description": "Develop the function to compute Hamming distance using 16-wide AVX-512 vectors for batch processing.",
            "dependencies": [],
            "details": "This function will apply Simd<f32, 16> to efficiently calculate Hamming distances for multiple string pairs. Include validation for input string lengths to ensure they are equal.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.786Z"
          },
          {
            "id": 5,
            "title": "Implement CPU feature detection for AVX-512",
            "description": "Add runtime detection for AVX-512 support using is_x86_feature_detected!() to ensure compatibility.",
            "dependencies": [],
            "details": "This implementation will check if the CPU supports AVX-512 features and set a flag accordingly. This will allow the system to auto-select between 8-wide and 16-wide processing based on availability.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.790Z"
          },
          {
            "id": 6,
            "title": "Create dispatch function to select 8-wide vs 16-wide based on CPU",
            "description": "Implement a dispatch function that automatically selects between 8-wide and 16-wide SIMD based on CPU feature detection.",
            "details": "Create a function that checks for AVX-512 support at runtime and dispatches to the appropriate batch function (8-wide or 16-wide). This should be integrated into the main batch processing pipeline.",
            "status": "done",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "parentTaskId": 92,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.793Z"
          },
          {
            "id": 7,
            "title": "Update process_simd8_batch() to handle 16-wide batches",
            "description": "Modify the batch processing function to support both 8-wide and 16-wide batches based on CPU capabilities.",
            "details": "Update process_simd8_batch() (or rename to process_simd_batch()) to handle both 8-wide and 16-wide batches. The function should use the dispatch logic to select the appropriate SIMD width.",
            "status": "done",
            "dependencies": [
              6
            ],
            "parentTaskId": 92,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.796Z"
          },
          {
            "id": 8,
            "title": "Update batch grouping logic for 16-pair batches",
            "description": "Modify the batch grouping logic to create batches of 16 pairs when AVX-512 is available, falling back to 8-pair batches otherwise.",
            "details": "Update the code that groups string pairs into batches to create 16-pair batches when 16-wide SIMD is available. Ensure remainders (8-15 pairs) are handled efficiently using 8-wide SIMD.",
            "status": "done",
            "dependencies": [
              6
            ],
            "parentTaskId": 92,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.798Z"
          },
          {
            "id": 9,
            "title": "Benchmark AVX-512 vs AVX2 on supported hardware",
            "description": "Run performance benchmarks comparing 16-wide AVX-512 implementations against 8-wide AVX2 implementations on compatible hardware.",
            "details": "Create benchmark tests on AVX-512 capable systems (Intel Xeon, AMD Zen4+) to measure the performance difference between 16-wide and 8-wide implementations. Verify the expected 1.5-2x speedup.",
            "status": "done",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "parentTaskId": 92,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:43.801Z"
          }
        ],
        "updatedAt": "2025-12-06T02:40:43.801Z"
      },
      {
        "id": "93",
        "title": "Optimize Remainder Batch Processing",
        "description": "Implement optimizations for processing remainder pairs using SIMD techniques to enhance performance.",
        "details": "1. For remainders of size 4-7, implement 4-wide or 8-wide SIMD processing with masking to reduce overhead. 2. For remainders of size 1-3, continue processing individually due to high overhead. 3. Utilize SIMD mask operations to handle partial batches effectively. 4. Consider creating specialized functions for 4-wide batch processing of 4-7 remainders. 5. Update the `process_remainder_batch()` function to incorporate these optimizations. 6. Ensure that the implementation is compatible with existing code and adheres to performance benchmarks.",
        "testStrategy": "1. Develop unit tests to validate the correctness of the optimized remainder processing against known outputs. 2. Benchmark the performance of the new implementation against the previous version to measure speedup. 3. Include tests for edge cases, particularly for remainders of sizes 1-3 and 4-7. 4. Monitor memory usage and ensure that the optimizations do not introduce regressions in memory efficiency.",
        "status": "done",
        "dependencies": [
          "91",
          "88",
          "38"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Masked SIMD Operations for Partial Batches",
            "description": "Develop SIMD operations that utilize masking to efficiently process partial batches of remainders, specifically for sizes 4-7.",
            "dependencies": [],
            "details": "This will involve creating SIMD instructions that can handle cases where not all elements in a batch are valid, using mask registers to skip over invalid entries.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.098Z"
          },
          {
            "id": 2,
            "title": "Create 4-Wide Batch Functions for Remainders",
            "description": "Design and implement specialized functions for processing batches of remainders of size 4-7 using 4-wide SIMD techniques.",
            "dependencies": [],
            "details": "These functions should optimize the processing of remainder pairs by leveraging SIMD capabilities to handle multiple pairs simultaneously, reducing overhead.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.102Z"
          },
          {
            "id": 3,
            "title": "Update process_remainder_batch() Function",
            "description": "Modify the existing process_remainder_batch() function to incorporate the newly developed SIMD optimizations for remainder processing.",
            "dependencies": [],
            "details": "Ensure that the updated function can seamlessly integrate the new SIMD paths while maintaining compatibility with existing code and functionality.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.105Z"
          },
          {
            "id": 4,
            "title": "Benchmark Remainder Processing Performance",
            "description": "Conduct performance benchmarks comparing the optimized remainder processing with the previous implementation to evaluate speed improvements.",
            "dependencies": [],
            "details": "This will involve setting up tests to measure execution time and resource usage, ensuring that the optimizations yield the expected performance gains.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.108Z"
          },
          {
            "id": 5,
            "title": "Develop Unit Tests for Optimized Processing",
            "description": "Create comprehensive unit tests to validate the correctness of the optimized remainder processing against known outputs and edge cases.",
            "dependencies": [],
            "details": "The tests should cover various scenarios, including edge cases for remainders of sizes 1-3 and 4-7, ensuring that the optimizations do not introduce errors.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T02:40:44.112Z"
          }
        ],
        "updatedAt": "2025-12-06T02:40:44.112Z"
      },
      {
        "id": "94",
        "title": "Contiguous Memory Layout for String Batches",
        "description": "Implement contiguous memory buffers for string pairs to improve cache locality and reduce pointer indirection. The core implementation is complete, but integration with existing code is deferred.",
        "status": "done",
        "dependencies": [
          "18",
          "65",
          "81"
        ],
        "priority": "high",
        "details": "Create a `ContiguousStringBatch` struct with a single data buffer, offsets, and indices. Pre-allocate the buffer with an estimated total size for all strings. Copy string data into the contiguous buffer for sequential memory access. This approach should be used for batches greater than 32 pairs where the benefit exceeds the copy overhead. The implementation includes a two-pass construction: first pass calculates sizes, second pass copies data. All required methods are implemented, including get(), get_len(), len(), is_empty(), and extract_valid_pairs(). A smart constructor is needed to choose between Standard and Contiguous based on size, and call sites need updating to use this constructor. Expected benefits include better prefetching, reduced pointer chasing, and improved SIMD memory access.",
        "testStrategy": "Benchmark the implementation by comparing the performance of batch processing with and without the contiguous memory layout. Measure cache utilization and speedup, aiming for a 10-20% improvement. Validate correctness by ensuring the output matches the existing implementation for various batch sizes. Integration benchmarks will be deferred.",
        "subtasks": [
          {
            "id": 4,
            "title": "Optimize for Batches Greater than 32 Pairs",
            "description": "Ensure that the contiguous memory layout is only used for batches larger than 32 pairs to maximize performance benefits.",
            "dependencies": [
              3
            ],
            "details": "Implement a check that determines the batch size before deciding to use the contiguous memory layout. If the batch size is less than or equal to 32, fallback to the standard processing method. Integration requires updating call sites to use a smart constructor that chooses between Standard and Contiguous based on size.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark Performance Improvements",
            "description": "Create benchmarks to compare the performance of the new contiguous memory layout against the existing implementation.",
            "dependencies": [
              4
            ],
            "details": "Develop tests that measure cache utilization and processing speed for both implementations. Aim for a 10-20% improvement in performance metrics. Integration benchmarks will be deferred.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 1,
            "title": "Define ContiguousStringBatch Struct",
            "description": "Create the ContiguousStringBatch struct that will hold the data buffer, offsets, and indices for string pairs.",
            "dependencies": [],
            "details": "The struct should include a buffer for string data, an array for offsets to track the start of each string, and an index array for quick access. Ensure the struct is designed for efficient memory access.\n<info added on 2025-12-06T07:37:25.969Z>\nSubtask 94.1 Complete: ContiguousStringBatch struct defined in fuzzy.rs with all required fields:\n- data_buffer: Vec<u8> for contiguous string data\n- offsets: Vec<usize> for string start positions\n- lengths: Vec<usize> for string lengths\n- indices: Vec<usize> for original indices\n- is_null: Vec<bool> for null flags\n- all_ascii: bool for ASCII detection\n- avg_len: usize for average length\n\nStructure is designed for efficient memory access with sequential data layout.\n</info added on 2025-12-06T07:37:25.969Z>",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T07:37:31.914Z"
          },
          {
            "id": 2,
            "title": "Implement Buffer Pre-allocation Logic",
            "description": "Develop the logic to pre-allocate the buffer based on an estimated total size for all strings in the batch.",
            "dependencies": [
              1
            ],
            "details": "Calculate the total size required for the buffer by summing the lengths of all strings in the batch. Implement a function that allocates this buffer dynamically.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T07:37:31.919Z"
          },
          {
            "id": 3,
            "title": "Copy String Data into Contiguous Buffer",
            "description": "Implement functionality to copy string data into the pre-allocated contiguous buffer for sequential access.",
            "dependencies": [
              2
            ],
            "details": "Create a method that iterates through the string pairs and copies them into the contiguous buffer while updating the offsets and indices arrays accordingly.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T07:37:31.923Z"
          }
        ],
        "updatedAt": "2025-12-06T07:37:54.638Z"
      },
      {
        "id": "95",
        "title": "Batch-Level Algorithm Dispatch",
        "description": "Optimize algorithm dispatch by analyzing batch characteristics and selecting the best variant for the entire batch.",
        "details": "Implement a `BatchCharacteristics` struct to analyze batch properties such as `homogeneous_length`, `avg_length`, `all_ascii`, and `high_diversity`. Scan the batch once before processing to determine these characteristics. Develop specialized processing paths: `process_homogeneous_batch()`, `process_short_batch()`, and `process_long_batch()`. Dispatch the optimal algorithm variant once per batch, reducing dispatch calls by 8-16x. This approach is expected to yield a 15-30% speedup by minimizing overhead and enabling batch-specific optimizations. Implement this in `polars/crates/polars-ops/src/frame/join/fuzzy.rs`.",
        "testStrategy": "Benchmark the new batch-level dispatch against the existing per-pair dispatch to measure speedup. Validate the correctness by comparing results with the current implementation. Test with various batch sizes and characteristics to ensure robustness. Include edge cases such as batches with mixed string lengths and character sets.",
        "status": "done",
        "dependencies": [
          "81",
          "43",
          "19"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement BatchCharacteristics Struct",
            "description": "Create a struct named `BatchCharacteristics` to encapsulate properties like `homogeneous_length`, `avg_length`, `all_ascii`, and `high_diversity`. This struct will be used to analyze batch characteristics before processing.",
            "dependencies": [],
            "details": "The struct should include methods to calculate each property based on the input batch data. Ensure it can handle various batch sizes and types effectively.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T07:39:29.591Z"
          },
          {
            "id": 2,
            "title": "Develop Batch Scanning Function",
            "description": "Implement a function to scan the batch and populate the `BatchCharacteristics` struct with the relevant properties. This function should be called before any processing begins.",
            "dependencies": [
              1
            ],
            "details": "The scanning function should iterate through the batch data, calculate the necessary properties, and store them in an instance of `BatchCharacteristics`. Optimize for performance to minimize overhead during batch processing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T07:39:29.597Z"
          },
          {
            "id": 3,
            "title": "Create Specialized Processing Functions",
            "description": "Develop three specialized processing functions: `process_homogeneous_batch()`, `process_short_batch()`, and `process_long_batch()`. Each function should handle its respective batch type based on characteristics.",
            "dependencies": [
              2
            ],
            "details": "Each function should implement the specific logic required for processing its batch type, ensuring they leverage the characteristics determined in the previous step. Consider edge cases for each batch type.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T07:39:29.602Z"
          },
          {
            "id": 4,
            "title": "Implement Algorithm Dispatch Logic",
            "description": "Create the logic to dispatch the optimal algorithm variant based on the characteristics of the batch. This should occur once per batch to reduce dispatch calls significantly.",
            "dependencies": [
              3
            ],
            "details": "The dispatch logic should evaluate the properties of `BatchCharacteristics` and select the appropriate processing function. Ensure that the logic is efficient and minimizes overhead during execution.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T07:39:29.606Z"
          },
          {
            "id": 5,
            "title": "Benchmark and Validate Performance",
            "description": "Conduct benchmarking to compare the new batch-level dispatch against the existing per-pair dispatch. Validate correctness and performance improvements.",
            "dependencies": [
              4
            ],
            "details": "Set up tests to measure execution time and correctness of results between the new and old implementations. Include various batch sizes and characteristics in the tests to ensure robustness and identify edge cases.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T07:39:29.611Z"
          }
        ],
        "updatedAt": "2025-12-06T07:39:29.611Z"
      },
      {
        "id": "96",
        "title": "Aggressive Function Inlining and Call Overhead Reduction",
        "description": "Reduce function call overhead in hot paths through aggressive inlining, macro conversion, and call stack flattening.",
        "details": "- Add #[inline(always)] to all batch SIMD helper functions.\n- Inline small helper functions directly into batch processing loops.\n- Convert repeated patterns to macros instead of function calls.\n- Flatten call stack in similarity computation by reducing intermediate functions.\n- Target functions: process_simd8_batch(), process_remainder_batch(), compute_batch_similarities_simd8_impl().\n- Expected speedup: 5-15% from reduced call overhead.\n- Location: polars/crates/polars-ops/src/frame/join/fuzzy.rs, similarity.rs.",
        "testStrategy": "- Benchmark performance before and after changes to measure speedup.\n- Verify correctness by comparing output with pre-optimization results.\n- Use profiling tools to ensure reduced call overhead and improved inlining.\n- Conduct regression tests on affected modules to ensure no functionality is broken.",
        "status": "done",
        "dependencies": [
          "56",
          "21"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Add #[inline(always)] to SIMD Helper Functions",
            "description": "Implement the #[inline(always)] attribute for all batch SIMD helper functions to encourage aggressive inlining by the compiler.",
            "dependencies": [],
            "details": "Locate all batch SIMD helper functions in the codebase and add the #[inline(always)] attribute to each. This will help reduce function call overhead in performance-critical paths.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:54:49.611Z"
          },
          {
            "id": 2,
            "title": "Inline Small Helper Functions",
            "description": "Identify and inline small helper functions directly into batch processing loops to minimize function call overhead.",
            "dependencies": [
              1
            ],
            "details": "Review the batch processing loops in the target functions and identify small helper functions that can be inlined. Replace calls to these functions with their implementations directly in the loops.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:54:49.621Z"
          },
          {
            "id": 3,
            "title": "Convert Repeated Patterns to Macros",
            "description": "Refactor repeated function call patterns into macros to reduce overhead and improve performance.",
            "dependencies": [
              2
            ],
            "details": "Analyze the code for repeated patterns that can be converted to macros. Create macros for these patterns and replace the function calls with the new macros in the relevant sections of the code.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:54:49.628Z"
          },
          {
            "id": 4,
            "title": "Flatten Call Stack in Similarity Computation",
            "description": "Reduce the number of intermediate functions in the similarity computation to flatten the call stack and improve performance.",
            "dependencies": [
              3
            ],
            "details": "Identify intermediate functions in the similarity computation process. Refactor the code to eliminate unnecessary function calls, integrating their logic directly into the main computation function where feasible.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:54:49.635Z"
          },
          {
            "id": 5,
            "title": "Benchmark and Validate Performance Improvements",
            "description": "Conduct benchmarks to measure performance improvements after inlining and macro conversion.",
            "dependencies": [
              4
            ],
            "details": "Set up benchmarking tests to compare performance before and after the optimizations. Validate that the output remains consistent with pre-optimization results and ensure that the expected speedup of 5-15% is achieved.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:54:49.643Z"
          }
        ],
        "updatedAt": "2025-12-06T08:54:49.643Z"
      },
      {
        "id": "97",
        "title": "SmallVec for Batch Buffers",
        "description": "Replace Vec with SmallVec for batch buffers to optimize memory usage and performance for small-medium batches.",
        "details": "Implement SmallVec in place of Vec for batch buffers in polars-ops. Update polars-ops/Cargo.toml to ensure smallvec is included. Replace Vec<u8> with SmallVec<[u8; 256]> for string data buffers and Vec<(usize, usize, f32)> with SmallVec<[(usize, usize, f32); 32]> for results. This change aims to eliminate heap allocations for batches with 32 or fewer pairs, reducing allocator pressure and improving performance by 5-10% for small-medium batch sizes. Implement these changes in polars/crates/polars-ops/src/frame/join/fuzzy.rs.",
        "testStrategy": "Benchmark the performance of the new SmallVec implementation against the previous Vec implementation. Measure speed improvements for small-medium batch sizes. Validate that no heap allocations occur for batches with 32 or fewer pairs. Ensure correctness by comparing results with the existing implementation for various batch sizes.",
        "status": "done",
        "dependencies": [
          "42",
          "18",
          "56"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Cargo.toml for SmallVec",
            "description": "Modify the polars-ops/Cargo.toml file to include the smallvec crate as a dependency, ensuring that the project can utilize SmallVec for batch buffers.",
            "dependencies": [],
            "details": "Add 'smallvec = \"<version>\"' to the dependencies section of the Cargo.toml file. Ensure that the version is compatible with the existing project setup and run 'cargo build' to verify the changes.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:05.116Z"
          },
          {
            "id": 2,
            "title": "Replace Vec<u8> with SmallVec<[u8; 256]>",
            "description": "Refactor the code in polars-ops to replace instances of Vec<u8> with SmallVec<[u8; 256]> for string data buffers, optimizing memory usage for small-medium batches.",
            "dependencies": [
              1
            ],
            "details": "Locate all occurrences of Vec<u8> in the relevant files and replace them with SmallVec<[u8; 256]>. Ensure that the new implementation maintains the same functionality and performance characteristics.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:05.124Z"
          },
          {
            "id": 3,
            "title": "Replace Vec<(usize, usize, f32)> with SmallVec<[(usize, usize, f32); 32]>",
            "description": "Change the data structure for results in polars-ops from Vec<(usize, usize, f32)> to SmallVec<[(usize, usize, f32); 32]> to enhance performance for small-medium batch sizes.",
            "dependencies": [
              1
            ],
            "details": "Identify all instances of Vec<(usize, usize, f32)> in the codebase and replace them with SmallVec<[(usize, usize, f32); 32]>. Test to ensure that the new structure works correctly with existing logic.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:05.130Z"
          },
          {
            "id": 4,
            "title": "Implement SmallVec changes in fuzzy.rs",
            "description": "Make the necessary code changes in polars/crates/polars-ops/src/frame/join/fuzzy.rs to utilize SmallVec instead of Vec for batch buffers.",
            "dependencies": [
              2,
              3
            ],
            "details": "Edit the fuzzy.rs file to implement the changes made in the previous subtasks. Ensure that all batch buffer operations are compatible with SmallVec and test for any potential issues.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:05.135Z"
          },
          {
            "id": 5,
            "title": "Benchmark performance of SmallVec implementation",
            "description": "Conduct performance benchmarks comparing the new SmallVec implementation against the previous Vec implementation to validate improvements in speed and memory usage.",
            "dependencies": [
              4
            ],
            "details": "Use benchmarking tools to measure the performance of the new SmallVec implementation. Focus on small-medium batch sizes and ensure that no heap allocations occur for batches with 32 or fewer pairs. Document the results for analysis.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:05.140Z"
          }
        ],
        "updatedAt": "2025-12-06T08:56:05.140Z"
      },
      {
        "id": "98",
        "title": "Pre-computed String Length Lookups",
        "description": "Pre-compute and cache string lengths during batch construction to optimize performance.",
        "details": "Implement a `lengths: Vec<usize>` field in both `StringBatch` and `ContiguousStringBatch` structs. During batch construction, iterate over each string once to compute and store its length in the `lengths` vector. Modify `can_reach_threshold()`, algorithm dispatch, and similarity computation functions to utilize these pre-computed lengths instead of calling `.len()` repeatedly. This optimization is particularly beneficial for UTF-8 strings where length computation is not O(1) in bytes, reducing redundant calculations in performance-critical paths.",
        "testStrategy": "Develop unit tests to ensure that the pre-computed lengths match the results of direct `.len()` calls for various string inputs. Benchmark the performance of batch processing with and without pre-computed lengths to measure speedup. Validate the correctness of modified functions by comparing their outputs with the existing implementations.",
        "status": "done",
        "dependencies": [
          "94",
          "95"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Lengths Field in Structs",
            "description": "Add a `lengths: Vec<usize>` field to both `StringBatch` and `ContiguousStringBatch` structs to store pre-computed string lengths.",
            "dependencies": [],
            "details": "Modify the struct definitions in the relevant files to include a new field for storing lengths. Ensure that the field is initialized properly during batch construction.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:59.951Z"
          },
          {
            "id": 2,
            "title": "Compute String Lengths During Batch Construction",
            "description": "Iterate over each string in the batch to compute and store its length in the `lengths` vector during the batch construction process.",
            "dependencies": [
              1
            ],
            "details": "Implement a loop that goes through each string in the batch, calculates its length, and stores it in the `lengths` vector. This should be done in a single pass to optimize performance.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:59.956Z"
          },
          {
            "id": 3,
            "title": "Modify can_reach_threshold() Function",
            "description": "Update the `can_reach_threshold()` function to utilize the pre-computed lengths instead of calling `.len()` repeatedly.",
            "dependencies": [
              2
            ],
            "details": "Refactor the `can_reach_threshold()` function to access the `lengths` vector for string lengths. Ensure that the logic remains intact and performance is improved.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:59.960Z"
          },
          {
            "id": 4,
            "title": "Update Algorithm Dispatch Logic",
            "description": "Modify the algorithm dispatch logic to use pre-computed string lengths from the `lengths` vector.",
            "dependencies": [
              3
            ],
            "details": "Ensure that all relevant parts of the algorithm dispatch logic are updated to reference the `lengths` vector instead of calculating lengths on-the-fly. This will enhance performance in critical paths.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:59.964Z"
          },
          {
            "id": 5,
            "title": "Benchmark and Validate Changes",
            "description": "Develop unit tests to validate that pre-computed lengths are accurate and benchmark performance improvements.",
            "dependencies": [
              4
            ],
            "details": "Create unit tests that compare the pre-computed lengths against direct `.len()` calls for various string inputs. Additionally, benchmark the performance of batch processing with and without pre-computed lengths to measure speedup.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T08:56:59.968Z"
          }
        ],
        "updatedAt": "2025-12-06T08:56:59.968Z"
      },
      {
        "id": "99",
        "title": "Specialized Fast Path for High Thresholds",
        "description": "Implement specialized fast paths optimized for high similarity thresholds (0.9) in fuzzy matching.",
        "status": "deferred",
        "dependencies": [
          "20",
          "56",
          "93"
        ],
        "priority": "medium",
        "details": "The proposed specialized fast paths for high similarity thresholds are already substantially implemented in the existing codebase. Early termination, length-based pre-filtering, and diagonal band optimization for Levenshtein are already present. Further optimizations would add unnecessary complexity without significant benefit. Future enhancements can be made incrementally if profiling indicates bottlenecks.",
        "testStrategy": "Existing unit tests and benchmarks already validate the current optimizations. Future profiling can guide incremental enhancements if needed.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create High Threshold Similarity Functions",
            "description": "Implement specialized *_similarity_high_threshold() functions for each similarity metric to handle high thresholds.",
            "dependencies": [],
            "details": "Develop functions for each metric that will specifically cater to high similarity thresholds (0.9). Ensure they follow the existing function structure for consistency.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Length-Based Pre-Filtering",
            "description": "Create an aggressive length-based pre-filtering mechanism for high similarity thresholds.",
            "dependencies": [
              1
            ],
            "details": "This pre-filtering should quickly eliminate candidates that do not meet the length criteria, optimizing the matching process for high thresholds.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add Early Prefix Match Checks",
            "description": "Integrate early prefix match checks into the high threshold similarity functions.",
            "dependencies": [
              1
            ],
            "details": "These checks will help in quickly identifying potential matches by comparing prefixes of the strings before performing full similarity calculations.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Optimize Levenshtein for Narrow Diagonal Band",
            "description": "Implement a narrower diagonal band for the Levenshtein distance calculation with small max_distance.",
            "dependencies": [
              1
            ],
            "details": "This optimization will focus on reducing the computational overhead for high threshold scenarios, enhancing performance significantly.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Develop High Threshold Fast Path Integration",
            "description": "Integrate all specialized functions and optimizations into a high_threshold_fast_path() function for batch processing.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "This function will utilize the newly created high threshold similarity functions and optimizations to process batches efficiently, dispatching based on thresholds.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "100",
        "title": "Profile-Guided Optimization (PGO) Build Configuration",
        "description": "Configure and document PGO builds for optimal performance through better branch prediction and code layout.",
        "details": "1. Document the PGO build process:\n   - Build with instrumentation using `RUSTFLAGS=\"-Cprofile-generate=/tmp/pgo-data\" cargo build --release`.\n   - Run representative benchmarks to collect profile data.\n   - Build with PGO using `RUSTFLAGS=\"-Cprofile-use=/tmp/pgo-data\" cargo build --release`.\n2. Add a PGO profile to `Cargo.toml` under `[profile.release-pgo]`.\n3. Create a benchmark workload for PGO training covering all metrics and dataset sizes.\n4. Document scenarios where PGO is beneficial despite the extra build time, aiming for a 10-20% speedup from improved branch prediction and code layout.",
        "testStrategy": "1. Verify the PGO build process by checking the generated profile data and ensuring it is used in subsequent builds.\n2. Run benchmarks before and after applying PGO to measure performance improvements.\n3. Confirm that the PGO profile is correctly added to `Cargo.toml` and used during builds.\n4. Review documentation for clarity and completeness, ensuring it covers when PGO is advantageous.",
        "status": "deferred",
        "dependencies": [
          "37",
          "72"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Document PGO Build Process",
            "description": "Create comprehensive documentation for the Profile-Guided Optimization (PGO) build process, including commands and expected outcomes.",
            "dependencies": [],
            "details": "Include steps for building with instrumentation using RUSTFLAGS, running benchmarks to collect profile data, and building with PGO using the collected data.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add PGO Profile to Cargo.toml",
            "description": "Modify the Cargo.toml file to include a new PGO profile under the [profile.release-pgo] section.",
            "dependencies": [
              1
            ],
            "details": "Ensure that the new profile settings are optimized for performance and properly formatted in the Cargo.toml file.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Benchmark Workload for PGO",
            "description": "Develop a benchmark workload that effectively trains the PGO system, covering various metrics and dataset sizes.",
            "dependencies": [
              1
            ],
            "details": "The workload should be representative of typical use cases to ensure accurate profiling and optimization.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Document PGO Benefits and Scenarios",
            "description": "Write documentation outlining scenarios where PGO provides significant performance benefits despite longer build times.",
            "dependencies": [
              1
            ],
            "details": "Focus on achieving a 10-20% speedup through improved branch prediction and code layout, and provide examples.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Verify PGO Build Process",
            "description": "Conduct tests to verify the PGO build process, ensuring that profile data is generated and utilized correctly.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Run benchmarks before and after applying PGO to measure performance improvements and confirm the PGO profile is correctly integrated.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-06T08:57:41.418Z"
      },
      {
        "id": "101",
        "title": "LTO (Link-Time Optimization) Configuration",
        "description": "Enable and document LTO for cross-module optimization and better inlining across crate boundaries.",
        "details": "1. Update Cargo.toml with optimal LTO settings:\n   - Set `lto = \"fat\"` for full LTO across all crates.\n   - Set `codegen-units = 1` for maximum optimization.\n   - Set `opt-level = 3` for maximum optimization level.\n2. Document the trade-offs in BUILD_OPTIMIZATION.md, noting 3-5x longer compile times versus 5-15% faster runtime.\n3. Create a fast development profile for quick iteration.\n4. Verify that LTO is enabled for release builds by checking the build output and ensuring the settings are applied.",
        "testStrategy": "1. Build the project in release mode and verify that LTO settings are applied by checking the build logs.\n2. Benchmark the runtime performance to ensure a 5-15% speedup is achieved.\n3. Validate the documentation for clarity and accuracy, ensuring all trade-offs are clearly explained.\n4. Test the fast development profile to confirm it allows for quicker iteration without LTO.",
        "status": "deferred",
        "dependencies": [
          "14",
          "23"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Cargo.toml for LTO Settings",
            "description": "Modify the Cargo.toml file to enable Link-Time Optimization (LTO) by setting the appropriate parameters for full optimization across all crates.",
            "dependencies": [],
            "details": "Set `lto = \"fat\"`, `codegen-units = 1`, and `opt-level = 3` in the Cargo.toml file to ensure maximum optimization during the build process.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Document LTO Trade-offs in BUILD_OPTIMIZATION.md",
            "description": "Create a detailed section in the BUILD_OPTIMIZATION.md file that explains the trade-offs of enabling LTO, including compile time and runtime performance implications.",
            "dependencies": [
              1
            ],
            "details": "Document the expected 3-5x longer compile times and the 5-15% faster runtime performance when LTO is enabled, ensuring clarity for future developers.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Fast Development Profile",
            "description": "Develop a fast development profile in Cargo to facilitate quicker iterations during the development process while using LTO.",
            "dependencies": [
              1
            ],
            "details": "Add a new profile in Cargo.toml that optimizes for speed during development, allowing for faster compile times without sacrificing too much performance.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Verify LTO Settings in Release Builds",
            "description": "Check the build output of the project to ensure that the LTO settings are correctly applied in release builds.",
            "dependencies": [
              1
            ],
            "details": "Build the project in release mode and inspect the logs to confirm that the LTO settings are active and functioning as intended.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark Performance with LTO Enabled",
            "description": "Conduct performance benchmarks to validate the runtime improvements achieved by enabling LTO, ensuring the expected speedup is realized.",
            "dependencies": [
              2,
              4
            ],
            "details": "Run benchmarks comparing the runtime performance of the application with and without LTO enabled to confirm a 5-15% speedup, documenting the results.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-06T08:57:41.439Z"
      },
      {
        "id": "102",
        "title": "Cache Line Alignment for DP Buffers",
        "description": "Align DP matrix buffers to 64-byte cache lines for optimal memory access and reduced cache misses.",
        "details": "Implement cache line alignment for dynamic programming (DP) buffers by using #[repr(align(64))] for buffer structs. Ensure that DP row arrays start on cache line boundaries to fit each row into whole cache lines, reducing false sharing and improving prefetching. Apply these changes to Levenshtein, Damerau-Levenshtein, and Jaro-Winkler match arrays. Profile cache miss rates using tools like perf or cachegrind to verify improvements. Expected benefits include a 5-10% speedup from reduced cache misses. Implement these changes in the file located at polars/crates/polars-ops/src/chunked_array/strings/similarity.rs.",
        "testStrategy": "Profile the cache miss rates before and after implementing the alignment using perf or cachegrind. Benchmark the performance of the updated DP buffers against the previous implementation to measure speed improvements. Validate the correctness of the alignment by ensuring no functional changes in the output of the similarity functions. Compare performance metrics to confirm the expected 5-10% speedup.",
        "status": "deferred",
        "dependencies": [
          "18",
          "82"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Cache Line Alignment for Levenshtein Buffer",
            "description": "Apply #[repr(align(64))] to the Levenshtein match array buffer struct to ensure it aligns with 64-byte cache lines, optimizing memory access and reducing cache misses.",
            "dependencies": [],
            "details": "Modify the struct definition for the Levenshtein buffer in the specified file to include the alignment attribute. Ensure that the buffer is properly initialized and used throughout the codebase to maintain alignment.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Cache Line Alignment for Damerau-Levenshtein Buffer",
            "description": "Apply #[repr(align(64))] to the Damerau-Levenshtein match array buffer struct for optimal cache line alignment, enhancing performance.",
            "dependencies": [
              1
            ],
            "details": "Update the Damerau-Levenshtein buffer struct in the same file to include the alignment attribute. Verify that all references to this buffer maintain the alignment requirement throughout the implementation.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Cache Line Alignment for Jaro-Winkler Buffer",
            "description": "Ensure the Jaro-Winkler match array buffer struct is aligned to 64-byte cache lines using #[repr(align(64))].",
            "dependencies": [
              2
            ],
            "details": "Modify the Jaro-Winkler buffer struct definition to include the cache line alignment attribute. Check all usages of this buffer to confirm that they adhere to the alignment specifications.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Profile Cache Miss Rates Before Alignment",
            "description": "Use profiling tools like perf or cachegrind to measure cache miss rates for the current implementation of DP buffers before applying alignment changes.",
            "dependencies": [
              3
            ],
            "details": "Run the profiling tools on the existing implementation of the DP buffers to gather baseline cache miss statistics. Document the results for comparison after alignment implementation.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Profile Cache Miss Rates After Alignment",
            "description": "Re-run profiling tools to measure cache miss rates for the DP buffers after implementing cache line alignment to verify performance improvements.",
            "dependencies": [
              4
            ],
            "details": "After implementing the cache line alignment changes, use perf or cachegrind again to profile the DP buffers. Compare the results with the baseline to assess the impact of the alignment changes on cache misses and performance.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-06T08:57:41.444Z"
      },
      {
        "id": "103",
        "title": "Prefetching for Next String Pair",
        "description": "Implement software prefetching to load the next string pair during current computations to hide memory latency.",
        "details": "Use `core::intrinsics::prefetch_read_data()` to prefetch the next string pairs during the current similarity computation. Adjust the prefetch distance based on the complexity of the algorithm. Enable this optimization only for large batches exceeding 32 pairs to ensure the benefits outweigh the overhead. Focus on x86_64 architectures with aggressive hardware prefetchers. Implement this in `polars/crates/polars-ops/src/frame/join/fuzzy.rs`.",
        "testStrategy": "Benchmark the performance with and without prefetching for large batches to measure the expected 5-10% speedup. Validate the correctness by ensuring no change in output results. Use profiling tools to verify reduced memory latency and improved cache utilization.",
        "status": "deferred",
        "dependencies": [
          "56",
          "94",
          "98"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Prefetching Logic",
            "description": "Develop the core logic for prefetching the next string pairs during similarity computations using `core::intrinsics::prefetch_read_data()`.",
            "dependencies": [],
            "details": "This involves identifying the appropriate locations in the code where prefetching should be invoked, specifically during the similarity computation loops. Ensure that the prefetch distance is adjustable based on algorithm complexity.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Determine Prefetch Distance",
            "description": "Analyze the algorithm's complexity to define the optimal prefetch distance for loading string pairs.",
            "dependencies": [
              1
            ],
            "details": "Conduct performance tests to find the best prefetch distance that balances memory latency and computational overhead. This will involve profiling different distances during execution.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Batch Size Condition Implementation",
            "description": "Implement a condition to enable prefetching only for batches exceeding 32 string pairs.",
            "dependencies": [
              1
            ],
            "details": "Modify the prefetching logic to check the size of the current batch before executing the prefetch command. This ensures that prefetching is only applied when beneficial.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate Prefetching into Fuzzy Join",
            "description": "Integrate the prefetching logic into the existing fuzzy join implementation in `fuzzy.rs`.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Ensure that the prefetching is seamlessly integrated into the existing code structure without disrupting current functionality. This will involve modifying the join logic to include the prefetching calls.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark and Validate Prefetching",
            "description": "Conduct benchmarks to compare performance with and without prefetching for large batches, and validate output correctness.",
            "dependencies": [
              4
            ],
            "details": "Use profiling tools to measure memory latency and cache utilization improvements. Ensure that the output results remain unchanged compared to the original implementation without prefetching.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-06T08:57:41.460Z"
      },
      {
        "id": "104",
        "title": "Compile-Time Optimization Flags",
        "description": "Document and configure optimal RUSTFLAGS and compiler settings for maximum performance.",
        "details": "1. Document optimal RUSTFLAGS:\n   - `target-cpu=native`: Utilize all CPU features available on the build machine.\n   - `opt-level=3`: Enable maximum optimization.\n   - `codegen-units=1`: Enhance optimization at the cost of compile time.\n2. Create architecture-specific build scripts:\n   - For x86_64 with AVX-512.\n   - For ARM with NEON.\n3. Document guidelines for using native vs portable builds.\n4. Add a build configuration guide with performance impact data in `BUILD_OPTIMIZATION.md` and build scripts.",
        "testStrategy": "1. Verify the build scripts execute correctly on supported architectures.\n2. Measure performance improvements using benchmarks before and after applying the optimization flags.\n3. Review the documentation for clarity and completeness, ensuring it includes performance impact data and usage scenarios.",
        "status": "deferred",
        "dependencies": [
          "37",
          "72"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Document Optimal RUSTFLAGS",
            "description": "Create a comprehensive document outlining the optimal RUSTFLAGS for performance, including `target-cpu=native`, `opt-level=3`, and `codegen-units=1`.",
            "dependencies": [],
            "details": "The document should clearly explain each RUSTFLAG, its purpose, and the expected impact on performance. Include examples of how to apply these flags in a Rust project.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create x86_64 Build Script",
            "description": "Develop a build script specifically for x86_64 architecture that utilizes AVX-512 instructions for enhanced performance.",
            "dependencies": [],
            "details": "The script should automate the build process while applying the optimal RUSTFLAGS and ensuring compatibility with AVX-512. Include comments for clarity.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create ARM Build Script",
            "description": "Develop a build script for ARM architecture that incorporates NEON optimizations for improved performance.",
            "dependencies": [],
            "details": "This script should follow a similar structure to the x86_64 script, applying the optimal RUSTFLAGS and ensuring compatibility with NEON. Provide detailed comments for future reference.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Document Native vs Portable Builds",
            "description": "Draft guidelines explaining the differences and use cases for native versus portable builds in Rust.",
            "dependencies": [],
            "details": "The guidelines should cover scenarios where each build type is preferable, including performance implications and compatibility considerations. Include examples to illustrate key points.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add Build Configuration Guide",
            "description": "Create a `BUILD_OPTIMIZATION.md` file that includes a comprehensive guide on build configurations and their performance impacts.",
            "dependencies": [],
            "details": "This guide should summarize the optimal RUSTFLAGS, architecture-specific scripts, and the guidelines for native vs portable builds. Include performance data and benchmarks to support the recommendations.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-06T08:57:41.464Z"
      },
      {
        "id": "105",
        "title": "On-the-Fly Vectorization for Medium Datasets",
        "description": "Implement streaming sparse vectorization as a lightweight alternative to pre-computed indices for medium-sized datasets (100K-1M rows).",
        "details": "Create a `StreamingVectorizer` struct that vectorizes strings on-the-fly during similarity computation. Pre-compute left vectors once and vectorize right strings on-demand. Introduce a `vectorization_mode` parameter with options: 'indexed', 'streaming', and 'auto', which automatically selects the mode based on dataset size and reuse pattern. Key implementation steps include: 1. Implement the `StreamingVectorizer` struct with on-the-fly n-gram generation. 2. Create the `compute_streaming_similarities()` function to handle similarity calculations. 3. Develop a cost model to choose between indexed and streaming modes. 4. Integrate an auto-selector based on dataset characteristics. 5. Benchmark the performance of indexed vs streaming on medium datasets (100K-1M rows). This approach aims to reduce memory overhead and improve speed for medium datasets where index building costs exceed query savings.",
        "testStrategy": "Develop unit tests to validate the correctness of the `StreamingVectorizer` and `compute_streaming_similarities()` functions against known similarity values. Benchmark the performance of both indexed and streaming modes on medium datasets to measure memory usage and speed improvements. Ensure that the implementation adheres to expected performance metrics of 10-20% memory reduction and 5-15% speedup.",
        "status": "done",
        "dependencies": [
          "73"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement StreamingVectorizer Struct",
            "description": "Create the StreamingVectorizer struct that will handle on-the-fly n-gram generation for string vectorization during similarity computations.",
            "dependencies": [],
            "details": "The StreamingVectorizer struct should include methods for generating n-grams from input strings and storing them in a way that allows for efficient retrieval during similarity calculations. Ensure that the struct is designed to handle medium-sized datasets effectively.",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2025-12-06T19:18:42.514Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop compute_streaming_similarities Function",
            "description": "Create the compute_streaming_similarities function to calculate similarity scores using the StreamingVectorizer.",
            "dependencies": [
              1
            ],
            "details": "This function will utilize the StreamingVectorizer to compute similarity scores between two sets of strings. It should handle edge cases and ensure that the calculations are efficient for medium datasets.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T19:18:42.521Z"
          },
          {
            "id": 3,
            "title": "Create Cost Model for Vectorization Modes",
            "description": "Develop a cost model to determine when to use indexed vs streaming vectorization modes based on dataset characteristics.",
            "dependencies": [
              1
            ],
            "details": "The cost model should analyze factors such as dataset size, memory usage, and query performance to decide the optimal vectorization mode. This will involve creating algorithms that can evaluate these parameters dynamically.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T19:18:42.527Z"
          },
          {
            "id": 4,
            "title": "Integrate Auto-Selector for Vectorization Mode",
            "description": "Implement an auto-selector that chooses the vectorization mode based on dataset characteristics and reuse patterns.",
            "dependencies": [
              3
            ],
            "details": "The auto-selector should evaluate the dataset's size and access patterns to automatically select between indexed, streaming, or auto modes. This will enhance the flexibility and efficiency of the vectorization process.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T19:18:42.531Z"
          },
          {
            "id": 5,
            "title": "Benchmark Performance of Vectorization Modes",
            "description": "Conduct benchmarks to compare the performance of indexed vs streaming vectorization modes on medium datasets.",
            "dependencies": [
              2,
              4
            ],
            "details": "Set up a series of tests to measure memory usage and speed improvements when using both indexed and streaming modes. Analyze the results to validate the effectiveness of the new implementation for medium datasets.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-06T19:19:17.920Z"
          }
        ],
        "updatedAt": "2025-12-06T19:19:17.920Z"
      },
      {
        "id": "106",
        "title": "U16 Sparse Matrix Storage for Memory Efficiency",
        "description": "Implement U16 sparse matrix storage to enhance memory efficiency and performance in sparse vector operations.",
        "details": "Create a SparseVectorStorage enum with U16 and F32 variants. Use the U16 variant for integer term counts when normalization is not required, effectively reducing memory usage by 2x. Implement the dot_product_u16() function for integer dot products and add SIMD support using pmaddwd for x86 or similar instructions for ARM architectures. Ensure conversion to f32 occurs only at the final step to maintain efficiency. Include auto-selection logic based on normalization requirements and benchmark memory usage and cache performance to validate improvements.",
        "testStrategy": "Verify the implementation by running benchmarks that compare memory usage and performance against previous implementations. Ensure that the dot_product_u16() function produces correct results and that SIMD optimizations yield the expected speedup. Document findings and performance metrics to confirm the anticipated 50% memory reduction and 20-30% speedup.",
        "status": "done",
        "dependencies": [
          "73",
          "74"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create SparseVectorStorage Enum",
            "description": "Define a SparseVectorStorage enum with U16 and F32 variants for efficient storage.",
            "dependencies": [],
            "details": "Implement the enum in the codebase, ensuring it supports both U16 and F32 variants for different storage needs.",
            "status": "done",
            "testStrategy": "Verify enum functionality through unit tests ensuring correct variant selection.",
            "updatedAt": "2025-12-06T19:22:25.000Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement dot_product_u16 Function",
            "description": "Develop the dot_product_u16 function for integer dot products using U16 storage.",
            "dependencies": [
              1
            ],
            "details": "Write the function to perform dot products on U16 data, leveraging integer arithmetic for efficiency.",
            "status": "done",
            "testStrategy": "Test with various U16 vectors to ensure correct dot product results.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T19:22:25.008Z"
          },
          {
            "id": 3,
            "title": "Add SIMD Support for dot_product_u16",
            "description": "Enhance dot_product_u16 with SIMD support using pmaddwd for x86 or similar for ARM.",
            "dependencies": [
              2
            ],
            "details": "Integrate SIMD instructions to optimize dot product calculations on supported architectures.",
            "status": "done",
            "testStrategy": "Benchmark SIMD-enhanced function against non-SIMD to verify performance gains.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T19:22:25.012Z"
          },
          {
            "id": 4,
            "title": "Implement Auto-Selection Logic",
            "description": "Create logic to auto-select storage variant based on normalization requirements.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop a mechanism to choose between U16 and F32 based on whether normalization is needed.",
            "status": "done",
            "testStrategy": "Test auto-selection logic with scenarios requiring and not requiring normalization.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T19:22:25.017Z"
          },
          {
            "id": 5,
            "title": "Benchmark Memory and Cache Performance",
            "description": "Conduct benchmarks to validate memory usage and cache performance improvements.",
            "dependencies": [
              3,
              4
            ],
            "details": "Run comprehensive benchmarks comparing new implementation against previous versions to measure efficiency gains.",
            "status": "done",
            "testStrategy": "Document benchmark results and ensure they meet expected improvements in memory and cache performance.",
            "parentId": "undefined",
            "updatedAt": "2025-12-06T19:22:25.022Z"
          }
        ],
        "updatedAt": "2025-12-06T19:22:25.022Z"
      },
      {
        "id": "107",
        "title": "Top-N Heap-Based Sparse Matrix Multiplication",
        "description": "Implement specialized sparse matrix multiplication that computes top-N matches per row without materializing the full similarity matrix.",
        "details": "Create the `sparse_top_n_per_row()` function using a BinaryHeap to maintain a min-heap of size `top_n` for each left row, ensuring only the top-N highest similarities are kept. This approach will optimize memory usage to O(n  top_n) instead of O(n  m). Utilize the Rayon library for parallel processing of rows to enhance performance. Integrate this functionality with the existing `compute_fuzzy_matches()` function to support the 'keep=\"best\"' strategy. Benchmark the new implementation against the full matrix approach for various values of N to validate performance improvements.",
        "testStrategy": "Verify the implementation by running benchmarks comparing memory usage and execution time against the full similarity matrix approach. Ensure that the `sparse_top_n_per_row()` function produces correct results by validating against known outputs for various input scenarios. Document performance metrics and findings in a report to assess the impact of the new implementation.",
        "status": "done",
        "dependencies": [
          "73"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement sparse_top_n_per_row() Function",
            "description": "Create the sparse_top_n_per_row() function using a BinaryHeap to maintain a min-heap of size top_n for each left row.",
            "dependencies": [],
            "details": "Develop the function to optimize memory usage to O(n  top_n) instead of O(n  m). Ensure the function keeps only the top-N highest similarities per row.",
            "status": "pending",
            "testStrategy": "Verify correctness by comparing results with known outputs.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate with compute_fuzzy_matches()",
            "description": "Integrate sparse_top_n_per_row() with the existing compute_fuzzy_matches() function to support the 'keep=\"best\"' strategy.",
            "dependencies": [
              1
            ],
            "details": "Modify compute_fuzzy_matches() to utilize sparse_top_n_per_row() for the 'keep=\"best\"' option, ensuring seamless integration.",
            "status": "pending",
            "testStrategy": "Test integration by running scenarios that require 'keep=\"best\"' and validate outputs.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Parallel Processing with Rayon",
            "description": "Utilize the Rayon library for parallel processing of rows in sparse_top_n_per_row().",
            "dependencies": [
              1
            ],
            "details": "Incorporate Rayon to enhance performance by processing rows in parallel, leveraging multi-core systems.",
            "status": "pending",
            "testStrategy": "Benchmark execution time with and without parallel processing to measure performance gains.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Benchmark and Validate Performance",
            "description": "Benchmark the new implementation against the full matrix approach for various values of N to validate performance improvements.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Conduct benchmarks to compare memory usage and execution time. Document performance improvements and validate correctness.",
            "status": "pending",
            "testStrategy": "Run benchmarks and compare results with the full matrix approach, ensuring performance gains and correctness.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-06T19:51:56.649Z"
      },
      {
        "id": "108",
        "title": "Dynamic Parallelization Axis Selection",
        "description": "Automatically select the parallelization axis for fuzzy joins based on DataFrame size asymmetry to optimize performance.",
        "details": "Implement the Dynamic Parallelization Axis Selection feature by creating a ParallelizationAxis enum with values Left, Right, and Auto. Develop selection logic that evaluates the size ratio of the left and right DataFrames, considering normalization requirements. If the right DataFrame is significantly larger (e.g., 10x) and no normalization is needed, the system should default to parallelizing over the right DataFrame. Implement the compute_fuzzy_matches_parallel_right() function to handle parallel processing for the right DataFrame and build a left-side blocking index to facilitate this. Ensure the auto-selector integrates seamlessly with the existing fuzzy join logic and benchmark the implementation on asymmetric datasets to validate performance improvements.",
        "testStrategy": "Develop unit tests to validate the correctness of the ParallelizationAxis selection logic and the compute_fuzzy_matches_parallel_right() function. Benchmark the performance on various asymmetric datasets (e.g., 100  100K, 100K  100) to ensure a 20-40% speedup is achieved. Validate that the left-side blocking index is correctly built and utilized during parallelization. Ensure comprehensive coverage of edge cases in the tests.",
        "status": "done",
        "dependencies": [
          "55"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-06T19:53:17.057Z"
      },
      {
        "id": "109",
        "title": "Zero-Copy Arrow String Access",
        "description": "Implement zero-copy access to Arrow string buffers to eliminate conversion overhead and improve performance.",
        "details": "Create an `ArrowStringBatch` struct that references Arrow buffers directly, allowing zero-copy access. Implement the `get_unchecked()` accessor for direct buffer access, including offsets, data, and null_bitmap. Integrate this with the existing batch processing infrastructure to ensure seamless operation without intermediate string reference creation. Update SIMD functions to accept Arrow batches and handle nulls using bitmap access. Benchmark the new implementation against the current approach to measure performance improvements.",
        "testStrategy": "Develop unit tests to verify the correctness of zero-copy accessors and ensure they match expected outputs. Benchmark the performance against the existing string reference approach to confirm the expected 10-20% speedup. Validate memory usage improvements by monitoring allocations and cache locality during tests.",
        "status": "done",
        "dependencies": [
          "94"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-06T19:54:37.765Z"
      },
      {
        "id": "110",
        "title": "Compile-Time SIMD Width Selection via Feature Flags",
        "description": "Implement compile-time SIMD width selection using feature flags to optimize performance by eliminating runtime checks.",
        "details": "1. Add SIMD feature flags to Cargo.toml: simd_avx2, simd_avx512, simd_neon, simd_sve.\n2. Define compile-time SIMD width constants based on these feature flags.\n3. Update batch processing functions to utilize compile-time SIMD widths for enhanced performance.\n4. Allow distribution of multiple binaries targeting different CPU architectures.\n5. Retain runtime detection as a fallback for dynamic builds.\n6. Document the build process for different SIMD targets in `BUILD_OPTIMIZATION.md`.",
        "testStrategy": "1. Verify that the feature flags are correctly added and recognized in Cargo.toml.\n2. Ensure that the compile-time constants are correctly defined and used in batch functions.\n3. Benchmark performance improvements by comparing compile-time and runtime dispatch.\n4. Validate that multiple binaries are correctly generated for different CPU architectures.\n5. Review documentation for clarity and completeness, ensuring it covers all SIMD targets.",
        "status": "done",
        "dependencies": [
          "90",
          "104"
        ],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2025-12-06T19:55:52.231Z"
      },
      {
        "id": "111",
        "title": "Cache-Oblivious Algorithm for Very Large Matrices",
        "description": "Implement a recursive cache-oblivious sparse matrix multiplication algorithm for datasets that exceed cache size.",
        "details": "Develop a recursive divide-and-conquer algorithm that optimizes sparse matrix multiplication by leveraging cache oblivious techniques. The implementation will include:\n- A base case where blocks fit in L3 cache (approximately 1MB or 1K pairs).\n- Recursive division of the larger dimension to ensure efficient processing.\n- Integration of natural parallelization using `rayon::join` to enhance performance.\n- Tuning of the base case size based on the actual L3 cache size to maximize efficiency.\n- Integration with fuzzy join operations for handling large datasets effectively.\n- Benchmarking on billion-scale datasets to validate performance improvements.\n\nSubtasks include:\n1. Implement the recursive cache-oblivious algorithm.\n2. Create a direct computation function for the base case.\n3. Tune the base case threshold based on cache size.\n4. Integrate with fuzzy join for large datasets.\n5. Conduct benchmarks on datasets with over 1 billion comparisons.",
        "testStrategy": "To ensure the correctness and performance of the implementation, the following testing strategies will be employed:\n- Unit tests for each subtask to validate individual components of the algorithm.\n- Integration tests to verify the interaction between the cache-oblivious algorithm and fuzzy join operations.\n- Performance benchmarks comparing the new implementation against existing methods on billion-scale datasets, targeting a 15-30% speedup. \n- Stress tests with varying dataset sizes and characteristics to ensure robustness and efficiency under different conditions.",
        "status": "done",
        "dependencies": [
          "107"
        ],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2025-12-06T19:56:55.870Z"
      },
      {
        "id": "112",
        "title": "Hybrid Dense/Sparse Vector Representation",
        "description": "Implement a hybrid vector representation that automatically switches between dense and sparse formats based on vector density to optimize performance.",
        "details": "1. Create a VectorStorage enum with Sparse and Dense variants. 2. Define a density threshold of 30% of possible n-grams or more than 1000 unique n-grams. 3. Implement the dense dot product optimized path for high-density vectors, ensuring O(1) lookup times. 4. Develop a density analysis function to evaluate vector density and select the appropriate representation. 5. Add auto-selection logic to switch between dense and sparse representations based on the defined threshold. 6. Benchmark the performance of dense vs sparse representations across various string types to validate the expected 2-3x speedup for high-density cases. Ensure that the implementation is modular and adheres to existing coding standards.",
        "testStrategy": "1. Create unit tests for the VectorStorage enum to ensure correct instantiation and behavior. 2. Validate the density analysis function with various input cases to confirm accurate density calculations. 3. Benchmark the performance of dense and sparse representations using a variety of string lengths and compositions, documenting the results. 4. Ensure that the auto-selection logic functions correctly by testing edge cases around the density threshold. 5. Review and validate the correctness of the dense dot product implementation against known outputs.",
        "status": "done",
        "dependencies": [
          "106"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-06T19:57:57.644Z"
      },
      {
        "id": "113",
        "title": "Quick Win Optimizations for polars-distance Plugin",
        "description": "Implement high-impact, low-complexity optimizations for the polars-distance plugin to achieve significant performance improvements.",
        "details": "Focus on ARM NEON and FMA instruction optimizations, PGO builds, and a custom allocator. Implement ARM NEON optimizations to achieve 2-4x speedup on Apple Silicon by vectorizing key computational paths. Use FMA instructions to enhance cosine similarity calculations, targeting a 20-40% improvement. Enable Profile-Guided Optimization (PGO) to achieve a 10-25% performance boost by compiling with runtime profiling data. Develop a custom memory allocator to reduce fragmentation and improve allocation speed, aiming for a 5-15% performance gain. Integrate these optimizations into the existing codebase, ensuring compatibility with current functionality.",
        "testStrategy": "Benchmark the performance improvements on ARM and x86 architectures. Validate ARM NEON and FMA optimizations by comparing execution times before and after implementation. Use a variety of datasets to ensure consistent performance gains. Test PGO builds by measuring performance improvements with and without profiling data. Verify the custom allocator's impact on memory usage and allocation speed through detailed profiling. Ensure all optimizations maintain existing functionality and accuracy by running the comprehensive test suite from Task 13.",
        "status": "done",
        "dependencies": [
          "6",
          "13",
          "109"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement ARM NEON SIMD Vectorization",
            "description": "Optimize key computational paths using ARM NEON SIMD instructions for Apple Silicon.",
            "dependencies": [],
            "details": "Focus on vectorizing loops and operations in the polars-distance plugin to achieve a 2-4x speedup on Apple Silicon.",
            "status": "done",
            "testStrategy": "Benchmark execution times before and after optimization on Apple Silicon.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:10:19.251Z"
          },
          {
            "id": 2,
            "title": "Enhance Cosine Similarity with FMA Instructions",
            "description": "Use FMA instructions to improve the performance of cosine similarity calculations.",
            "dependencies": [
              1
            ],
            "details": "Integrate FMA instructions into the cosine similarity function to target a 20-40% performance improvement.",
            "status": "done",
            "testStrategy": "Compare cosine similarity execution times before and after FMA integration.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:10:19.258Z"
          },
          {
            "id": 3,
            "title": "Enable Profile-Guided Optimization (PGO)",
            "description": "Configure the build system to support PGO for runtime profiling and optimization.",
            "dependencies": [
              1,
              2
            ],
            "details": "Set up the build process to collect profiling data and use it to optimize the final build, aiming for a 10-25% performance boost.",
            "status": "done",
            "testStrategy": "Measure performance improvements with and without PGO using profiling data.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:10:19.266Z"
          },
          {
            "id": 4,
            "title": "Develop Custom Memory Allocator",
            "description": "Create a custom memory allocator to reduce fragmentation and improve allocation speed.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement a memory allocator tailored to the plugin's needs, targeting a 5-15% performance gain.",
            "status": "done",
            "testStrategy": "Test allocation speed and fragmentation before and after using the custom allocator.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:10:19.272Z"
          }
        ],
        "updatedAt": "2025-12-08T10:10:19.272Z"
      },
      {
        "id": "114",
        "title": "Core Performance Optimizations for polars-distance",
        "description": "Implement advanced algorithmic optimizations to significantly enhance the performance of polars-distance.",
        "details": "This task focuses on implementing several advanced optimizations to improve the performance of polars-distance. The key areas include:\n\n1. **Vectorized Threshold Filtering**: Implement vectorized operations to filter queries based on thresholds, aiming for a 20-40% speedup.\n\n2. **Branchless Implementations**: Refactor code to minimize branching, enhancing CPU pipelining and achieving a 10-20% speedup.\n\n3. **Advanced Multi-Level Prefetching**: Implement prefetching strategies to hide memory latency, targeting a 10-20% improvement.\n\n4. **Loop Fusion for Multiple Metrics**: Optimize loops to compute multiple metrics simultaneously, achieving a 30-50% speedup when computing two or more metrics.\n\n5. **Specialized Allocators**: Develop custom memory allocators to reduce fragmentation and improve allocation speed, providing an additional 5-10% performance gain.\n\nThese optimizations build on the improvements from Phase 13, targeting an overall 30-60% performance boost.",
        "testStrategy": "Develop comprehensive benchmarks to measure the performance improvements of each optimization. Validate the vectorized threshold filtering and branchless implementations by comparing execution times before and after optimization. Use datasets of varying sizes to test multi-level prefetching and loop fusion effectiveness. Ensure specialized allocators reduce memory fragmentation and improve allocation speed. Document all performance metrics and verify that the cumulative optimizations achieve the targeted 30-60% improvement over Phase 13.",
        "status": "done",
        "dependencies": [
          "113",
          "109",
          "13"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze Current Filtering Mechanism",
            "description": "Review the existing threshold filtering implementation.",
            "dependencies": [],
            "details": "Examine the current codebase to understand how threshold filtering is implemented and identify areas for vectorization.",
            "status": "done",
            "testStrategy": "Compare execution times before and after changes.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:20:21.117Z"
          },
          {
            "id": 2,
            "title": "Design Vectorized Filtering Approach",
            "description": "Create a design for implementing vectorized threshold filtering.",
            "dependencies": [
              1
            ],
            "details": "Develop a plan to replace scalar operations with vectorized ones, focusing on SIMD instructions.",
            "status": "done",
            "testStrategy": "Ensure the design supports expected performance improvements.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:20:21.122Z"
          },
          {
            "id": 3,
            "title": "Implement Vectorized Filtering",
            "description": "Develop the vectorized threshold filtering based on the design.",
            "dependencies": [
              2
            ],
            "details": "Write code to implement the vectorized filtering using SIMD operations, ensuring compatibility with existing data structures.",
            "status": "done",
            "testStrategy": "Run benchmarks to measure performance gains.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:20:21.130Z"
          },
          {
            "id": 4,
            "title": "Optimize Data Structures for Vectorization",
            "description": "Modify data structures to support efficient vectorized operations.",
            "dependencies": [
              3
            ],
            "details": "Refactor data structures to align with memory boundaries and optimize for SIMD processing.",
            "status": "done",
            "testStrategy": "Test for memory alignment and access speed improvements.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:20:21.140Z"
          },
          {
            "id": 5,
            "title": "Validate and Benchmark Vectorized Filtering",
            "description": "Test the new vectorized filtering implementation.",
            "dependencies": [
              4
            ],
            "details": "Conduct thorough testing and benchmarking to validate performance improvements and correctness of the implementation.",
            "status": "done",
            "testStrategy": "Use datasets of varying sizes to ensure robustness and measure speedup.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:20:21.148Z"
          }
        ],
        "updatedAt": "2025-12-08T10:20:21.148Z"
      },
      {
        "id": "115",
        "title": "Multiple Accumulators for Cosine Similarity ILP",
        "description": "Implement multiple accumulators to enhance instruction-level parallelism in cosine similarity calculations.",
        "details": "To improve the performance of cosine similarity calculations, modify the existing SIMD code to use four independent accumulators. This change aims to hide FMA latency and achieve better instruction-level parallelism. Specifically, replace the single accumulator chains (dot_acc, norm_a_acc, norm_b_acc) with four accumulators each, resulting in a total of 12 accumulators. Process SIMD chunks in groups of four and use a step_by(4) approach in the main loop. At the end of the loop, reduce all accumulators by combining them: dot = (dot0 + dot1) + (dot2 + dot3). Handle any remaining elements that are not divisible by 4*SIMD_WIDTH using the existing single-accumulator approach. Implement these changes in the following functions: dot_product_and_norms_avx2_fma(), dot_product_and_norms_avx512(), and dot_product_and_norms_neon() located in polars/crates/polars-ops/src/chunked_array/array/similarity.rs.",
        "testStrategy": "Verify that the numerical results of the modified implementation match the existing implementation. Benchmark the performance on vectors ranging from 64 to 4096 elements to ensure a 2-3x speedup for vectors larger than 128 elements. Use performance profiling tools to confirm reduced FMA latency and improved throughput.",
        "status": "done",
        "dependencies": [
          "6",
          "25",
          "30"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement 4-accumulator version of dot_product_and_norms_avx2_fma()",
            "description": "Modify the function to use four independent accumulators with f64x4 vectors.",
            "dependencies": [],
            "details": "Update the dot_product_and_norms_avx2_fma() function to replace single accumulator chains with four accumulators each. Use f64x4 vectors to process SIMD chunks in groups of four and apply a step_by(4) approach in the main loop.",
            "status": "pending",
            "testStrategy": "Verify numerical results match existing implementation and benchmark performance.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement 4-accumulator version of dot_product_and_norms_avx512()",
            "description": "Modify the function to use four independent accumulators with f64x8 vectors.",
            "dependencies": [
              1
            ],
            "details": "Update the dot_product_and_norms_avx512() function to replace single accumulator chains with four accumulators each. Use f64x8 vectors to process SIMD chunks in groups of four and apply a step_by(4) approach in the main loop.",
            "status": "pending",
            "testStrategy": "Verify numerical results match existing implementation and benchmark performance.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement 4-accumulator version of dot_product_and_norms_neon()",
            "description": "Modify the function to use four independent accumulators with f64x2 vectors.",
            "dependencies": [
              1
            ],
            "details": "Update the dot_product_and_norms_neon() function to replace single accumulator chains with four accumulators each. Use f64x2 vectors to process SIMD chunks in groups of four and apply a step_by(4) approach in the main loop.",
            "status": "pending",
            "testStrategy": "Verify numerical results match existing implementation and benchmark performance.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add handling for vectors not divisible by 4*SIMD_WIDTH",
            "description": "Implement remainder processing for vectors not divisible by 4*SIMD_WIDTH.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Ensure that any remaining elements that are not divisible by 4*SIMD_WIDTH are processed using the existing single-accumulator approach.",
            "status": "pending",
            "testStrategy": "Verify that remainder processing produces correct results.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark single vs 4-accumulator on various vector sizes",
            "description": "Conduct performance benchmarking for single vs 4-accumulator implementations.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Benchmark the performance on vectors of sizes 64, 128, 256, 512, 1024, and 4096. Compare the speedup achieved with the 4-accumulator approach.",
            "status": "pending",
            "testStrategy": "Use performance profiling tools to confirm reduced FMA latency and expected speedup.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T08:45:08.938Z"
      },
      {
        "id": "116",
        "title": "Direct f32 SIMD Path for Embeddings",
        "description": "Implement a direct f32 SIMD path to optimize embedding vector operations by avoiding f64 casting overhead.",
        "details": "To enhance performance for f32 embedding vectors, implement a direct SIMD path that utilizes native f32 operations. This involves detecting f32 data types and applying SIMD operations specific to f32:\n1. Implement `dot_product_and_norms_f32_avx2()` using f32x8 vectors for AVX2.\n2. Implement `dot_product_and_norms_f32_avx512()` using f32x16 vectors for AVX-512.\n3. Implement `dot_product_and_norms_f32_neon()` using f32x4 vectors for ARM NEON.\n4. Modify `compute_cosine_similarity()` to detect f32 dtype and select the appropriate SIMD path.\n5. Integrate the multi-accumulator technique from Task 115 to enhance instruction-level parallelism for f32 paths.\nPlace the implementation in `polars/crates/polars-ops/src/chunked_array/array/similarity.rs`.",
        "testStrategy": "1. Verify numerical accuracy of the f32 SIMD path against existing f64 implementations to ensure precision is sufficient for similarity calculations.\n2. Benchmark performance improvements on typical embedding sizes (384, 768, 1536) to confirm expected speedup.\n3. Use profiling tools to ensure SIMD operations are correctly utilized and no f64 casting occurs.\n4. Validate the integration of multi-accumulator technique by comparing performance metrics with and without the technique.",
        "status": "done",
        "dependencies": [
          "6",
          "25",
          "115"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement dot_product_and_norms_f32_avx2()",
            "description": "Develop the AVX2 SIMD path using f32x8 vectors for dot product and norms.",
            "dependencies": [],
            "details": "Use AVX2 instructions to process 8 floats per iteration, optimizing for f32 data types.",
            "status": "pending",
            "testStrategy": "Compare results with existing f64 implementation for accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement dot_product_and_norms_f32_avx512()",
            "description": "Develop the AVX-512 SIMD path using f32x16 vectors for dot product and norms.",
            "dependencies": [
              1
            ],
            "details": "Utilize AVX-512 instructions to handle 16 floats per iteration, focusing on f32 optimization.",
            "status": "pending",
            "testStrategy": "Ensure numerical accuracy against f64 path and measure performance gains.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement dot_product_and_norms_f32_neon()",
            "description": "Create the ARM NEON SIMD path using f32x4 vectors for dot product and norms.",
            "dependencies": [
              1
            ],
            "details": "Implement NEON instructions to process 4 floats per iteration, targeting ARM architectures.",
            "status": "pending",
            "testStrategy": "Validate results against f64 path and benchmark on ARM devices.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add dtype detection in compute_cosine_similarity()",
            "description": "Modify compute_cosine_similarity() to detect f32 dtype and select the appropriate SIMD path.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement logic to choose between f32 and f64 paths based on input data type.",
            "status": "pending",
            "testStrategy": "Test with mixed dtype inputs to ensure correct path selection.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark f32 native vs f64 cast",
            "description": "Conduct benchmarks comparing f32 native operations to f64 casting on typical embedding sizes.",
            "dependencies": [
              4
            ],
            "details": "Measure performance on embedding sizes of 384, 768, and 1536 dimensions to evaluate speedup.",
            "status": "pending",
            "testStrategy": "Use profiling tools to confirm expected performance improvements.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T08:45:08.946Z"
      },
      {
        "id": "117",
        "title": "SIMD Inner Loop for Jaro-Winkler Bit-Parallel Path",
        "description": "Vectorize the character search inner loop in jaro_similarity_bitparallel() using SIMD for improved performance.",
        "details": "Implement SIMD vectorization for the character search inner loop in the function jaro_similarity_bitparallel() located in polars/crates/polars-ops/src/chunked_array/strings/similarity.rs. Replace the scalar byte-by-byte comparison with SIMD operations to process 32 bytes at a time. Develop a helper function find_unmatched_char_simd() that utilizes u8x32 SIMD vectors and simd_eq() for efficient character matching. Convert results to a bitmask and combine with the existing match bitmask using bitwise operations. Use trailing_zeros() to identify the first valid match position. This optimization targets strings of length 32-64 characters, aiming for a 30-50% speedup.",
        "testStrategy": "Verify the correctness of the SIMD implementation by comparing results with the existing scalar version for strings of 32-64 characters. Benchmark the performance against the current implementation to ensure a 30-50% speedup. Use a variety of test cases to cover edge cases and typical usage scenarios.",
        "status": "done",
        "dependencies": [
          "5",
          "31",
          "36"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement find_unmatched_char_simd() helper function",
            "description": "Develop the helper function using u8x32 SIMD vectors for character matching.",
            "dependencies": [],
            "details": "Create find_unmatched_char_simd() using u8x32 vectors and simd_eq() to match characters efficiently. Convert results to a bitmask.",
            "status": "pending",
            "testStrategy": "Test with known character sets to ensure correct bitmask generation.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate SIMD search into jaro_similarity_bitparallel()",
            "description": "Replace the scalar comparison with SIMD operations in the inner loop.",
            "dependencies": [
              1
            ],
            "details": "Modify jaro_similarity_bitparallel() to use find_unmatched_char_simd() for the inner loop, processing 32 bytes at a time.",
            "status": "pending",
            "testStrategy": "Compare results with the scalar version to verify correctness.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Handle edge cases for 32-byte boundaries",
            "description": "Ensure correct handling of search windows crossing 32-byte boundaries and remainder bytes.",
            "dependencies": [
              2
            ],
            "details": "Implement logic to handle cases where the search window crosses 32-byte boundaries and process any remaining bytes.",
            "status": "pending",
            "testStrategy": "Test with strings of varying lengths to cover boundary cases.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Optimize for early exit in common cases",
            "description": "Implement early exit optimization when the needle is found in the first SIMD chunk.",
            "dependencies": [
              3
            ],
            "details": "Add logic to exit early if a match is found in the first SIMD chunk, reducing unnecessary processing.",
            "status": "pending",
            "testStrategy": "Benchmark with common cases to ensure early exit improves performance.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark SIMD vs scalar search",
            "description": "Measure performance improvements of SIMD search over scalar search for strings of 32-64 characters.",
            "dependencies": [
              4
            ],
            "details": "Conduct benchmarks comparing the SIMD implementation to the scalar version, targeting a 30-50% speedup.",
            "status": "pending",
            "testStrategy": "Use a variety of test cases to measure performance gains and validate speedup targets.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T08:45:08.951Z"
      },
      {
        "id": "118",
        "title": "Fast Transposition Counting with Popcount and Bit Manipulation",
        "description": "Optimize transposition counting in jaro_similarity_bitparallel() using efficient bit manipulation techniques.",
        "details": "Implement a new function `count_transpositions_fast()` to optimize transposition counting by utilizing bit manipulation techniques such as trailing zeros and Kernighan's algorithm. This function will iterate only through set bits in `s1_matches` and `s2_matches`, significantly reducing the number of iterations compared to the current sequential approach. The implementation will:\n1. Use `trailing_zeros()` to find the next matched position using a single TZCNT/CTZ instruction.\n2. Apply `x &= x - 1` to clear the lowest set bit, following Kernighan's algorithm.\n3. Use a branchless comparison to increment the transposition count.\n4. Iterate only through actual matches, typically reducing iterations to ~30% of the string length.\n\nThe function will be implemented in `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`.",
        "testStrategy": "Verify that the transposition counts produced by the new implementation match those of the existing implementation. Benchmark the new function to ensure a 10-20% speedup over the current method. Use a variety of test cases to cover different string lengths and match scenarios, ensuring accuracy and performance improvements.",
        "status": "done",
        "dependencies": [
          "36",
          "117"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement count_transpositions_fast() Function",
            "description": "Develop the count_transpositions_fast() function using bit manipulation techniques.",
            "dependencies": [],
            "details": "Use trailing_zeros() and Kernighan's algorithm to iterate through set bits in s1_matches and s2_matches.",
            "status": "pending",
            "testStrategy": "Verify correctness with existing transposition counts.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Fast Transposition Counting",
            "description": "Replace the existing transposition counting logic in jaro_similarity_bitparallel() with the new fast version.",
            "dependencies": [
              1
            ],
            "details": "Modify jaro_similarity_bitparallel() to call count_transpositions_fast() instead of the current method.",
            "status": "pending",
            "testStrategy": "Ensure the function produces the same results as before with improved performance.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop Unit Tests for Edge Cases",
            "description": "Create unit tests to cover various edge cases for the new transposition counting function.",
            "dependencies": [
              1
            ],
            "details": "Test cases include all matches, no matches, single match, and maximum 64 matches.",
            "status": "pending",
            "testStrategy": "Run tests to confirm the function handles all edge cases correctly.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Benchmark Transposition Counting Speedup",
            "description": "Benchmark the performance improvement of the new transposition counting function in isolation.",
            "dependencies": [
              2
            ],
            "details": "Measure the execution time of count_transpositions_fast() compared to the previous method.",
            "status": "pending",
            "testStrategy": "Ensure a 10-20% speedup over the current implementation.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Measure Overall Jaro-Winkler Improvement",
            "description": "Evaluate the overall performance improvement of Jaro-Winkler with the new transposition counting and SIMD search.",
            "dependencies": [
              4
            ],
            "details": "Combine results from Task 117 and the new transposition counting to assess overall speedup.",
            "status": "pending",
            "testStrategy": "Benchmark the entire Jaro-Winkler function to verify combined improvements.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T08:45:08.957Z"
      },
      {
        "id": "119",
        "title": "Batch Row Processing for Cosine Similarity",
        "description": "Implement batch processing for cosine similarity to improve performance by processing multiple row pairs simultaneously.",
        "details": "Create a new function `cosine_similarity_batch_4()` in `polars/crates/polars-ops/src/chunked_array/array/similarity.rs` to process 4 row pairs at once. Interleave memory loads from all 4 pairs to enhance memory streaming and utilize SIMD operations (f64x4) across rows for dot products. Adjust the existing `cosine_similarity_arr()` function to handle remaining rows (1-3) individually. This approach aims to amortize loop overhead and improve memory bandwidth utilization.",
        "testStrategy": "Verify the correctness of `cosine_similarity_batch_4()` by comparing its output to the existing single row pair processing method. Benchmark performance on datasets ranging from 1K to 100K rows to ensure a 20-40% speedup. Use profiling tools to confirm improved memory bandwidth utilization and SIMD efficiency.",
        "status": "done",
        "dependencies": [
          "6",
          "25",
          "26",
          "30",
          "56"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement cosine_similarity_batch_4() function",
            "description": "Create a function to process 4 row pairs and return 4 similarities.",
            "dependencies": [],
            "details": "Develop the function in `similarity.rs` to handle 4 row pairs simultaneously, interleaving memory loads for efficiency.",
            "status": "pending",
            "testStrategy": "Compare output with single row processing for accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Update cosine_similarity_arr() for batch processing",
            "description": "Modify the function to group rows into batches of 4 for processing.",
            "dependencies": [
              1
            ],
            "details": "Adjust the existing function to organize data into groups of 4, calling the new batch function for processing.",
            "status": "pending",
            "testStrategy": "Ensure correct grouping and processing by verifying output consistency.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Handle remainder rows individually",
            "description": "Efficiently process remaining 1-3 rows that don't fit into batches.",
            "dependencies": [
              2
            ],
            "details": "Implement logic to handle leftover rows after batch processing, ensuring no data is missed.",
            "status": "pending",
            "testStrategy": "Check for correct handling of remainder rows by comparing results with full processing.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add SIMD acceleration using f64x4",
            "description": "Utilize SIMD operations to accelerate processing across row pairs.",
            "dependencies": [
              1
            ],
            "details": "Incorporate SIMD (f64x4) to perform dot products across rows, enhancing performance.",
            "status": "pending",
            "testStrategy": "Profile SIMD operations to confirm performance improvements.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark batch vs individual processing",
            "description": "Evaluate performance improvements of batch processing on large datasets.",
            "dependencies": [
              3,
              4
            ],
            "details": "Conduct benchmarks on datasets ranging from 1K to 100K rows to measure speedup and efficiency.",
            "status": "pending",
            "testStrategy": "Use profiling tools to compare batch and individual processing times, aiming for a 20-40% speedup.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T08:45:08.962Z"
      },
      {
        "id": "120",
        "title": "Prefetch Hints for Large Cosine Similarity Vectors",
        "description": "Add software prefetch hints to improve performance of cosine similarity calculations for large vectors.",
        "details": "For large vectors that don't fit in cache, use prefetch intrinsics to request upcoming data in cosine similarity SIMD loops. Implement prefetching 8 SIMD chunks ahead into L1 cache using _MM_HINT_T0 for x86_64 architecture. Adjust prefetch distance based on computation vs memory latency. Modify functions: dot_product_and_norms_avx2_fma(), dot_product_and_norms_avx512(), dot_product_and_norms_neon() in polars/crates/polars-ops/src/chunked_array/array/similarity.rs. For ARM, use __builtin_prefetch or core::intrinsics::prefetch_read_data.",
        "testStrategy": "Use performance profiling tools to measure cache miss reduction and verify speedup on large vectors (4K+ elements). Compare performance with and without prefetching to ensure a 10-15% speedup. Validate correctness by ensuring no change in output results.",
        "status": "done",
        "dependencies": [
          "6",
          "25",
          "26",
          "30"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Add prefetch intrinsics to dot_product_and_norms_avx2_fma()",
            "description": "Implement prefetching using _mm_prefetch with _MM_HINT_T0 in the AVX2 FMA function.",
            "dependencies": [],
            "details": "Modify the function dot_product_and_norms_avx2_fma() to include prefetch intrinsics using _mm_prefetch with _MM_HINT_T0. Ensure prefetching is done 8 SIMD chunks ahead.",
            "status": "pending",
            "testStrategy": "Use performance profiling tools to measure cache miss reduction.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add prefetch intrinsics to dot_product_and_norms_avx512()",
            "description": "Implement prefetching for 512-bit aligned data in the AVX512 function.",
            "dependencies": [
              1
            ],
            "details": "Modify the function dot_product_and_norms_avx512() to include prefetch intrinsics for 512-bit aligned data. Use _mm_prefetch with _MM_HINT_T0, prefetching 8 SIMD chunks ahead.",
            "status": "pending",
            "testStrategy": "Verify speedup using performance profiling tools.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add prefetch intrinsics to dot_product_and_norms_neon()",
            "description": "Implement ARM prefetch instructions in the NEON function.",
            "dependencies": [
              1
            ],
            "details": "Modify the function dot_product_and_norms_neon() to use ARM-specific prefetch instructions like __builtin_prefetch or core::intrinsics::prefetch_read_data.",
            "status": "pending",
            "testStrategy": "Measure cache miss reduction on ARM architecture.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Tune prefetch distance based on profiling",
            "description": "Adjust prefetch distance by testing different chunk sizes to optimize performance.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Profile the functions to determine optimal prefetch distance. Test prefetching 4, 8, and 16 chunks ahead to find the best balance between computation and memory latency.",
            "status": "pending",
            "testStrategy": "Use profiling tools to compare performance across different prefetch distances.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark with hardware performance counters",
            "description": "Measure the impact of prefetching on cache miss reduction using hardware counters.",
            "dependencies": [
              4
            ],
            "details": "Conduct benchmarks using hardware performance counters to quantify cache miss reduction and overall performance improvement. Compare results with and without prefetching.",
            "status": "pending",
            "testStrategy": "Ensure a 10-15% speedup and validate correctness by comparing output results.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T08:45:08.965Z"
      },
      {
        "id": "121",
        "title": "Pre-Normalized Vector Fast Path for Cosine Similarity",
        "description": "Optimize cosine similarity computation for pre-normalized vectors by skipping norm calculations.",
        "details": "Implement an optimized path for pre-normalized (unit) vectors where cosine similarity equals the dot product, eliminating the need for norm computation. This involves:\n1. Implementing `dot_product_simd()` for f64 vectors to compute the dot product using SIMD instructions.\n2. Implementing `dot_product_simd_f32()` for f32 vectors.\n3. Adding a `normalized: bool` parameter to the `cosine_similarity_arr()` function to indicate pre-normalized vectors.\n4. Exposing the `normalized` parameter in the Python API, allowing users to specify when vectors are pre-normalized, as shown in the example:\n```python\ndf.select(\n    pl.col(\"embedding\").arr.cosine_similarity(other, normalized=True)\n)\n```\nImplementation will be done in `polars/crates/polars-ops/src/chunked_array/array/similarity.rs` for Rust and `py-polars/src/polars/` for Python bindings.",
        "testStrategy": "1. Verify that results for unit vectors match the full cosine similarity computation.\n2. Benchmark the performance to ensure a 33% speedup over the full computation.\n3. Conduct tests with various pre-normalized embeddings such as OpenAI and CLIP embeddings to ensure correctness and performance gains.\n4. Validate the Python API integration by testing the `normalized` parameter functionality.",
        "status": "done",
        "dependencies": [
          "25"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement dot_product_simd() for f64 vectors",
            "description": "Develop the SIMD-based dot product function for f64 vectors.",
            "dependencies": [],
            "details": "Use SIMD instructions to compute the dot product of f64 vectors in Rust, optimizing for performance by eliminating norm calculations.",
            "status": "pending",
            "testStrategy": "Verify correctness against scalar implementation and benchmark performance.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement dot_product_simd_f32() for f32 vectors",
            "description": "Create a SIMD-based dot product function for f32 vectors.",
            "dependencies": [
              1
            ],
            "details": "Implement the SIMD dot product for f32 vectors using native f32 SIMD instructions in Rust.",
            "status": "pending",
            "testStrategy": "Ensure results match scalar implementation and measure performance improvements.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add normalized parameter to cosine_similarity_arr()",
            "description": "Introduce a boolean parameter to indicate pre-normalized vectors.",
            "dependencies": [
              1,
              2
            ],
            "details": "Modify the cosine_similarity_arr() function to accept a 'normalized' parameter, allowing the use of the fast path for pre-normalized vectors.",
            "status": "pending",
            "testStrategy": "Test with both normalized and non-normalized vectors to ensure correct path selection.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Update Python bindings to expose normalized parameter",
            "description": "Expose the 'normalized' parameter in the Python API for cosine similarity.",
            "dependencies": [
              3
            ],
            "details": "Modify the Python bindings to allow users to specify the 'normalized' parameter when calling cosine similarity functions.",
            "status": "pending",
            "testStrategy": "Test Python API with examples to ensure the parameter is correctly exposed and functional.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add documentation and examples for normalized parameter",
            "description": "Document the usage of the 'normalized' parameter and provide examples.",
            "dependencies": [
              4
            ],
            "details": "Update documentation to include explanations and examples of using the 'normalized=True' option in cosine similarity computations.",
            "status": "pending",
            "testStrategy": "Review documentation for clarity and accuracy, and verify examples work as intended.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T08:45:08.970Z"
      },
      {
        "id": "122",
        "title": "Implement Myers' Bit-Parallel Algorithm for Damerau-Levenshtein",
        "description": "Implement Myers' bit-parallel algorithm for Damerau-Levenshtein on short strings to optimize performance.",
        "details": "Extend Myers' algorithm to handle transpositions using an additional bitmask. Track previous character positions for transposition detection. Integrate this into the `damerau_levenshtein_distance_bytes()` function for strings 64 chars. For longer strings, fall back to the existing SIMD dynamic programming approach. This implementation aims to achieve a 2-3x speedup for short strings (10-64 chars).",
        "testStrategy": "Verify correctness by comparing results with the existing implementation. Benchmark performance against RapidFuzz to ensure the expected speedup. Use a variety of test cases, including edge cases and typical real-world data, to validate the implementation.",
        "status": "done",
        "dependencies": [
          "19"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core Function for Myers' Bit-Parallel Algorithm",
            "description": "Develop the core function for Myers' bit-parallel algorithm to compute Damerau-Levenshtein distance using bit vectors.",
            "dependencies": [],
            "details": "Create the `damerau_levenshtein_myers()` function to handle basic operations using bit vectors for short strings.",
            "status": "done",
            "testStrategy": "Compare results with existing implementations for accuracy.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:37.166Z"
          },
          {
            "id": 2,
            "title": "Add Transposition Tracking Logic",
            "description": "Implement logic to track and handle transpositions using an additional bitmask.",
            "dependencies": [
              1
            ],
            "details": "Extend the core function to include a bitmask that tracks previous character positions for detecting transpositions.",
            "status": "done",
            "testStrategy": "Verify transpositions are correctly detected and handled in test cases.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:37.173Z"
          },
          {
            "id": 3,
            "title": "Integrate Algorithm into Main Dispatch Function",
            "description": "Integrate the new algorithm into `damerau_levenshtein_distance_bytes()` for strings up to 64 characters.",
            "dependencies": [
              2
            ],
            "details": "Modify the dispatch logic to use the new algorithm for strings 64 chars, falling back to SIMD for longer strings.",
            "status": "done",
            "testStrategy": "Ensure correct dispatch and fallback behavior through integration tests.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:37.180Z"
          },
          {
            "id": 4,
            "title": "Develop Comprehensive Unit Tests",
            "description": "Create unit tests to validate the new implementation against the existing one.",
            "dependencies": [
              3
            ],
            "details": "Write tests that compare the new implementation's results with the existing algorithm across various cases.",
            "status": "pending",
            "testStrategy": "Use a variety of test cases, including edge cases, to ensure accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark Performance and Document Results",
            "description": "Benchmark the new implementation against the existing dynamic programming approach for strings 10-64 chars.",
            "dependencies": [
              4
            ],
            "details": "Measure and document the performance improvements achieved by the new implementation.",
            "status": "pending",
            "testStrategy": "Benchmark against RapidFuzz and document speedup results.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T09:50:59.766Z"
      },
      {
        "id": "123",
        "title": "Implement Diagonal Band Optimization for Damerau-Levenshtein",
        "description": "Optimize Damerau-Levenshtein algorithm using a diagonal band approach to improve performance.",
        "details": "Implement a diagonal band algorithm for Damerau-Levenshtein to reduce complexity from O(mn) to O(mk). This involves adapting the levenshtein_distance_adaptive_band() function to handle transpositions by extending the band by 1-2 on each side. Use three rows (prev_prev, prev, curr) within the band and integrate SIMD min operations. Add adaptive band width estimation based on length difference.",
        "testStrategy": "Verify correctness by comparing results with full matrix computation. Benchmark the speedup using medium to long strings (50-500 chars) and ensure a 5-10x performance improvement. Validate against known Damerau-Levenshtein outputs.",
        "status": "done",
        "dependencies": [
          "18",
          "122"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core Banded Damerau-Levenshtein Function",
            "description": "Develop the core function for banded Damerau-Levenshtein distance calculation.",
            "dependencies": [],
            "details": "Create the damerau_levenshtein_distance_banded() function using a diagonal band approach. Adapt the existing levenshtein_distance_adaptive_band() to handle transpositions.",
            "status": "done",
            "testStrategy": "Compare results with full matrix computation for correctness.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:38.762Z"
          },
          {
            "id": 2,
            "title": "Handle Transpositions Across Band Boundaries",
            "description": "Extend the band width to handle transpositions across boundaries.",
            "dependencies": [
              1
            ],
            "details": "Modify the band to extend by 2 on each side to accommodate transpositions. Ensure the algorithm correctly identifies and processes transpositions.",
            "status": "done",
            "testStrategy": "Verify transpositions are correctly handled by comparing with known outputs.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:38.769Z"
          },
          {
            "id": 3,
            "title": "Add Adaptive Band Width Estimation",
            "description": "Implement adaptive band width estimation based on string length difference.",
            "dependencies": [
              1
            ],
            "details": "Calculate the band width dynamically by considering the length difference between the strings. Adjust the band size accordingly.",
            "status": "done",
            "testStrategy": "Test with varying string lengths to ensure adaptive band width is correctly applied.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:38.775Z"
          },
          {
            "id": 4,
            "title": "Integrate SIMD Min Operations Within Band",
            "description": "Use SIMD operations to optimize minimum calculations within the band.",
            "dependencies": [
              1
            ],
            "details": "Integrate SIMD min operations, reusing logic from the Levenshtein implementation, to enhance performance within the band.",
            "status": "done",
            "testStrategy": "Benchmark SIMD operations against non-SIMD to verify performance gains.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:38.784Z"
          },
          {
            "id": 5,
            "title": "Integrate Banded Algorithm into Main Dispatch",
            "description": "Incorporate the banded algorithm into the main dispatch for strings longer than 64 characters.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Modify the main dispatch logic to use the banded algorithm for strings greater than 64 characters, ensuring seamless integration.",
            "status": "done",
            "testStrategy": "Test dispatch logic with various string lengths to ensure correct algorithm selection.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:38.790Z"
          },
          {
            "id": 6,
            "title": "Benchmark Banded vs Full Matrix Performance",
            "description": "Benchmark the performance of the banded algorithm against the full matrix approach.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Conduct benchmarks on strings ranging from 50 to 500 characters to evaluate performance improvements. Aim for a 5-10x speedup.",
            "status": "pending",
            "testStrategy": "Use performance benchmarks to validate speedup and compare against expected improvements.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T09:50:59.775Z"
      },
      {
        "id": "124",
        "title": "Implement Transposition-Aware SIMD for Damerau-Levenshtein",
        "description": "Optimize the transposition check in the Damerau-Levenshtein algorithm using SIMD for improved performance.",
        "details": "Implement a transposition-aware SIMD optimization by pre-computing transposition positions or using a two-pass algorithm. First, compute standard Levenshtein with SIMD, then apply transposition corrections. Use a bitmap to mark transposition-eligible cells. Alternatively, use spatial indexing for fast transposition position lookup. This aims to achieve an additional 30-50% speedup on top of the diagonal band optimization.",
        "testStrategy": "Verify correctness by comparing results with the existing Damerau-Levenshtein implementation. Benchmark performance improvements using a variety of string lengths and types, ensuring the expected speedup is achieved. Validate against known Damerau-Levenshtein outputs and ensure no regressions in accuracy.",
        "status": "deferred",
        "dependencies": [
          "123"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement precompute_transposition_positions() function",
            "description": "Develop a function to precompute positions eligible for transpositions in the Damerau-Levenshtein algorithm.",
            "dependencies": [],
            "details": "Create a function that identifies and marks positions where transpositions can occur. Use a bitmap to efficiently store these positions.",
            "status": "pending",
            "testStrategy": "Verify correctness by comparing with manual transposition checks.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create two-pass SIMD algorithm",
            "description": "Develop a two-pass SIMD algorithm for the Damerau-Levenshtein distance calculation.",
            "dependencies": [
              1
            ],
            "details": "First pass computes standard Levenshtein distance using SIMD. Second pass applies transposition corrections based on precomputed positions.",
            "status": "pending",
            "testStrategy": "Compare results with existing implementations to ensure accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Optimize transposition position lookup with spatial indexing",
            "description": "Enhance the lookup speed for transposition positions using spatial indexing techniques.",
            "dependencies": [
              1
            ],
            "details": "Implement spatial indexing to quickly access transposition-eligible positions, reducing lookup time during the correction pass.",
            "status": "pending",
            "testStrategy": "Benchmark lookup times against non-indexed approach.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Benchmark two-pass vs single-pass algorithm",
            "description": "Conduct performance benchmarks to compare the two-pass SIMD algorithm with a single-pass approach.",
            "dependencies": [
              2,
              3
            ],
            "details": "Measure execution time and speedup of the two-pass algorithm against a single-pass implementation using various string datasets.",
            "status": "pending",
            "testStrategy": "Use a range of string lengths and types to ensure comprehensive benchmarking.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Profile SIMD lane utilization improvement",
            "description": "Profile the SIMD lane utilization to verify performance improvements in the two-pass algorithm.",
            "dependencies": [
              4
            ],
            "details": "Use profiling tools to analyze SIMD lane usage and identify any bottlenecks or inefficiencies in the implementation.",
            "status": "pending",
            "testStrategy": "Ensure that SIMD lanes are fully utilized and identify areas for further optimization.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T10:06:56.413Z"
      },
      {
        "id": "125",
        "title": "Implement Fully Vectorized Match-Finding Loop for Jaro-Winkler",
        "description": "Replace the scalar inner loop in Jaro match-finding with a fully vectorized SIMD implementation to optimize performance.",
        "details": "1. Load 32 characters and match flags together using SIMD instructions.\n2. Use SIMD comparison for both character equality and match status.\n3. Combine results with an AND operation to identify valid match positions.\n4. Utilize trailing_zeros() to find the first valid match position.\n5. Implement an ARM NEON version using uint8x16_t for compatibility.\n6. Ensure the code is integrated into polars/crates/polars-ops/src/chunked_array/strings/similarity.rs.",
        "testStrategy": "1. Benchmark the new implementation against the current scalar version using strings longer than 64 characters.\n2. Validate correctness by comparing results with the existing Jaro-Winkler implementation.\n3. Ensure the expected 2-4x speedup is achieved by testing with various string lengths and types.\n4. Verify ARM NEON compatibility and performance on supported architectures.",
        "status": "done",
        "dependencies": [
          "5",
          "124"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement SIMD Character and Match Flag Loading",
            "description": "Load 32 characters and match flags using SIMD instructions.",
            "dependencies": [],
            "details": "Use SIMD instructions to load 32 characters and their corresponding match flags simultaneously. Ensure compatibility with the existing data structures.",
            "status": "done",
            "testStrategy": "Verify correct loading by comparing with scalar loading results.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:44.659Z"
          },
          {
            "id": 2,
            "title": "Perform SIMD Comparison for Character Equality and Match Status",
            "description": "Use SIMD comparison for character equality and match status.",
            "dependencies": [
              1
            ],
            "details": "Implement SIMD comparison operations to check both character equality and match status in parallel, leveraging SIMD registers.",
            "status": "done",
            "testStrategy": "Compare SIMD comparison results with scalar results for accuracy.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:44.665Z"
          },
          {
            "id": 3,
            "title": "Identify Valid Match Positions Using SIMD AND Operation",
            "description": "Combine comparison results with an AND operation to find valid match positions.",
            "dependencies": [
              2
            ],
            "details": "Use SIMD AND operations to combine character equality and match status results, identifying valid match positions efficiently.",
            "status": "done",
            "testStrategy": "Ensure match positions match expected results from scalar implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:44.670Z"
          },
          {
            "id": 4,
            "title": "Implement Trailing Zeros Functionality",
            "description": "Utilize trailing_zeros() to find the first valid match position.",
            "dependencies": [
              3
            ],
            "details": "Implement a function using trailing_zeros() to quickly locate the first valid match position from the SIMD results.",
            "status": "done",
            "testStrategy": "Test trailing zeros functionality against known data to ensure accuracy.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T09:33:44.680Z"
          },
          {
            "id": 5,
            "title": "Develop ARM NEON Version Using uint8x16_t",
            "description": "Implement an ARM NEON version using uint8x16_t for compatibility.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create an ARM NEON version of the SIMD operations using uint8x16_t to ensure compatibility across different architectures.",
            "status": "pending",
            "testStrategy": "Test ARM NEON implementation on ARM hardware to verify functionality and performance.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Integrate and Benchmark Vectorized Implementation",
            "description": "Integrate the vectorized implementation and benchmark against scalar version.",
            "dependencies": [
              5
            ],
            "details": "Integrate the SIMD implementation into the existing codebase and perform benchmarks to compare performance with the scalar version.",
            "status": "pending",
            "testStrategy": "Benchmark performance on strings of varying lengths and compare with scalar implementation to ensure expected speedup.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T09:51:04.470Z"
      },
      {
        "id": "126",
        "title": "Implement Block-Based Jaro for Very Long Strings",
        "description": "Implement block-based processing for Jaro-Winkler on very long strings to improve cache efficiency and enable parallel processing.",
        "details": "- Process strings in cache-friendly blocks (64-byte blocks fit in L1).\n- For each block, load only the relevant match window from s2.\n- Handle block boundary edge cases correctly.\n- Add parallel block processing with Rayon for very long strings.\n- Tune block size for optimal cache utilization.\n- Code Location: polars/crates/polars-ops/src/chunked_array/strings/similarity.rs.",
        "testStrategy": "- Benchmark performance improvements on strings ranging from 500 to 2000 characters.\n- Validate correctness by comparing results with existing Jaro-Winkler implementation.\n- Ensure parallel processing does not introduce race conditions or incorrect results.\n- Test edge cases for block boundaries and ensure cache efficiency is achieved.",
        "status": "done",
        "dependencies": [
          "5"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement jaro_similarity_blocked() main function",
            "description": "Develop the main function for block-based Jaro similarity.",
            "dependencies": [],
            "details": "Create a function to process strings in 64-byte blocks, ensuring cache efficiency.",
            "status": "done",
            "testStrategy": "Test with strings of varying lengths to ensure correct block processing.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.150Z"
          },
          {
            "id": 2,
            "title": "Implement find_matches_in_block() with SIMD",
            "description": "Develop a SIMD-optimized function to find matches within a block.",
            "dependencies": [
              1
            ],
            "details": "Use SIMD instructions to efficiently find matches within each block.",
            "status": "done",
            "testStrategy": "Validate SIMD operations with unit tests comparing to non-SIMD results.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.158Z"
          },
          {
            "id": 3,
            "title": "Handle block boundary edge cases",
            "description": "Ensure correct handling of edge cases at block boundaries.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement logic to manage overlaps and partial blocks at string ends.",
            "status": "done",
            "testStrategy": "Test with strings that have boundary overlaps to ensure correctness.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.165Z"
          },
          {
            "id": 4,
            "title": "Add parallel block processing with Rayon",
            "description": "Integrate Rayon for parallel processing of blocks.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Use Rayon to parallelize block processing for performance gains.",
            "status": "done",
            "testStrategy": "Benchmark parallel vs. sequential processing to ensure performance improvement.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.171Z"
          },
          {
            "id": 5,
            "title": "Tune block size for optimal cache utilization",
            "description": "Optimize block size for best cache performance.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Profile different block sizes to find the optimal size for L1 cache.",
            "status": "done",
            "testStrategy": "Profile cache usage and performance with different block sizes.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.203Z"
          },
          {
            "id": 6,
            "title": "Benchmark blocked vs non-blocked on 500-2000 char strings",
            "description": "Compare performance of blocked implementation against non-blocked.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Conduct benchmarks to measure performance improvements on long strings.",
            "status": "done",
            "testStrategy": "Benchmark and compare results with existing non-blocked implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.236Z"
          }
        ],
        "updatedAt": "2025-12-08T10:06:21.236Z"
      },
      {
        "id": "127",
        "title": "Implement Hybrid Algorithm Selection for Jaro-Winkler",
        "description": "Develop a system to intelligently select the optimal Jaro-Winkler algorithm based on string characteristics for improved performance.",
        "details": "Create a unified dispatch function to select the optimal algorithm based on string length: 0-8 chars use a direct tiny implementation, 9-64 chars use optimized bit-parallel with u64 bitmasks, 65-256 chars use fully vectorized SIMD, 257-1024 chars use block-based with SIMD, and >1024 chars use parallel block-based. Incorporate character diversity analysis to determine when a hash-based path is more efficient. Tune thresholds based on benchmarking results.",
        "testStrategy": "Benchmark the performance improvements on a variety of string lengths and types to ensure a 10-20% overall improvement. Validate the correctness by comparing results with existing Jaro-Winkler implementations. Test the dispatch function to ensure it selects the correct algorithm path based on input characteristics.",
        "status": "done",
        "dependencies": [
          "5",
          "126"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement jaro_similarity_optimal() Dispatch Function",
            "description": "Develop the dispatch function to select the optimal algorithm based on string length.",
            "dependencies": [],
            "details": "Create a function that selects the appropriate Jaro-Winkler algorithm based on string length. Use direct tiny implementation for 0-8 chars, optimized bit-parallel for 9-64 chars, fully vectorized SIMD for 65-256 chars, block-based SIMD for 257-1024 chars, and parallel block-based for >1024 chars.",
            "status": "done",
            "testStrategy": "Test with strings of various lengths to ensure correct algorithm selection.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.776Z"
          },
          {
            "id": 2,
            "title": "Add jaro_similarity_tiny() for Short Strings",
            "description": "Implement the tiny version of the Jaro-Winkler algorithm for very short strings (0-8 chars).",
            "dependencies": [
              1
            ],
            "details": "Develop a direct implementation of the Jaro-Winkler algorithm optimized for strings with 0-8 characters. Focus on minimizing overhead and maximizing performance for short strings.",
            "status": "done",
            "testStrategy": "Compare results with standard Jaro-Winkler for correctness. Benchmark performance on short strings.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.781Z"
          },
          {
            "id": 3,
            "title": "Implement Character Diversity Analysis",
            "description": "Develop a method to analyze character diversity to determine when to use hash-based vs SIMD paths.",
            "dependencies": [
              1
            ],
            "details": "Create a function to analyze the diversity of characters in the input strings. Use this analysis to decide whether a hash-based or SIMD path is more efficient for processing.",
            "status": "done",
            "testStrategy": "Test with strings of varying character diversity to ensure correct path selection.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.786Z"
          },
          {
            "id": 4,
            "title": "Add Benchmarks for Each Algorithm",
            "description": "Develop benchmarks to evaluate the performance of each algorithm on different string length ranges.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create a suite of benchmarks to measure the performance of each algorithm implementation across different string lengths. Use these benchmarks to gather data for tuning thresholds.",
            "status": "done",
            "testStrategy": "Run benchmarks and compare performance metrics across different implementations.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.793Z"
          },
          {
            "id": 5,
            "title": "Tune Length Thresholds Based on Benchmarks",
            "description": "Adjust the length thresholds for algorithm selection based on benchmark results.",
            "dependencies": [
              4
            ],
            "details": "Analyze benchmark data to determine optimal length thresholds for each algorithm. Adjust the dispatch function to use these thresholds for improved performance.",
            "status": "done",
            "testStrategy": "Validate that the new thresholds improve performance by at least 10-20% overall.",
            "parentId": "undefined",
            "updatedAt": "2025-12-08T10:06:21.801Z"
          }
        ],
        "updatedAt": "2025-12-08T10:06:21.801Z"
      },
      {
        "id": "134",
        "title": "Common Prefix/Suffix Removal Optimization",
        "description": "Implement optimization for removing common prefixes and suffixes before edit distance calculations to improve performance.",
        "details": "1. Create `remove_common_affix()` function to return trimmed slices and prefix/suffix lengths.\n2. Implement `find_common_prefix_simd()` using `u8x32` vectors for strings longer than 64 bytes.\n3. Implement `find_common_suffix_simd()` similarly for suffixes.\n4. Integrate these functions into `levenshtein_distance_adaptive_band()` and all Levenshtein variants.\n5. Integrate into `damerau_levenshtein_distance_banded()` and all DL variants.\n6. Apply suffix removal in `jaro_similarity()`.\n7. Code location: `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`.",
        "testStrategy": "1. Benchmark performance improvements using strings with common prefixes/suffixes.\n2. Validate correctness by comparing results with existing implementations.\n3. Use unit tests with edge cases, including nulls and empty strings.\n4. Verify integration with Levenshtein and Damerau-Levenshtein functions.",
        "status": "done",
        "dependencies": [
          "38"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `remove_common_affix()` Function",
            "description": "Develop a function to remove common prefixes and suffixes from strings.",
            "dependencies": [],
            "details": "Implement `remove_common_affix()` to return trimmed slices and prefix/suffix lengths.",
            "status": "pending",
            "testStrategy": "Use unit tests to verify correct prefix/suffix removal.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement `find_common_prefix_simd()`",
            "description": "Develop SIMD-based function to find common prefixes in strings longer than 64 bytes.",
            "dependencies": [
              1
            ],
            "details": "Use `u8x32` vectors to implement `find_common_prefix_simd()` for efficient prefix detection.",
            "status": "pending",
            "testStrategy": "Benchmark performance and validate with test cases.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement `find_common_suffix_simd()`",
            "description": "Develop SIMD-based function to find common suffixes in strings longer than 64 bytes.",
            "dependencies": [
              1
            ],
            "details": "Use `u8x32` vectors to implement `find_common_suffix_simd()` for efficient suffix detection.",
            "status": "pending",
            "testStrategy": "Benchmark performance and validate with test cases.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate Functions into Levenshtein Variants",
            "description": "Integrate prefix/suffix removal functions into Levenshtein distance calculations.",
            "dependencies": [
              2,
              3
            ],
            "details": "Modify `levenshtein_distance_adaptive_band()` and other variants to use new functions.",
            "status": "pending",
            "testStrategy": "Compare results with existing implementations to ensure correctness.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate Functions into Damerau-Levenshtein Variants",
            "description": "Integrate prefix/suffix removal functions into Damerau-Levenshtein distance calculations.",
            "dependencies": [
              4
            ],
            "details": "Modify `damerau_levenshtein_distance_banded()` and other variants to use new functions.",
            "status": "pending",
            "testStrategy": "Compare results with existing implementations to ensure correctness.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Apply Suffix Removal in `jaro_similarity()`",
            "description": "Integrate suffix removal optimization into Jaro similarity calculations.",
            "dependencies": [
              5
            ],
            "details": "Modify `jaro_similarity()` to apply suffix removal for performance improvement.",
            "status": "pending",
            "testStrategy": "Validate with test cases and measure performance improvements.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T12:01:32.268Z"
      },
      {
        "id": "135",
        "title": "Implement MBLEVEN2018 Algorithm for Tiny Edit Distances",
        "description": "Implement the mbleven2018 algorithm for computing Levenshtein distance when max edit distance is  3, using a precomputed lookup table for efficiency.",
        "details": "1. Create a constant lookup table `MBLEVEN2018_MATRIX` with encoded edit sequences (01=DELETE, 10=INSERT, 11=SUBSTITUTE).\n2. Implement the `levenshtein_mbleven2018()` function that iterates over each edit sequence from the lookup table.\n3. Integrate this function into the `levenshtein_distance()` dispatcher, ensuring it is called when `max_dist  3` and after removing any common affix.\n4. Optimize for high-similarity string pairs to achieve a 2-5x speedup compared to Myers' algorithm.",
        "testStrategy": "1. Develop unit tests covering typical and edge cases, including strings with edit distances of 0 to 3.\n2. Benchmark the performance against the existing Myers' bit-parallel implementation to confirm the expected speedup.\n3. Validate correctness by comparing results with known Levenshtein distances for small edit distances.\n4. Ensure integration tests confirm the dispatcher correctly selects the mbleven2018 algorithm when applicable.",
        "status": "done",
        "dependencies": [
          "19",
          "95"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-08T12:01:32.301Z"
      },
      {
        "id": "136",
        "title": "Score Hint Doubling (Iterative Band Widening) for Levenshtein",
        "description": "Implement iterative band widening for Levenshtein distance to optimize performance for moderately similar strings.",
        "details": "1. Implement `levenshtein_distance_adaptive()` function with an iterative band widening loop.\n2. Initialize with `score_hint = 31` or less if `score_cutoff` is smaller.\n3. Attempt computation with the current hint as the maximum distance.\n4. If the result is less than or equal to the hint, return it as the actual distance.\n5. If the hint is greater than or equal to `score_cutoff`, return `score_cutoff + 1` to indicate threshold exceedance.\n6. Otherwise, double the hint and retry the computation.\n7. Develop `levenshtein_distance_banded_with_max()` to dispatch the appropriate algorithm.\n8. Implement `damerau_levenshtein_distance_adaptive()` following the same pattern for Damerau-Levenshtein.",
        "testStrategy": "1. Create unit tests to verify the adaptive function against known Levenshtein distances.\n2. Benchmark performance improvements for strings with moderate similarity.\n3. Validate results with edge cases, including nulls and empty strings.\n4. Compare results with RapidFuzz to ensure accuracy and performance gains.",
        "status": "done",
        "dependencies": [
          "134"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement `levenshtein_distance_adaptive()` function",
            "description": "Create the adaptive function with iterative band widening.",
            "dependencies": [],
            "details": "Develop the function to initialize with `score_hint = 31` or less if `score_cutoff` is smaller. Implement the loop to attempt computation with the current hint as the maximum distance.",
            "status": "pending",
            "testStrategy": "Create unit tests to verify the function against known distances.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop band widening logic",
            "description": "Implement logic to double the hint and retry computation.",
            "dependencies": [
              1
            ],
            "details": "If the result is less than or equal to the hint, return it as the actual distance. Otherwise, double the hint and retry the computation.",
            "status": "pending",
            "testStrategy": "Benchmark performance improvements for strings with moderate similarity.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Handle score cutoff exceedance",
            "description": "Implement logic to handle cases where the hint exceeds the score cutoff.",
            "dependencies": [
              1
            ],
            "details": "If the hint is greater than or equal to `score_cutoff`, return `score_cutoff + 1` to indicate threshold exceedance.",
            "status": "pending",
            "testStrategy": "Validate results with edge cases, including nulls and empty strings.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement `levenshtein_distance_banded_with_max()` function",
            "description": "Create a function to dispatch the appropriate algorithm based on the band width.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Develop the function to choose between standard and banded algorithms based on the current hint and score cutoff.",
            "status": "pending",
            "testStrategy": "Compare results with RapidFuzz to ensure accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement `damerau_levenshtein_distance_adaptive()` function",
            "description": "Adapt the iterative band widening approach for Damerau-Levenshtein.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Follow the same pattern as `levenshtein_distance_adaptive()` to implement the adaptive function for Damerau-Levenshtein.",
            "status": "pending",
            "testStrategy": "Ensure consistency with Levenshtein results and validate against known distances.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T12:01:32.313Z"
      },
      {
        "id": "137",
        "title": "Small Band Diagonal Shifting Algorithm for Levenshtein",
        "description": "Implement a right-shifting diagonal band optimization for Levenshtein distance when the band width fits in a single 64-bit word.",
        "details": "Create the `levenshtein_small_band_diagonal()` function using a right-shifting approach. Initialize VP with high bits set using `~0u64 << (64 - max - 1)`. Track diagonal and horizontal masks separately. In Phase 1 (j < s1.len() - max), track the diagonal and slide the band. In Phase 2 (j >= s1.len() - max), track horizontal and shift the horizontal_mask right. Key difference: VP = hn | !((d0 >> 1) | hp). Implement early termination with break score: 2 * max + s2.len() - s1.len(). Integrate into `levenshtein_distance_banded_with_max()` for bands  64. Expected impact is a 2-3x speedup for banded computation, avoiding multi-word BlockPatternMatchVector operations.",
        "testStrategy": "Verify correctness by comparing results with the existing Levenshtein implementation. Benchmark the speedup using strings of varying lengths and ensure a 2-3x performance improvement. Validate against known Levenshtein outputs. Include edge cases such as maximum band width and minimal string lengths.",
        "status": "done",
        "dependencies": [
          "27",
          "28",
          "136"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize VP with High Bits Set",
            "description": "Set up the VP variable with high bits using the specified bitwise operation.",
            "dependencies": [],
            "details": "Use `~0u64 << (64 - max - 1)` to initialize VP, ensuring the high bits are set correctly.",
            "status": "pending",
            "testStrategy": "Verify VP initialization with test cases for various max values.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Phase 1: Diagonal Tracking",
            "description": "Implement the first phase of the algorithm to track the diagonal and slide the band.",
            "dependencies": [
              1
            ],
            "details": "In Phase 1, track the diagonal by updating the VP and diagonal masks as j < s1.len() - max.",
            "status": "pending",
            "testStrategy": "Compare diagonal tracking results with expected outputs for small strings.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Phase 2: Horizontal Tracking",
            "description": "Implement the second phase to track horizontal movement and shift the horizontal mask right.",
            "dependencies": [
              2
            ],
            "details": "In Phase 2, update the horizontal_mask and shift it right as j >= s1.len() - max.",
            "status": "pending",
            "testStrategy": "Ensure horizontal tracking shifts correctly by testing with edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate Early Termination Logic",
            "description": "Add logic to terminate the algorithm early based on the break score.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement early termination using the break score: 2 * max + s2.len() - s1.len().",
            "status": "pending",
            "testStrategy": "Test early termination with strings where the break score condition is met.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate into Levenshtein Distance Function",
            "description": "Integrate the small band diagonal function into the main Levenshtein distance computation.",
            "dependencies": [
              4
            ],
            "details": "Integrate `levenshtein_small_band_diagonal()` into `levenshtein_distance_banded_with_max()` for bands  64.",
            "status": "pending",
            "testStrategy": "Benchmark integrated function for performance improvements and validate against known outputs.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T12:01:32.319Z"
      },
      {
        "id": "138",
        "title": "Ukkonen Dynamic Band Adjustment for Levenshtein",
        "description": "Implement dynamic adjustment of first_block and last_block during banded Levenshtein computation using Ukkonen's algorithm.",
        "details": "1. Track scores per block using a Vec<usize> with initial values scores[i] = (i+1) * 64.\n2. Compute the maximum allowed score dynamically based on current scores and position.\n3. Implement logic to expand the last_block by checking if the next block could affect the result.\n4. Implement band shrinking from both ends based on in-band conditions.\n5. Handle band collapse by returning max+1 when last_block < first_block.\n6. Integrate with the existing BlockPatternMatchVector infrastructure.\n7. Ensure proper HP/HN carry propagation during block processing.",
        "testStrategy": "1. Develop unit tests to validate dynamic band adjustments under various scenarios.\n2. Benchmark performance improvements against existing implementations.\n3. Validate integration with BlockPatternMatchVector using test cases from RapidFuzz.\n4. Test edge cases where band collapse should occur, ensuring correct handling.",
        "status": "done",
        "dependencies": [
          "137"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Score Tracking for Blocks",
            "description": "Set up initial score tracking using a Vec<usize> for each block.",
            "dependencies": [],
            "details": "Create a Vec<usize> where each element is initialized to (i+1) * 64 to track scores per block.",
            "status": "pending",
            "testStrategy": "Verify Vec<usize> initialization with unit tests.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Dynamic Score Calculation",
            "description": "Compute the maximum allowed score dynamically based on current scores and position.",
            "dependencies": [
              1
            ],
            "details": "Develop logic to calculate the maximum score dynamically, adjusting based on current block scores and positions.",
            "status": "pending",
            "testStrategy": "Test dynamic score calculation with varying block scores.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Expand Last Block Logic",
            "description": "Implement logic to expand the last_block by checking if the next block could affect the result.",
            "dependencies": [
              2
            ],
            "details": "Add conditions to expand last_block when the next block's potential impact on results is detected.",
            "status": "pending",
            "testStrategy": "Create test cases to ensure last_block expands correctly.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Band Shrinking Logic",
            "description": "Shrink the band from both ends based on in-band conditions.",
            "dependencies": [
              3
            ],
            "details": "Develop logic to adjust first_block and last_block based on conditions within the band.",
            "status": "pending",
            "testStrategy": "Test band shrinking with edge cases and normal scenarios.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate with BlockPatternMatchVector",
            "description": "Integrate the dynamic band adjustment with the existing BlockPatternMatchVector infrastructure.",
            "dependencies": [
              4
            ],
            "details": "Ensure compatibility and integration with BlockPatternMatchVector, handling HP/HN carry propagation.",
            "status": "pending",
            "testStrategy": "Validate integration with existing test cases from RapidFuzz.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T12:01:32.327Z"
      },
      {
        "id": "139",
        "title": "SIMD Batch Processing for Fuzzy Joins (cdist pattern)",
        "description": "Implement SIMD parallelism for batch processing of fuzzy joins using the cdist pattern from RapidFuzz.",
        "details": "1. Create `BatchPatternMatchVector` struct to store pattern match vectors for multiple strings.\n2. Implement `levenshtein_batch_simd_u64()` for strings up to 64 chars using AVX2 for 4 parallel comparisons.\n3. Implement `levenshtein_batch_simd_u32()` for strings up to 32 chars for 8 parallel comparisons.\n4. Implement `levenshtein_batch_simd_u16()` for strings up to 16 chars for 16 parallel comparisons.\n5. Use SIMD vectors for VP, VN, and currDist to process multiple comparisons simultaneously.\n6. Handle length heterogeneity within batches.\n7. Integrate the batch API into `fuzzy_join()` for the candidate comparison phase.\n8. Code location: `polars/crates/polars-ops/src/chunked_array/strings/similarity.rs`.",
        "testStrategy": "1. Verify correctness by comparing SIMD batch outputs with existing single comparison methods.\n2. Benchmark performance improvements on datasets ranging from 1K to 100K rows to ensure a 4-8x speedup.\n3. Use profiling tools to confirm SIMD utilization and memory bandwidth improvements.\n4. Test edge cases with varying string lengths and content diversity.",
        "status": "done",
        "dependencies": [
          "19",
          "27",
          "28",
          "95"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Create BatchPatternMatchVector Struct",
            "description": "Design and implement the BatchPatternMatchVector struct to store pattern match vectors for multiple strings.",
            "dependencies": [],
            "details": "Define a struct in Rust to hold vectors for pattern matching across multiple strings. Ensure it supports SIMD operations.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify struct initialization and data storage.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement levenshtein_batch_simd_u64()",
            "description": "Develop the SIMD function for Levenshtein distance for strings up to 64 characters using AVX2.",
            "dependencies": [
              1
            ],
            "details": "Use AVX2 instructions to perform 4 parallel comparisons for strings up to 64 characters.",
            "status": "pending",
            "testStrategy": "Compare results with non-SIMD implementation for correctness and benchmark performance.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement levenshtein_batch_simd_u32()",
            "description": "Develop the SIMD function for Levenshtein distance for strings up to 32 characters.",
            "dependencies": [
              1
            ],
            "details": "Utilize AVX2 to perform 8 parallel comparisons for strings up to 32 characters.",
            "status": "pending",
            "testStrategy": "Ensure correctness by comparing with existing methods and measure speedup.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement levenshtein_batch_simd_u16()",
            "description": "Develop the SIMD function for Levenshtein distance for strings up to 16 characters.",
            "dependencies": [
              1
            ],
            "details": "Use AVX2 to perform 16 parallel comparisons for strings up to 16 characters.",
            "status": "pending",
            "testStrategy": "Validate results against non-SIMD methods and assess performance improvements.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Handle Length Heterogeneity in Batches",
            "description": "Implement logic to handle varying string lengths within batches.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Adjust SIMD operations to accommodate strings of different lengths within the same batch.",
            "status": "pending",
            "testStrategy": "Test with mixed-length strings to ensure correct handling and performance.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Integrate Batch API into fuzzy_join()",
            "description": "Incorporate the SIMD batch processing API into the fuzzy_join function for candidate comparisons.",
            "dependencies": [
              5
            ],
            "details": "Modify fuzzy_join to use the new SIMD batch functions for improved performance during candidate comparisons.",
            "status": "pending",
            "testStrategy": "Run integration tests to verify functionality and benchmark overall performance improvements.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T12:01:32.333Z"
      },
      {
        "id": "140",
        "title": "mbleven2018 Algorithm for Tiny Edit Distances",
        "description": "Implement the mbleven2018 algorithm for computing Levenshtein distance when max edit distance  3 using a precomputed lookup table.",
        "details": "Implementation:\n1. Create MBLEVEN2018_MATRIX constant lookup table with encoded edit sequences (01=DELETE, 10=INSERT, 11=SUBSTITUTE)\n2. Rows indexed by: (max_dist + max_dist*max_dist) / 2 + len_diff - 1\n3. Implement levenshtein_mbleven2018(s1, s2, max_dist) -> Option<usize>\n4. For each possible edit sequence from lookup table, try applying operations\n5. Track best_dist and return Some(best_dist) if <= max_dist\n6. Integrate into levenshtein_distance() dispatcher after common affix removal when max_dist  3\n\nExpected Impact: 2-5x speedup for high-similarity string pairs (edit distance 1-3)\n\nCode Location: polars/crates/polars-ops/src/chunked_array/strings/similarity.rs",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "134",
          "136"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create MBLEVEN2018_MATRIX Constant Lookup Table",
            "description": "Develop a constant lookup table with encoded edit sequences for the mbleven2018 algorithm.",
            "dependencies": [],
            "details": "Define MBLEVEN2018_MATRIX with encoded sequences (01=DELETE, 10=INSERT, 11=SUBSTITUTE). Ensure rows are indexed by the formula (max_dist + max_dist*max_dist) / 2 + len_diff - 1.",
            "status": "pending",
            "testStrategy": "Verify table correctness by checking encoded sequences against expected values.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement levenshtein_mbleven2018 Function",
            "description": "Develop the function to compute Levenshtein distance using the mbleven2018 algorithm.",
            "dependencies": [
              1
            ],
            "details": "Create levenshtein_mbleven2018(s1, s2, max_dist) -> Option<usize>. Iterate over each edit sequence from the lookup table and apply operations to calculate the distance.",
            "status": "pending",
            "testStrategy": "Develop unit tests for typical and edge cases with edit distances 0 to 3.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Function into Dispatcher",
            "description": "Integrate the levenshtein_mbleven2018 function into the existing levenshtein_distance dispatcher.",
            "dependencies": [
              2
            ],
            "details": "Modify the levenshtein_distance() dispatcher to call levenshtein_mbleven2018 when max_dist  3, ensuring it follows common affix removal.",
            "status": "pending",
            "testStrategy": "Test integration by comparing results with known distances and ensuring correct function calls.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Optimize for High-Similarity Pairs",
            "description": "Optimize the algorithm for high-similarity string pairs to achieve a speedup.",
            "dependencies": [
              3
            ],
            "details": "Focus on optimizing the levenshtein_mbleven2018 function to handle high-similarity pairs efficiently, aiming for a 2-5x speedup.",
            "status": "pending",
            "testStrategy": "Benchmark performance against existing implementations to confirm speedup.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Develop Comprehensive Test Suite",
            "description": "Create a test suite to validate the implementation and performance of the mbleven2018 algorithm.",
            "dependencies": [
              4
            ],
            "details": "Develop tests covering various scenarios, including edge cases and performance benchmarks. Compare results with existing algorithms to ensure accuracy.",
            "status": "pending",
            "testStrategy": "Run tests and benchmarks to validate correctness and performance improvements.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-08T12:01:32.338Z"
      },
      {
        "id": "141",
        "title": "Implement Position-Based Character Matching for Jaro-Winkler",
        "description": "Replace O(nm) scanning with O(1) character position lookup for Jaro-Winkler similarity. This approach is expected to achieve a 2-3x speedup for medium-length strings.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "Use a position list mapping each character value to its positions in the shorter string, reducing per-character lookup complexity. Focus on optimizing the lookup process to ensure maximum efficiency.",
        "testStrategy": "Compare outputs against current implementation, benchmark at 1K/10K/100K scales. Ensure that the new implementation consistently outperforms the old one, particularly for medium-length strings.",
        "subtasks": [
          {
            "id": 1,
            "title": "Design CharacterPositionIndex Struct",
            "description": "Create a struct to map characters to their positions in a string.",
            "dependencies": [],
            "details": "Define a struct that efficiently maps each character to its positions in the shorter string. Use a hash map or similar data structure for quick lookups.",
            "status": "pending",
            "testStrategy": "Verify correct mapping with unit tests.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Jaro Similarity for 64 Characters",
            "description": "Develop a position-based Jaro similarity function for strings up to 64 characters.",
            "dependencies": [
              1
            ],
            "details": "Use the CharacterPositionIndex to optimize the Jaro similarity calculation for strings with a maximum length of 64 characters.",
            "status": "pending",
            "testStrategy": "Compare results with existing implementation for accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Extend Jaro Similarity to 65-128 Characters",
            "description": "Enhance the function to handle strings between 65 and 128 characters using u128.",
            "dependencies": [
              2
            ],
            "details": "Modify the implementation to use u128 for handling larger strings, ensuring efficient position tracking and comparison.",
            "status": "pending",
            "testStrategy": "Test with strings of varying lengths to ensure correct handling.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Early Exit Optimization",
            "description": "Add logic to exit early if a match is impossible.",
            "dependencies": [
              3
            ],
            "details": "Incorporate checks to terminate the similarity calculation early if conditions indicate no possible match, improving efficiency.",
            "status": "pending",
            "testStrategy": "Benchmark to confirm performance improvements.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate Position-Based Matching into Dispatch",
            "description": "Incorporate the new position-based matching logic into the main dispatch function.",
            "dependencies": [
              4
            ],
            "details": "Update the dispatch mechanism to use the new position-based approach, ensuring seamless integration with existing logic.",
            "status": "pending",
            "testStrategy": "Run integration tests to verify correct dispatch behavior.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Benchmark Position-Based Matching",
            "description": "Measure performance improvements of the new implementation.",
            "dependencies": [
              5
            ],
            "details": "Conduct benchmarks at various scales (1K/10K/100K) to quantify speed improvements over the previous implementation.",
            "status": "pending",
            "testStrategy": "Compare benchmark results with the old implementation.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Develop Comprehensive Testing Suite",
            "description": "Create tests to ensure the new implementation is robust and accurate.",
            "dependencies": [
              6
            ],
            "details": "Design a suite of tests covering edge cases, typical use cases, and performance scenarios to validate the new implementation.",
            "status": "pending",
            "testStrategy": "Ensure all tests pass and results match expected outcomes.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-09T08:02:19.635Z"
      },
      {
        "id": "142",
        "title": "AVX2 SIMD Parallel Match Finding",
        "description": "Use AVX2 intrinsics to find multiple character matches simultaneously in Jaro-Winkler. This task can be implemented in parallel with Task 141 for additional 1.3-1.5x speedup.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "Process match-finding using AVX2 256-bit vectors to compare 32 characters at once, accelerating the inner loop of the Jaro algorithm. Implement in parallel with Task 141 to achieve enhanced performance.",
        "testStrategy": "Verify identical results to scalar, benchmark SIMD vs scalar paths. Measure speedup when implemented in parallel with Task 141.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement find_matches_avx2 Function",
            "description": "Develop the find_matches_avx2 function using AVX2 intrinsics.",
            "dependencies": [],
            "details": "Use AVX2 256-bit vectors to compare 32 characters at once. Ensure safety guards are in place to handle edge cases.",
            "status": "pending",
            "testStrategy": "Verify results match scalar implementation.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Runtime CPU Feature Detection",
            "description": "Detect CPU features at runtime to ensure AVX2 support.",
            "dependencies": [
              1
            ],
            "details": "Use CPUID instruction to check for AVX2 support and enable/disable SIMD paths accordingly.",
            "status": "pending",
            "testStrategy": "Test on systems with and without AVX2 support.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop SSE2 Fallback for Older CPUs",
            "description": "Create a fallback mechanism using SSE2 for CPUs lacking AVX2 support.",
            "dependencies": [
              2
            ],
            "details": "Implement SSE2 intrinsics to handle match finding on older CPUs, ensuring compatibility.",
            "status": "pending",
            "testStrategy": "Verify functionality on non-AVX2 systems.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create jaro_similarity_simd_avx2 Wrapper",
            "description": "Develop a wrapper function for SIMD-based Jaro similarity calculation.",
            "dependencies": [
              1,
              3
            ],
            "details": "Integrate AVX2 and SSE2 paths into a unified function, selecting the appropriate path at runtime.",
            "status": "pending",
            "testStrategy": "Compare results with existing Jaro-Winkler function.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate SIMD Functions with Dispatch Mechanism",
            "description": "Integrate SIMD functions into the existing dispatch mechanism for Jaro-Winkler.",
            "dependencies": [
              4
            ],
            "details": "Modify the dispatch logic to call SIMD functions based on CPU capabilities.",
            "status": "pending",
            "testStrategy": "Ensure correct function calls based on CPU detection.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Benchmark on AVX2 and Non-AVX2 Systems",
            "description": "Conduct benchmarking to measure performance improvements on different systems.",
            "dependencies": [
              5
            ],
            "details": "Run benchmarks comparing SIMD and scalar paths, measuring speedup on AVX2 and non-AVX2 systems.",
            "status": "pending",
            "testStrategy": "Analyze performance metrics and validate speedup claims.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-09T08:02:19.643Z"
      },
      {
        "id": "143",
        "title": "Parallel Batch Processing with Rayon",
        "description": "Process large datasets in parallel for Jaro-Winkler similarity using Rayon.",
        "details": "Parallelize across multiple CPU cores for datasets larger than a threshold, with each thread processing a chunk of pairs independently.",
        "testStrategy": "Verify identical results to sequential, benchmark on multi-core systems.",
        "priority": "medium",
        "dependencies": [
          "141",
          "142"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Rayon Dependency",
            "description": "Integrate the Rayon library into the project for parallel processing.",
            "dependencies": [],
            "details": "Update the Cargo.toml file to include the Rayon dependency. Ensure the project builds successfully with the new dependency.",
            "status": "pending",
            "testStrategy": "Verify build success and no dependency conflicts.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Parallel Jaro-Winkler Similarity",
            "description": "Develop a parallel version of the Jaro-Winkler similarity function using Rayon.",
            "dependencies": [
              1
            ],
            "details": "Refactor the existing Jaro-Winkler function to use Rayon for parallel processing. Ensure each thread processes a chunk of data independently.",
            "status": "pending",
            "testStrategy": "Compare results with the sequential version to ensure correctness.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Determine Optimal Threshold via Benchmarking",
            "description": "Identify the dataset size threshold for switching to parallel processing.",
            "dependencies": [
              2
            ],
            "details": "Conduct benchmarks on various dataset sizes to determine the optimal threshold for parallel execution.",
            "status": "pending",
            "testStrategy": "Analyze benchmark results to identify the threshold that maximizes performance.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Chunk-Based Parallel Processing",
            "description": "Optimize parallel processing by chunking data for better cache locality.",
            "dependencies": [
              2
            ],
            "details": "Divide datasets into chunks that fit into cache lines to improve performance. Use Rayon to process these chunks in parallel.",
            "status": "pending",
            "testStrategy": "Benchmark to ensure improved cache performance.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Update Main Function for Parallel Dispatch",
            "description": "Modify the main function to use the parallel version of Jaro-Winkler when appropriate.",
            "dependencies": [
              3,
              4
            ],
            "details": "Add logic to the main function to dispatch to the parallel version based on the dataset size threshold.",
            "status": "pending",
            "testStrategy": "Test with datasets of varying sizes to ensure correct function dispatch.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Benchmarking at Multiple Scales",
            "description": "Conduct performance benchmarks at 10K, 100K, and 1M dataset scales.",
            "dependencies": [
              5
            ],
            "details": "Run benchmarks on datasets of 10K, 100K, and 1M to measure performance improvements and scalability.",
            "status": "pending",
            "testStrategy": "Analyze benchmark data for performance trends and scalability.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Verify Thread Safety",
            "description": "Ensure the parallel implementation is thread-safe and free of race conditions.",
            "dependencies": [
              2,
              4
            ],
            "details": "Review the code for potential race conditions and use appropriate synchronization mechanisms if necessary.",
            "status": "pending",
            "testStrategy": "Use tools like ThreadSanitizer to detect and resolve any thread safety issues.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-09T08:02:19.649Z"
      },
      {
        "id": "144",
        "title": "Early Exit with Length-Based Upper Bound",
        "description": "Skip full computation when Jaro similarity will definitely be below threshold.",
        "details": "Check if the maximum possible similarity based on string lengths can meet the threshold before computing full Jaro similarity.",
        "testStrategy": "Verify threshold filtering produces correct results, benchmark rejection scenarios.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Jaro Max Possible Upper Bound Function",
            "description": "Develop a function to calculate the maximum possible Jaro similarity based on string lengths.",
            "dependencies": [],
            "details": "Create a function that computes the upper bound of Jaro similarity using only string lengths. This will help in early exit if the threshold cannot be met.",
            "status": "pending",
            "testStrategy": "Unit test with various string lengths to ensure correct upper bound calculation.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Jaro-Winkler Max Possible with Prefix Bonus",
            "description": "Enhance the Jaro max possible function to include the Winkler prefix bonus.",
            "dependencies": [
              1
            ],
            "details": "Modify the Jaro max possible function to account for the Winkler prefix bonus, providing a more accurate upper bound.",
            "status": "pending",
            "testStrategy": "Test with strings having common prefixes to verify the prefix bonus is applied correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add Jaro-Winkler with Threshold Variant",
            "description": "Create a variant of the Jaro-Winkler function that uses the threshold for early exit.",
            "dependencies": [
              2
            ],
            "details": "Develop a new function that incorporates the threshold check using the max possible similarity to skip unnecessary computations.",
            "status": "pending",
            "testStrategy": "Functional tests to ensure early exits occur when the threshold is not met.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Expose Threshold Parameter in Python Bindings",
            "description": "Update Python bindings to allow users to set the similarity threshold.",
            "dependencies": [
              3
            ],
            "details": "Modify the Python API to include a threshold parameter, enabling users to specify their desired similarity threshold.",
            "status": "pending",
            "testStrategy": "Integration tests to confirm the threshold parameter is correctly passed and utilized.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate with Fuzzy Join Operations",
            "description": "Incorporate the new threshold-based functions into existing fuzzy join operations.",
            "dependencies": [
              4
            ],
            "details": "Update the fuzzy join logic to utilize the new early exit functions, improving performance by reducing unnecessary calculations.",
            "status": "pending",
            "testStrategy": "Benchmark tests to measure performance improvements in fuzzy join operations.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Benchmark on Typical Fuzzy Matching Workloads",
            "description": "Evaluate the performance of the new implementation on standard workloads with a 90% rejection rate.",
            "dependencies": [
              5
            ],
            "details": "Conduct benchmarks to assess the effectiveness of the early exit strategy in real-world scenarios, focusing on rejection rates.",
            "status": "pending",
            "testStrategy": "Performance benchmarks comparing old and new implementations, focusing on rejection efficiency.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-09T08:02:19.656Z"
      },
      {
        "id": "145",
        "title": "Cache-Optimized Batch Processing",
        "description": "Process string pairs in cache-friendly batches to minimize memory latency for Jaro-Winkler.",
        "details": "Process in batches that fit in L2 cache, pre-fetch next batch while processing current.",
        "testStrategy": "Use perf counters to measure cache behavior, benchmark throughput.",
        "priority": "medium",
        "dependencies": [
          "143"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Profile Cache Miss Patterns",
            "description": "Analyze current implementation for cache miss patterns.",
            "dependencies": [],
            "details": "Use profiling tools to identify cache miss patterns in the current Jaro-Winkler implementation.",
            "status": "pending",
            "testStrategy": "Use perf counters to measure cache misses.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Batch Collection from ChunkedArray",
            "description": "Develop a method to collect batches from ChunkedArray.",
            "dependencies": [
              1
            ],
            "details": "Create a function to efficiently collect string pairs into batches from ChunkedArray.",
            "status": "pending",
            "testStrategy": "Ensure correct batch collection with unit tests.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Determine Optimal Batch Size",
            "description": "Benchmark to find the optimal batch size for L2/L3 cache.",
            "dependencies": [
              2
            ],
            "details": "Conduct experiments to determine the batch size that fits well within L2/L3 cache limits.",
            "status": "pending",
            "testStrategy": "Benchmark different batch sizes and analyze cache hit rates.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add Software Prefetching Hints",
            "description": "Implement prefetching hints for the next batch.",
            "dependencies": [
              3
            ],
            "details": "Use compiler or intrinsic functions to add prefetching hints for the next batch of data.",
            "status": "pending",
            "testStrategy": "Measure performance improvement with prefetching enabled.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Combine with Parallel Processing",
            "description": "Integrate batch processing with parallel execution.",
            "dependencies": [
              4
            ],
            "details": "Use parallel processing libraries to execute batches in parallel, leveraging multi-core systems.",
            "status": "pending",
            "testStrategy": "Verify parallel execution correctness and benchmark performance.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Benchmark Cache Hit Rates",
            "description": "Benchmark cache hit rates before and after optimization.",
            "dependencies": [
              5
            ],
            "details": "Use profiling tools to compare cache hit rates before and after implementing optimizations.",
            "status": "pending",
            "testStrategy": "Analyze cache hit rate improvements and overall performance gains.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-09T08:02:19.662Z"
      },
      {
        "id": "146",
        "title": "Jaro-Winkler for Long Strings (>64 chars)",
        "description": "Optimize Jaro-Winkler for strings longer than 64 characters using u128 bitmasks and chunked processing.",
        "details": "Implement specialized version using u128 bitmasks for 65-128 chars and chunked processing for longer strings.",
        "testStrategy": "Verify correctness on long strings, benchmark length scaling.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement jaro_similarity_u128 for 65-128 char strings",
            "description": "Develop a specialized Jaro-Winkler function using u128 bitmasks for strings between 65 and 128 characters.",
            "dependencies": [],
            "details": "Use u128 bitmasks to efficiently calculate matches and transpositions for strings of length 65-128.",
            "status": "pending",
            "testStrategy": "Test with strings of 65-128 characters to ensure accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement jaro_similarity_blocked_v2 for strings >128 chars",
            "description": "Create a version of Jaro-Winkler that processes strings longer than 128 characters using chunked processing.",
            "dependencies": [
              1
            ],
            "details": "Divide strings into chunks and process each using optimized bitmask operations.",
            "status": "pending",
            "testStrategy": "Verify correctness with strings longer than 128 characters.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Optimize cross-block transposition counting",
            "description": "Enhance the algorithm to accurately count transpositions across string chunks.",
            "dependencies": [
              2
            ],
            "details": "Implement logic to track and count transpositions that occur across chunk boundaries.",
            "status": "pending",
            "testStrategy": "Test with strings that have transpositions across chunks.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate into jaro_similarity_bytes_optimized",
            "description": "Incorporate the new functions into the existing optimized Jaro-Winkler dispatch logic.",
            "dependencies": [
              3
            ],
            "details": "Modify the dispatch logic to call the appropriate function based on string length.",
            "status": "pending",
            "testStrategy": "Ensure all paths are correctly integrated and functional.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Benchmark on varying string lengths",
            "description": "Conduct performance benchmarks on strings of different lengths to evaluate optimization impact.",
            "dependencies": [
              4
            ],
            "details": "Run benchmarks on strings of 50, 100, 200, and 500 characters to measure performance improvements.",
            "status": "pending",
            "testStrategy": "Compare benchmark results against previous versions to assess speedup.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-09T08:02:19.668Z"
      },
      {
        "id": "147",
        "title": "Unified Jaro-Winkler Dispatcher Optimization",
        "description": "Reduce dispatch overhead and ensure optimal path selection for Jaro-Winkler.",
        "details": "Consolidate and optimize the dispatch logic to minimize branching overhead and ensure the fastest path is always selected based on string characteristics.",
        "testStrategy": "Ensure all paths are still reachable and correct, profile overhead.",
        "priority": "medium",
        "dependencies": [
          "141",
          "142",
          "144",
          "145",
          "146"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Profile Current Dispatch Overhead",
            "description": "Analyze the current dispatch logic to identify overhead.",
            "dependencies": [],
            "details": "Use profiling tools to measure the time and resource usage of the current dispatch logic.",
            "status": "pending",
            "testStrategy": "Compare profiling results before and after optimization.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Consolidate Redundant Dispatch Checks",
            "description": "Identify and merge redundant checks in the dispatch logic.",
            "dependencies": [
              1
            ],
            "details": "Review the dispatch code to find and consolidate checks that are performed multiple times.",
            "status": "pending",
            "testStrategy": "Ensure all necessary checks remain and are executed correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Length-Based Jump Table",
            "description": "Create a jump table for O(1) path selection based on string length.",
            "dependencies": [
              2
            ],
            "details": "Develop a jump table that allows for constant-time path selection by string length.",
            "status": "pending",
            "testStrategy": "Verify that the jump table selects the correct path for various string lengths.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Remove Dead Code Paths",
            "description": "Eliminate unreachable code and redundant conditions from the dispatcher.",
            "dependencies": [
              3
            ],
            "details": "Analyze the code to identify and remove any dead paths or unnecessary conditions.",
            "status": "pending",
            "testStrategy": "Ensure that all remaining paths are reachable and necessary.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add #[cold] Attributes to Unlikely Error Paths",
            "description": "Mark unlikely error paths with #[cold] attributes to optimize performance.",
            "dependencies": [
              4
            ],
            "details": "Identify error paths that are rarely executed and annotate them with #[cold].",
            "status": "pending",
            "testStrategy": "Check that performance improves without affecting error handling.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Benchmark Dispatch Overhead Before and After Optimization",
            "description": "Measure the performance improvements from the optimization.",
            "dependencies": [
              5
            ],
            "details": "Conduct benchmarks to compare dispatch overhead before and after the optimization process.",
            "status": "pending",
            "testStrategy": "Use consistent benchmarking methods to validate performance gains.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-09T08:02:19.673Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-09T08:02:19.674Z",
      "taskCount": 141,
      "completedCount": 134,
      "tags": [
        "master"
      ]
    }
  }
}